# ë°œí‘œ ìë£Œ ë©”ëª¨ README

> ì¶”í›„ ì œê±°

---

## í”„ë¡œì íŠ¸ëª…

**ê³µì‹œ ë¬¸ì„œ ë° ì• ë„ë¦¬ìŠ¤íŠ¸ ë¶„ì„ ë¦¬í¬íŠ¸ ê¸°ë°˜ì˜ íˆ¬ì Q&A ì‹œìŠ¤í…œ**

- ì‚¬ìš©ìì˜ íˆ¬ì ì§€ì‹ ìˆ˜ì¤€ ì¸ì‹ ë¡œì§ìœ¼ë¡œ ì ì‘í˜• ì‹œìŠ¤í…œ (ê°œì¸í™”)

---

## [ğŸ§© ETLì˜ ì •ì˜]

**ETL**ì€ **Extract** â†’ **Transform** â†’ **Loadì˜** ì¤„ì„ë§ì´ì—ìš”.
ì¦‰, ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê³ (Extract), ê°€ê³µí•˜ê³ (Transform), ì €ì¥í•˜ëŠ”(Load) ì „ì²´ ê³¼ì •ì„ ë§í•´ìš”.

âœ… í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ë©´
**ë°ì´í„°ë¥¼ ê¹¨ë—í•˜ê²Œ ì •ë¦¬í•´ì„œ ì“¸ ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ì˜®ê¸°ëŠ” íŒŒì´í”„ë¼ì¸**ì´ì—ìš”.

âš™ï¸ 2. ETLì˜ í”„ë¡œì„¸ìŠ¤ ìš”ì•½
| ë‹¨ê³„ | ì˜ë¯¸ | ì˜ˆì‹œ |
| ----------------- | ------------------------------------ | ---------------------------- |
| **E (Extract)** | ì—¬ëŸ¬ ê³³(ì›¹, DB, API ë“±)ì—ì„œ ì›ë³¸ ë°ì´í„°ë¥¼ ëŒì–´ì˜¤ëŠ” ë‹¨ê³„ | APIë¡œ XML ë°ì´í„° ë‹¤ìš´ë¡œë“œ |
| **T (Transform)** | ë¶ˆí•„ìš”í•œ ë°ì´í„°ë¥¼ ì œê±°í•˜ê³ , ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ê°€ê³µ | XML â†’ ìì—°ì–´ ë¬¸ì„œí™” (MD + YAML ë³€í™˜) |
| **L (Load)** | ê°€ê³µëœ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ë‚˜ ë²¡í„°DBì— ì €ì¥ | Embedding í›„ VectorDBì— ì €ì¥ |

```scss
API (XML)
 â†“
> [Extract]
 â†“
ë¬¸ì„œí™” (MD + YAML)
 â†“
> [Transform]
 â†“
ì²­í¬ ì„ë² ë”© â†’ ë²¡í„°DB
 â†“
> [Load]
 â†“
ì§ˆì˜ â†’ ê²€ìƒ‰ â†’ ë‹µë³€ ìƒì„±
```

### ğŸ§  ì™œ ETLì´ ì¤‘ìš”í•œê°€?

| ì´ìœ                      | ì„¤ëª…                                                                                              |
| ------------------------ | ------------------------------------------------------------------------------------------------- |
| **1ï¸âƒ£ ì •í™•ì„± í™•ë³´**       | ì›ë³¸ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ì“°ë©´ ì˜¤ë¥˜ë‚˜ ì¤‘ë³µì´ ë§ì•„ìš”. ETLì€ ë°ì´í„°ë¥¼ ì •ì œ(clean)í•´ì„œ ì¼ê´€ì„±ì„ ë³´ì¥í•´ìš”. |
| **2ï¸âƒ£ íš¨ìœ¨ì„± í–¥ìƒ**       | ì‚¬ëŒì´ ì¼ì¼ì´ ë³€í™˜í•˜ëŠ” ëŒ€ì‹  ìë™í™”ëœ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì²˜ë¦¬ ì†ë„ê°€ ìˆ˜ì‹­ ë°° ë¹¨ë¼ì§‘ë‹ˆë‹¤.               |
| **3ï¸âƒ£ ë¶„ì„/AI í™œìš© ê¸°ë°˜** | LLM, ë¨¸ì‹ ëŸ¬ë‹, ëŒ€ì‹œë³´ë“œ, ë¦¬í¬íŠ¸ ë“± ëª¨ë“  ë°ì´í„° ë¶„ì„ì€ ETLì„ ê±°ì¹œ í›„ ê°€ëŠ¥í•©ë‹ˆë‹¤.                   |
| **4ï¸âƒ£ ì¬ì‚¬ìš©ì„± í™•ë³´**     | ë™ì¼í•œ ETL íŒŒì´í”„ë¼ì¸ì„ ìœ ì§€í•˜ë©´, ë‚˜ì¤‘ì— ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì¶”ê°€ë¡œ ì—°ê²°í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤.                |

### êµ¬í˜„

> RAG ìœ„ì£¼, FineTuning ê°„ë‹¨ êµ¬í˜„

---

## [ë¬¸ì„œí™”]

> XML â†’ ì˜ë¯¸ìˆëŠ” ìì—°ì–´ ë ˆì´ì–´(ë¬¸ì„œí™”) â†’ ì²­í¬/ì„ë² ë”©

### 1. ì™œ ë¬¸ì„œí™”ê°€ ìœ ë¦¬í•œê°€

- ì˜ë¯¸ ë‹¨ìœ„ ë³´ì¡´: XMLì€ í•„ë“œ/íƒœê·¸ ë‚˜ì—´(ë§¥ë½ ë¶€ì¡±). ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œ â€œì¦ê°/ë¹„êµ/ê¸°ê°„/ì‚¬ìœ â€ë¥¼ í•¨ê»˜ ì„œìˆ í•˜ë©´ ê²€ìƒ‰ ì ì¤‘ë¥ â†‘, ë‹µë³€ í’ˆì§ˆâ†‘.
- ì¤‘ë³µÂ·ë…¸ì´ì¦ˆ ì œê±°: íƒœê·¸ëª…/ì†ì„±/ë¶ˆí•„ìš” ê³µë°± ë“± ë¶ˆìš© í† í°ì„ ì¤„ì—¬ ì„ë² ë”© íš¨ìœ¨â†‘.
- ì¶œì²˜ ì¸ìš©ì´ ì‰¬ì›€: â€œDART, ì ‘ìˆ˜ë²ˆí˜¸, í˜ì´ì§€/ì„¹ì…˜â€ì„ **ë¬¸ì¥ê³¼** í•¨ê»˜ ê°™ì´ ì €ì¥í•˜ë‹ˆ ê·¼ê±° ì œì‹œ ì •í™•.
- ì§ˆë¬¸ ì í•©ì„±: ì‚¬ìš©ìê°€ ìì—°ì–´ë¡œ ë¬»ê¸° ë•Œë¬¸ì—, ì»¨í…ìŠ¤íŠ¸ë„ ìì—°ì–´ê°€ ë§ìŒ.

**ë”°ë¼ì„œ ë¬¸ì„œí™” ì—†ì´ ì›ë³¸ XML ê·¸ëŒ€ë¡œ ì„ë² ë”©í•˜ë©´**:
íƒœê·¸ê°€ ì„ì¸ í…ìŠ¤íŠ¸ê°€ ëŠ˜ì–´ì ¸ ê²€ìƒ‰ í’ˆì§ˆ ì €í•˜, ìˆ«ìë§Œ ë§ì€ ì²­í¬ëŠ” ë¬¸ë§¥ ë§¤ì¹­ ì•½í™”, ì‘ë‹µì— ê·¼ê±° ë¬¸êµ¬ê°€ ì–´ìƒ‰í•´ì§ˆ í™•ë¥ ì´ ë†’ìŠµë‹ˆë‹¤.

### 2. ETL - RAG

- E : Dart -> API -> XML -> ì•½ 5000ê°œ -> Google Drive / -> 1ê°œì˜ .jsonl (5000ë¼ì¸) ë¡œì»¬ì— ì €ì¥
- T : JSONL & XML -> ì •ê·œí™” -> ìì—°ì–´ ë¬¸ì„œí™” (yaml / md) -> ë¡œì»¬ ì €ì¥ (docs/{corp}/{year}/{reprt_code}/{rcept_no}.md) -> ì²­í¬ (1ì°¨/2ì°¨ ) + ë©”íƒ€ ë™ë´‰ -> .parquet íŒŒì¼ ìƒì„±
- L : Parquet & MD -> text ì„ë² ë”© -> pgvector -> (í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ìƒ‰ì¸) -> ë¬¸ì„œ CDN ì—…ë¡œë“œ ë³´ë¥˜ -> ì„œë¹™ (ì§ˆì˜ ë° ì‘ë‹µ)

### 3. ETL - FineTuning

-

---

## [Fine-Tuning]

### 1. ëª©ì 

- ì¦ê¶Œì‚¬ ì• ë„ë¦¬ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìš”ì•½/ì½”ë©˜íŠ¸ë¥¼ ì´ìš©í•´ì„œ ë¦¬ìŠ¤í¬/ë°¸ë¥˜ì—ì´ì…˜/ê°€ì´ë˜ìŠ¤ í•´ì„ í¬í•¨í•œ "ê³µì‹œ í•´ì„ ì§€ëŠ¥" í•™ìŠµ

### 2. Llama Factory

- ì°¸ê³  : [SK Tech ë¸”ë¡œê·¸](https://devocean.sk.com/blog/techBoardDetail.do?ID=166098)
- ì •ì˜ :  
  LLaMA FactoryëŠ” Metaì˜ LLaMA ë° ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ LLM(ëŒ€í˜•ì–¸ì–´ëª¨ë¸) ê³„ì—´ ëª¨ë¸ì„ ì‰½ê²Œ íŒŒì¸íŠœë‹, í”„ë¡¬í”„íŠ¸ ë¯¸ì„¸ì¡°ì •(Instruction Tuning), ì±„íŒ…/ì›¹UI ë°°í¬ê¹Œì§€ ì§€ì›í•˜ëŠ” ì˜¬ì¸ì› ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.  
  ê¸°ì¡´ HuggingFace Transformers ê¸°ë°˜ì´ë©°, ë‹¤ì–‘í•œ ë¯¸ì„¸ì¡°ì • ë°©ë²•(LoRA, QLoRA, P-Tuning ë“±), ë°ì´í„°ì…‹ í¬ë§· ì§€ì›, Web UI(ê·¸ë¼ë””ì˜¤ ê¸°ë°˜)ë¡œ GUI í™˜ê²½ì—ì„œ ì†ì‰½ê²Œ ì»¤ìŠ¤í…€ í•™ìŠµ/í…ŒìŠ¤íŠ¸/ë°°í¬ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
  ì¦‰, **LLM ëª¨ë¸ íŒŒì¸íŠœë‹ê³¼ ìš´ì˜**ì„ ì½”ë“œ ëª‡ ì¤„, í˜¹ì€ í´ë¦­ë§Œìœ¼ë¡œ ì§„í–‰í•  ìˆ˜ ìˆê²Œ ë•ëŠ” í•™ìŠµÂ·ì„œë¹™ìš© **í”„ë ˆì„ì›Œí¬**ì…ë‹ˆë‹¤.

#### [ì„¤ì¹˜]

- ë‹¤ì‹œ ì •ë¦¬ ì˜ˆì •

### íŒŒì¸íŠœë‹ ì˜µì…˜

| í•­ëª©                            | ê°’                        | ì„¤ëª…                                                   |
| ------------------------------- | ------------------------- | ------------------------------------------------------ |
| **Stage**                       | `sft`                     | Supervised Fine-Tuning ë‹¨ê³„                            |
| **Base Model**                  | `meta-llama/Llama-3.2-3B` | íŒŒì¸íŠœë‹í•  ê¸°ë³¸ ëª¨ë¸                                   |
| **Template**                    | `alpaca`                  | í”„ë¡¬í”„íŠ¸ í˜•ì‹ í…œí”Œë¦¿ (instruction, input, output êµ¬ì¡°) |
| **Finetuning Type**             | `lora`                    | PEFT(íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  íŠœë‹) ë°©ì‹                        |
| **Rank (r)**                    | `8`                       | ì €ë­í¬ ì°¨ì› ìˆ˜                                         |
| **Alpha**                       | `16`                      | ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜                                          |
| **Dropout**                     | `0.05`                    | LoRA ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨                                     |
| **Quantization Bit**            | `4`                       | 4bit ì–‘ìí™”ë¡œ GPU ë©”ëª¨ë¦¬ ì ˆì•½                          |
| **ë°©ì‹**                        | QLoRA                     | ì–‘ìí™”ëœ ëª¨ë¸ì— LoRA íŠœë‹ ì ìš©                         |
| **Batch Size**                  | `1`                       | GPU ë©”ëª¨ë¦¬ ì œì•½ ê³ ë ¤                                   |
| **Gradient Accumulation Steps** | `8`                       | ì‹¤ì§ˆì  ë°°ì¹˜ í¬ê¸° í™•ì¥                                  |
| **Learning Rate**               | `1e-4`                    | í•™ìŠµë¥                                                  |
| **Epochs**                      | `3`                       | í•™ìŠµ ë°˜ë³µ íšŸìˆ˜                                         |
| **Scheduler**                   | `cosine`                  | í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬                                        |
| **Warmup Ratio**                | `0.1`                     | ì „ì²´ ìŠ¤í…ì˜ 10%ëŠ” ì›Œë°ì—… êµ¬ê°„                          |
| **Eval Strategy**               | `steps`                   | ì¼ì • stepë§ˆë‹¤ í‰ê°€                                     |
| **Eval Steps / Save Steps**     | `500`                     | 500 stepë§ˆë‹¤ í‰ê°€ ë° ì €ì¥                              |
| **Save Total Limit**            | `3`                       | ì²´í¬í¬ì¸íŠ¸ ìµœëŒ€ 3ê°œ ìœ ì§€                               |
| **Load Best Model**             | `true`                    | eval_loss ê¸°ì¤€ìœ¼ë¡œ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°             |
| **Metric for Best Model**       | `eval_loss`               | ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (`greater_is_better=false`)              |
| **Precision**                   | `fp16`                    | Half-Precisionìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½                         |
| **Gradient Checkpointing**      | `true`                    | ê·¸ë˜ë””ì–¸íŠ¸ ìºì‹œë¡œ VRAM ì ˆê°                            |
| **Flash Attention**             | `fa2`                     | ê³ ì† Attention ì—°ì‚°                                    |
| **Packing**                     | `true`                    | ì‹œí€€ìŠ¤ íš¨ìœ¨ í–¥ìƒ                                       |
| **Num Workers**                 | `4`                       | DataLoader ë³‘ë ¬ ì²˜ë¦¬                                   |
| **Pin Memory**                  | `true`                    | ë°ì´í„° ë¡œë”© ì†ë„ í–¥ìƒ                                  |
| **Optimizer**                   | `adamw_torch`             | AdamW ìµœì í™”ê¸° ì‚¬ìš©                                    |
| **Seed**                        | `42`                      | ì¬í˜„ì„± í™•ë³´                                            |
| **Report**                      | `tensorboard`             | í•™ìŠµ ë¡œê·¸ ì‹œê°í™”                                       |

### íŒŒì¸íŠœë‹ ê²°ê³¼

```text
***** Running training *****
>>   Num examples = 955
>>   Num Epochs = 3
>>   Instantaneous batch size per device = 1
>>   Total train batch size (w. parallel, distributed & accumulation) = 8
>>   Gradient Accumulation steps = 8
>>   Total optimization steps = 360
>>   Number of trainable parameters = 12,156,928

***** train metrics *****
  epoch                    =         3.0
  total_flos               = 185635548GF
  train_loss               =      0.8458
  train_runtime            =  2:30:35.06
  train_samples_per_second =       0.317
  train_steps_per_second   =        0.04

***** eval metrics *****
  epoch                   =        3.0
  eval_loss               =     0.5694
  eval_runtime            = 0:01:40.81
  eval_samples_per_second =      1.061
  eval_steps_per_second   =      1.061
```

        output_dir: output/llama-3-8b-Instruct-bnb-4bit/qlora
        #logging_steps: 10
        #save_steps: 500
        plot_loss: true
        overwrite_output_dir: true

        per_device_train_batch_size: 1
        gradient_accumulation_steps: 4
        learning_rate: 1.0e-4
        num_train_epochs: 3.0
        lr_scheduler_type: cosine
        warmup_ratio: 0.1
        bf16: true
        #report_to: none

        seed: 42
        val_size: 0.1
        per_device_eval_batch_size: 1
        eval_strategy: steps
        eval_steps: 100


        do_eval: true
        #eval_strategy: steps
        #eval_steps: 100
        save_strategy: steps
        save_steps: 100
        logging_steps: 20

        load_best_model_at_end: true
        metric_for_best_model: "eval_loss"
        greater_is_better: false

        report_to: ["tensorboard"]
        resize_vocab: false
        upcast_layernorm: true
        ```

7. í•™ìŠµ ì‹œì‘

   - CLI

     ```bash
     # ë‚´ ê²½ë¡œ í™•ì¸ (/workspace/LLaMA-Factory/)
     pwd

     # (ì„ íƒ) ìºì‹œ ê²½ë¡œ ê³ ì •í•´ë‘ë©´ ì¬ì‚¬ìš©ì— ì¢‹ì•„ìš”
     export HF_HOME=/workspace/.cache/huggingface

     # í•™ìŠµ ì‹œì‘
     llamafactory-cli train config/llama3-8b-instruct-bnb-4bit-unsloth.yaml
     ```

   - Config ì¶”ì²œ ì„¤ì •

     ```yaml
     #.yaml íŒŒì¼
     # í‰ê°€/ì €ì¥/ë¡œê¹… ì „ëµ
     do_eval: true
     eval_strategy: steps # ë˜ëŠ” "epoch"
     eval_steps: 100 # ë°ì´í„°/ìŠ¤í… ê·œëª¨ì— ë§ê²Œ
     save_strategy: steps
     save_steps: 100
     logging_steps: 20

     # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥
     load_best_model_at_end: true
     metric_for_best_model: "eval_loss"
     greater_is_better: false

     # ë¡œê¹… ë°±ì—”ë“œ(ì„ íƒ)
     report_to: ["tensorboard"] # ë˜ëŠ” ["wandb"] ì‚¬ìš© ì‹œ WANDB_API_KEY í•„ìš”

     # í† í¬ë‚˜ì´ì € ê²½ê³  ëŒ€ì‘
     resize_vocab: true

     #(ê¶Œì¥) 4bit í•™ìŠµ ì•ˆì •í™”
     upcast_layernorm: true
     ```

   - Output í™•ì¸
     ```python
     # ì»¤ë§¨ë“œ
     ls -al {configì— ì§€ì •í•œ ouput_dir}
     # ì˜ˆì‹œ
     ls -al /output/llama-3-8b-Instruct-bnb-4bit/qlora
     ```

8. ì¶”ë¡ 

   ```bash
   # ì¸ì ì„¤ì •
   llamafactory-cli chat \
   --model_name_or_path="unsloth/llama-3-8b-Instruct-bnb-4bit" \
   --adapter_name_or_path="output/llama-3-8b-Instruct-bnb-4bit/qlora" \
   --template="llama3" \
   --finetuning_type="lora" \
   --quantization_bit=4 \
   --temperature=0

   # í…ŒìŠ¤íŠ¸ í›„ íˆìŠ¤í† ë¦¬ ì œê±°
   clear

   # ì±— ì¢…ë£Œ
   exit
   ```

9. ëª¨ë¸ ì €ì¥

   - í•™ìŠµëœ Lora adapterë¥¼ base ëª¨ë¸ê³¼ í•©ì³ ì €ì¥
   - ì €ì¥ íŒŒë¼ë¯¸í„°
     - model_name_or_path : base ëª¨ë¸ì„ ì§€ì •, í•™ìŠµ íš¨ìœ¨í™”ë¥¼ ìœ„í•´ ì‚¬ìš©í–ˆë˜ ì–‘ìí™” ëª¨ë¸ì´ ì•„ë‹Œ ì›ë˜ ëª¨ë¸ì„ ì§€ì •
     - adapter_name_or_path: Lora adapterê°€ ì €ì¥ëœ ìœ„ì¹˜
     - template : ëª¨ë¸ì˜ í˜•ì‹
     - finetuning_type: íŒŒì¸íŠœë‹ ë°©ë²•
     - export_dir: ë³‘í•©ëœ ëª¨ë¸ì´ ì €ì¥ë  ìœ„ì¹˜
     - export_size: ëª¨ë¸ì˜ í° ê²½ìš° ë¶„í• ë  í¬ê¸° (GB)
     - export_device: ëª¨ë¸ ë³‘í•©ì„ ì²˜ë¦¬í•  ë””ë°”ì´ìŠ¤ ì§€ì • (cpu and cuda)
     - export_hub_model_id : huggingfaceì— ì—…ë¡œë“œ í•  ê²½ìš° ì•„ì´ë””
   - CLI

     ```bash
     # llama3 ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— ë¼ì´ì„¼ìŠ¤ ë™ì˜
     huggingface-cli login

     # Lora Adapterë¥¼ ë³‘í•©
     llamafactory-cli export \
     --model_name_or_path="meta-llama/Meta-Llama-3-8B-Instruct" \
     --adapter_name_or_path="output/llama-3-8b-Instruct-bnb-4bit/qlora" \
     --template="llama3" \
     --finetuning_type="lora" \
     --export_dir="/output/Meta-Llama-3-8B-Instruct" \
     --export_size=2 \
     --export_device="cpu"

     # ë¶„í•  ì €ì¥ í™•ì¸
     ls -al output/Meta-Llama-3-8B-Instruct/
     ```

10. HuggingFace ëª¨ë¸ ì—…ë¡œë“œ
    - ê°•ì˜ ìë£Œ ì°¸ê³ 

---

## [ë­ê·¸ë˜í”„]

- ì „ì²´ ì—°ë™ ê°œë…ë„
  ```text
  [ Streamlit UI ]
      â†“ (user_level session_state)
  [ LangGraph App ]
      â†“
  [ Router Node ]
  â””â”€â”€ ë ˆë²¨ë³„ íŒŒë¼ë¯¸í„° (top_k, context_len)
  [ Generate Node ]
  â””â”€â”€ PROMPT_TEMPLATES[level] ê¸°ë°˜ ì‹œìŠ¤í…œ/ìœ ì € í”„ë¡¬í”„íŠ¸ êµ¬ì„±
      â†“
  [ FT ëª¨ë¸ + pgvector ê²€ìƒ‰ ]
      â†“
  [ ê²°ê³¼ + ref ë°˜í™˜ ]
  ```
- ì°¸ê³  : ![[graph/readme.md]]
