####################################
# llama-factory 학습 설정 파일
####################################

# 기본 모델 설정
model_name_or_path: meta-llama/Llama-3.2-3B           # 사용할 베이스 모델 (허깅페이스 경로)
adapter_name_or_path: null                            # LoRA를 처음 학습할 때는 null
template: alpaca                                      # 데이터 형식 (instruction/input/output 구조)
finetuning_type: lora                                 # 미세조정 방식 (LoRA / QLoRA 가능)

# 데이터 설정
dataset_dir: ./dataset                                # 학습 데이터 폴더 경로
train_file: train.json                                # 학습용 JSON 파일
test_file: test.json                                  # 검증용 JSON 파일
cutoff_len: 1024                                      # 토큰 최대 길이

# 학습 하이퍼파라미터
num_train_epochs: 3                                   # 전체 학습 반복 횟수 (Epoch)
per_device_train_batch_size: 2                        # GPU당 배치 사이즈
gradient_accumulation_steps: 8                        # 메모리 부족 시 그래디언트 누적
learning_rate: 2e-4                                   # 학습률
lr_scheduler_type: cosine                             # 학습률 스케줄러
warmup_ratio: 0.03                                    # 워밍업 비율
logging_steps: 10                                     # 로그 출력 주기

# 저장 경로
output_dir: ./service/fine_tuning/llama_factory/output_lora        # 학습 결과 저장 폴더
save_steps: 500                                       # 모델 저장 주기
save_total_limit: 3                                   # 저장할 체크포인트 개수

# 하드웨어 설정
fp16: true                                            # GPU FP16(half precision) 사용
gradient_checkpointing: true                          # GPU 메모리 절약 옵션
