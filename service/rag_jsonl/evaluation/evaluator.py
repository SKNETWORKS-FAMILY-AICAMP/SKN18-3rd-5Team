#!/usr/bin/env python3
"""
RAG ì‹œìŠ¤í…œ í‰ê°€ê¸°
evaluation_queries.jsonì„ ì‚¬ìš©í•œ ìë™ í‰ê°€
"""

import json
import time
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict

from .metrics import MetricsCalculator, EvaluationMetrics, RetrievalMetrics, GenerationMetrics
from ..rag_system import RAGSystem
from ..models.config import EmbeddingModelType
import psycopg2
from psycopg2.extras import RealDictCursor
import os

logger = logging.getLogger(__name__)


@dataclass
class EvaluationConfig:
    """í‰ê°€ ì„¤ì •"""
    queries_path: str = "service/rag_jsonl/cli/evaluation_queries.json"
    top_k: int = 5
    enable_generation: bool = False
    model_type: EmbeddingModelType = EmbeddingModelType.MULTILINGUAL_E5_SMALL
    output_dir: str = "service/rag_jsonl/results"
    save_detailed_results: bool = True
    save_to_db: bool = True  # DB ì €ì¥ ì—¬ë¶€
    db_config: Optional[Dict[str, str]] = None  # DB ì„¤ì •


class RAGEvaluator:
    """RAG ì‹œìŠ¤í…œ í‰ê°€ê¸°"""
    
    def __init__(
        self, 
        rag_system: Optional[RAGSystem] = None,
        config: Optional[EvaluationConfig] = None
    ):
        """
        Args:
            rag_system: í‰ê°€í•  RAG ì‹œìŠ¤í…œ (Noneì´ë©´ ìƒˆë¡œ ìƒì„±)
            config: í‰ê°€ ì„¤ì •
        """
        self.config = config or EvaluationConfig()
        self.metrics_calculator = MetricsCalculator()
        
        # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if rag_system:
            self.rag_system = rag_system
        else:
            self.rag_system = RAGSystem(
                model_type=self.config.model_type,
                db_config=self.config.db_config.get_db_config() if hasattr(self.config.db_config, 'get_db_config') else self.config.db_config,
                enable_generation=self.config.enable_generation
            )
        
        # í‰ê°€ ì¿¼ë¦¬ ë¡œë“œ
        self.evaluation_queries = self._load_evaluation_queries()
        
        # DB ì—°ê²° (í‰ê°€ ê²°ê³¼ ì €ì¥ìš©)
        self.db_conn = None
        if self.config.save_to_db:
            self._connect_db()
        
        logger.info(f"RAG Evaluator ì´ˆê¸°í™” ì™„ë£Œ: {len(self.evaluation_queries)}ê°œ ì¿¼ë¦¬")
    
    def _load_evaluation_queries(self) -> List[Dict[str, Any]]:
        """í‰ê°€ ì¿¼ë¦¬ ë¡œë“œ"""
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ê³„ì‚°
        project_root = Path(__file__).parents[3]  # service/rag_jsonl/evaluation/ -> project_root
        queries_path = project_root / self.config.queries_path
        
        try:
            with open(queries_path, 'r', encoding='utf-8') as f:
                queries = json.load(f)
            logger.info(f"í‰ê°€ ì¿¼ë¦¬ ë¡œë“œ ì™„ë£Œ: {len(queries)}ê°œ")
            return queries
        except FileNotFoundError:
            logger.error(f"í‰ê°€ ì¿¼ë¦¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {queries_path}")
            return []
        except json.JSONDecodeError as e:
            logger.error(f"í‰ê°€ ì¿¼ë¦¬ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []
    
    def _connect_db(self):
        """DB ì—°ê²°"""
        try:
            db_config = self.config.db_config or {
                'host': os.getenv('PG_HOST', 'localhost'),
                'port': os.getenv('PG_PORT', '5432'),
                'database': os.getenv('PG_DB', 'postgres'),
                'user': os.getenv('PG_USER', 'postgres'),
                'password': os.getenv('PG_PASSWORD', 'postgres')
            }
            
            self.db_conn = psycopg2.connect(**db_config)
            logger.info("í‰ê°€ ê²°ê³¼ DB ì—°ê²° ì„±ê³µ")
            
        except Exception as e:
            logger.warning(f"í‰ê°€ ê²°ê³¼ DB ì—°ê²° ì‹¤íŒ¨: {e}. DB ì €ì¥ ë¹„í™œì„±í™”.")
            self.db_conn = None
            self.config.save_to_db = False
    
    def _save_to_db(self, result: EvaluationMetrics, model_name: str):
        """í‰ê°€ ê²°ê³¼ë¥¼ DBì— ì €ì¥"""
        if not self.db_conn:
            return
        
        try:
            cursor = self.db_conn.cursor()
            
            sql = """
                INSERT INTO vector_db.evaluation_results (
                    model_name, query_id, query_text, query_type, difficulty,
                    recall_at_k, precision_at_k, mrr, ndcg_at_k, 
                    avg_similarity, keyword_coverage,
                    exact_match, keyword_f1, keyword_precision, keyword_recall,
                    contains_ground_truth,
                    overall_score, response_time_ms, retrieved_docs_count,
                    error_message
                ) VALUES (
                    %s, %s, %s, %s, %s,
                    %s, %s, %s, %s,
                    %s, %s,
                    %s, %s, %s, %s,
                    %s,
                    %s, %s, %s,
                    %s
                )
            """
            
            values = (
                model_name,
                result.query_id,
                result.query,
                result.query_type,
                result.difficulty,
                result.retrieval.recall_at_k,
                result.retrieval.precision_at_k,
                result.retrieval.mrr,
                result.retrieval.ndcg_at_k,
                result.retrieval.avg_similarity,
                result.retrieval.keyword_coverage,
                result.generation.exact_match if result.generation else None,
                result.generation.keyword_f1 if result.generation else None,
                result.generation.keyword_precision if result.generation else None,
                result.generation.keyword_recall if result.generation else None,
                result.generation.contains_ground_truth if result.generation else None,
                result.overall_score,
                result.response_time_ms,
                result.retrieved_docs_count,
                result.error_message
            )
            
            cursor.execute(sql, values)
            self.db_conn.commit()
            cursor.close()
            
            logger.debug(f"í‰ê°€ ê²°ê³¼ DB ì €ì¥ ì™„ë£Œ: {result.query_id}")
            
        except Exception as e:
            logger.error(f"í‰ê°€ ê²°ê³¼ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            self.db_conn.rollback()
    
    def evaluate_all(self) -> List[EvaluationMetrics]:
        """ëª¨ë“  ì¿¼ë¦¬ í‰ê°€"""
        results = []
        
        logger.info(f"ì „ì²´ {len(self.evaluation_queries)}ê°œ ì¿¼ë¦¬ í‰ê°€ ì‹œì‘")
        
        for i, query_data in enumerate(self.evaluation_queries, 1):
            logger.info(f"[{i}/{len(self.evaluation_queries)}] í‰ê°€ ì¤‘: {query_data['query']}")
            
            try:
                result = self.evaluate_single_query(query_data)
                results.append(result)
                
                # DBì— ì €ì¥
                if self.config.save_to_db:
                    self._save_to_db(result, self.config.model_type.value)
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if i % 3 == 0 or i == len(self.evaluation_queries):
                    logger.info(f"ì§„í–‰ë¥ : {i}/{len(self.evaluation_queries)} ({i/len(self.evaluation_queries)*100:.1f}%)")
                    
            except Exception as e:
                logger.error(f"ì¿¼ë¦¬ {query_data['query_id']} í‰ê°€ ì‹¤íŒ¨: {e}")
                # ì—ëŸ¬ ê²°ê³¼ ìƒì„±
                error_result = EvaluationMetrics(
                    query_id=query_data['query_id'],
                    query=query_data['query'],
                    query_type=query_data.get('query_type', 'unknown'),
                    difficulty=query_data.get('difficulty', 'unknown'),
                    retrieval=RetrievalMetrics(),
                    error_message=str(e)
                )
                results.append(error_result)
        
        # ê²°ê³¼ ì €ì¥
        if self.config.save_detailed_results:
            self._save_results(results)
        
        return results
    
    def evaluate_single_query(self, query_data: Dict[str, Any]) -> EvaluationMetrics:
        """ë‹¨ì¼ ì¿¼ë¦¬ í‰ê°€"""
        start_time = time.time()
        
        query = query_data['query']
        expected_keywords = query_data.get('expected_keywords', [])
        expected_docs = query_data.get('expected_docs', [])
        ground_truth = query_data.get('ground_truth_answer', '')
        
        # RAG ì‹œìŠ¤í…œìœ¼ë¡œ ì¿¼ë¦¬ ì‹¤í–‰
        try:
            if self.config.enable_generation:
                rag_response = self.rag_system.query(query)
                retrieved_docs = rag_response.retrieved_documents
                generated_answer = rag_response.generated_answer.text if rag_response.generated_answer else ""
            else:
                # ê²€ìƒ‰ë§Œ ìˆ˜í–‰
                retrieved_docs = self.rag_system.search_only(query, top_k=self.config.top_k)
                generated_answer = ""
        except Exception as e:
            logger.error(f"RAG ì‹œìŠ¤í…œ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
            raise
        
        response_time = (time.time() - start_time) * 1000  # ms
        
        # ê²€ìƒ‰ ë©”íŠ¸ë¦­ ê³„ì‚°
        retrieval_metrics = self.metrics_calculator.calculate_retrieval_metrics(
            query=query,
            retrieved_docs=retrieved_docs,
            expected_keywords=expected_keywords,
            expected_docs=expected_docs,
            k=self.config.top_k
        )
        
        # ìƒì„± ë©”íŠ¸ë¦­ ê³„ì‚° (ë‹µë³€ ìƒì„±ì´ í™œì„±í™”ëœ ê²½ìš°)
        generation_metrics = None
        if self.config.enable_generation and generated_answer:
            generation_metrics = self.metrics_calculator.calculate_generation_metrics(
                generated_answer=generated_answer,
                ground_truth_answer=ground_truth,
                expected_keywords=expected_keywords
            )
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        overall_score = self._calculate_overall_score(retrieval_metrics, generation_metrics)
        
        return EvaluationMetrics(
            query_id=query_data['query_id'],
            query=query,
            query_type=query_data.get('query_type', 'unknown'),
            difficulty=query_data.get('difficulty', 'unknown'),
            retrieval=retrieval_metrics,
            generation=generation_metrics,
            overall_score=overall_score,
            response_time_ms=response_time,
            retrieved_docs_count=len(retrieved_docs)
        )
    
    def _calculate_overall_score(
        self, 
        retrieval: RetrievalMetrics, 
        generation: Optional[GenerationMetrics]
    ) -> float:
        """ì „ì²´ ì ìˆ˜ ê³„ì‚° (0-100)"""
        
        # ê²€ìƒ‰ ì ìˆ˜ (70% ê°€ì¤‘ì¹˜)
        retrieval_score = (
            retrieval.recall_at_k * 0.3 +
            retrieval.precision_at_k * 0.3 +
            retrieval.mrr * 0.2 +
            retrieval.keyword_coverage * 0.2
        ) * 70
        
        # ìƒì„± ì ìˆ˜ (30% ê°€ì¤‘ì¹˜, ìƒì„±ì´ í™œì„±í™”ëœ ê²½ìš°ë§Œ)
        generation_score = 0.0
        if generation:
            generation_score = (
                generation.keyword_f1 * 0.5 +
                (1.0 if generation.contains_ground_truth else 0.0) * 0.3 +
                (1.0 if generation.exact_match else 0.0) * 0.2
            ) * 30
        
        return retrieval_score + generation_score
    
    def _save_results(self, results: List[EvaluationMetrics]):
        """í‰ê°€ ê²°ê³¼ ì €ì¥"""
        output_dir = Path(self.config.output_dir)
        output_dir.mkdir(exist_ok=True)
        
        # ìƒì„¸ ê²°ê³¼ (JSON)
        detailed_results = [asdict(result) for result in results]
        detailed_path = output_dir / f"detailed_results_{int(time.time())}.json"
        
        with open(detailed_path, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ìƒì„¸ ê²°ê³¼ ì €ì¥: {detailed_path}")
        
        # ìš”ì•½ ë¦¬í¬íŠ¸ (í…ìŠ¤íŠ¸)
        summary_path = output_dir / f"summary_report_{int(time.time())}.txt"
        self._generate_summary_report(results, summary_path)
        
        logger.info(f"ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥: {summary_path}")
    
    def _generate_summary_report(self, results: List[EvaluationMetrics], output_path: Path):
        """ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("RAG ì‹œìŠ¤í…œ í‰ê°€ ë¦¬í¬íŠ¸\n")
            f.write("=" * 80 + "\n\n")
            
            # ì „ì²´ í†µê³„
            f.write("ğŸ“Š ì „ì²´ í†µê³„\n")
            f.write("-" * 40 + "\n")
            
            valid_results = [r for r in results if not r.error_message]
            error_count = len(results) - len(valid_results)
            
            f.write(f"ì´ ì¿¼ë¦¬ ìˆ˜: {len(results)}\n")
            f.write(f"ì„±ê³µí•œ ì¿¼ë¦¬: {len(valid_results)}\n")
            f.write(f"ì‹¤íŒ¨í•œ ì¿¼ë¦¬: {error_count}\n")
            
            if valid_results:
                avg_score = sum(r.overall_score for r in valid_results) / len(valid_results)
                avg_response_time = sum(r.response_time_ms for r in valid_results) / len(valid_results)
                
                f.write(f"í‰ê·  ì ìˆ˜: {avg_score:.2f}/100\n")
                f.write(f"í‰ê·  ì‘ë‹µì‹œê°„: {avg_response_time:.0f}ms\n\n")
                
                # ê²€ìƒ‰ ë©”íŠ¸ë¦­ í‰ê· 
                f.write("ğŸ” ê²€ìƒ‰ í’ˆì§ˆ (í‰ê· )\n")
                f.write("-" * 40 + "\n")
                
                avg_recall = sum(r.retrieval.recall_at_k for r in valid_results) / len(valid_results)
                avg_precision = sum(r.retrieval.precision_at_k for r in valid_results) / len(valid_results)
                avg_mrr = sum(r.retrieval.mrr for r in valid_results) / len(valid_results)
                avg_keyword_coverage = sum(r.retrieval.keyword_coverage for r in valid_results) / len(valid_results)
                
                f.write(f"Recall@{self.config.top_k}: {avg_recall:.3f}\n")
                f.write(f"Precision@{self.config.top_k}: {avg_precision:.3f}\n")
                f.write(f"MRR: {avg_mrr:.3f}\n")
                f.write(f"í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€: {avg_keyword_coverage:.3f}\n\n")
                
                # ë‚œì´ë„ë³„ ì„±ëŠ¥
                f.write("ğŸ“ˆ ë‚œì´ë„ë³„ ì„±ëŠ¥\n")
                f.write("-" * 40 + "\n")
                
                difficulty_stats = {}
                for result in valid_results:
                    diff = result.difficulty
                    if diff not in difficulty_stats:
                        difficulty_stats[diff] = []
                    difficulty_stats[diff].append(result.overall_score)
                
                for difficulty, scores in difficulty_stats.items():
                    avg_score = sum(scores) / len(scores)
                    f.write(f"{difficulty}: {avg_score:.2f} ({len(scores)}ê°œ ì¿¼ë¦¬)\n")
                
                f.write("\n")
                
                # ê°œë³„ ì¿¼ë¦¬ ê²°ê³¼
                f.write("ğŸ“‹ ê°œë³„ ì¿¼ë¦¬ ê²°ê³¼\n")
                f.write("-" * 40 + "\n")
                
                for result in valid_results:
                    f.write(f"[{result.query_id}] {result.query}\n")
                    f.write(f"  ì ìˆ˜: {result.overall_score:.1f}/100\n")
                    f.write(f"  ê²€ìƒ‰: Recall={result.retrieval.recall_at_k:.3f}, "
                           f"Precision={result.retrieval.precision_at_k:.3f}\n")
                    f.write(f"  ì‘ë‹µì‹œê°„: {result.response_time_ms:.0f}ms\n")
                    
                    if result.generation:
                        f.write(f"  ìƒì„±: F1={result.generation.keyword_f1:.3f}, "
                               f"ì •í™•ì¼ì¹˜={result.generation.exact_match}\n")
                    f.write("\n")
            
            # ì—ëŸ¬ ì¿¼ë¦¬
            if error_count > 0:
                f.write("âŒ ì‹¤íŒ¨í•œ ì¿¼ë¦¬\n")
                f.write("-" * 40 + "\n")
                for result in results:
                    if result.error_message:
                        f.write(f"[{result.query_id}] {result.query}\n")
                        f.write(f"  ì—ëŸ¬: {result.error_message}\n\n")
    
    def compare_models(
        self, 
        model_types: List[EmbeddingModelType],
        output_dir: str = "model_comparison"
    ) -> Dict[str, List[EvaluationMetrics]]:
        """ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ í‰ê°€"""
        
        logger.info(f"ëª¨ë¸ ë¹„êµ í‰ê°€ ì‹œì‘: {len(model_types)}ê°œ ëª¨ë¸")
        
        comparison_results = {}
        
        for model_type in model_types:
            logger.info(f"ëª¨ë¸ í‰ê°€ ì¤‘: {model_type.value}")
            
            # ìƒˆë¡œìš´ RAG ì‹œìŠ¤í…œ ìƒì„±
            rag_system = RAGSystem(
                model_type=model_type,
                enable_generation=self.config.enable_generation
            )
            
            # í‰ê°€ê¸° ìƒì„± ë° ì‹¤í–‰
            evaluator = RAGEvaluator(rag_system, self.config)
            results = evaluator.evaluate_all()
            
            comparison_results[model_type.value] = results
        
        # ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
        self._generate_comparison_report(comparison_results, output_dir)
        
        return comparison_results
    
    def _generate_comparison_report(
        self, 
        comparison_results: Dict[str, List[EvaluationMetrics]],
        output_dir: str
    ):
        """ëª¨ë¸ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        report_path = output_path / f"model_comparison_{int(time.time())}.txt"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("RAG ëª¨ë¸ ë¹„êµ ë¦¬í¬íŠ¸\n")
            f.write("=" * 80 + "\n\n")
            
            # ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½
            f.write("ğŸ“Š ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½\n")
            f.write("-" * 60 + "\n")
            f.write(f"{'ëª¨ë¸':<30} {'í‰ê· ì ìˆ˜':<10} {'í‰ê· ì‘ë‹µì‹œê°„':<15} {'Recall@5':<10}\n")
            f.write("-" * 60 + "\n")
            
            for model_name, results in comparison_results.items():
                valid_results = [r for r in results if not r.error_message]
                if valid_results:
                    avg_score = sum(r.overall_score for r in valid_results) / len(valid_results)
                    avg_time = sum(r.response_time_ms for r in valid_results) / len(valid_results)
                    avg_recall = sum(r.retrieval.recall_at_k for r in valid_results) / len(valid_results)
                    
                    f.write(f"{model_name:<30} {avg_score:<10.2f} {avg_time:<15.0f} {avg_recall:<10.3f}\n")
            
            f.write("\n")
            
            # ì¿¼ë¦¬ë³„ ìƒì„¸ ë¹„êµ
            f.write("ğŸ“‹ ì¿¼ë¦¬ë³„ ìƒì„¸ ë¹„êµ\n")
            f.write("-" * 60 + "\n")
            
            # ì²« ë²ˆì§¸ ëª¨ë¸ì˜ ì¿¼ë¦¬ ìˆœì„œë¥¼ ê¸°ì¤€ìœ¼ë¡œ
            first_model = list(comparison_results.keys())[0]
            first_results = comparison_results[first_model]
            
            for result in first_results:
                if result.error_message:
                    continue
                    
                f.write(f"[{result.query_id}] {result.query}\n")
                f.write("-" * 40 + "\n")
                
                for model_name, results in comparison_results.items():
                    model_result = next((r for r in results if r.query_id == result.query_id), None)
                    if model_result and not model_result.error_message:
                        f.write(f"{model_name}: {model_result.overall_score:.1f}ì  "
                               f"(Recall={model_result.retrieval.recall_at_k:.3f}, "
                               f"ì‘ë‹µì‹œê°„={model_result.response_time_ms:.0f}ms)\n")
                
                f.write("\n")
        
        logger.info(f"ëª¨ë¸ ë¹„êµ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
