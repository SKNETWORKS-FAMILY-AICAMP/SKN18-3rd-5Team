#!/usr/bin/env python3
"""
RAG ì‹œìŠ¤í…œ í‰ê°€ CLI
evaluation_queries.jsonì„ ì‚¬ìš©í•œ ìë™ í‰ê°€ ì‹¤í–‰
"""

import sys
import argparse
import logging
from pathlib import Path
from typing import List

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from service.rag.evaluation import RAGEvaluator, EvaluationConfig
from service.rag.models.config import EmbeddingModelType

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="RAG ì‹œìŠ¤í…œ í‰ê°€ ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê¸°ë³¸ í‰ê°€ (E5 ëª¨ë¸, ê²€ìƒ‰ë§Œ)
  python evaluate_rag.py

  # ë‹µë³€ ìƒì„± í¬í•¨ í‰ê°€
  python evaluate_rag.py --enable-generation

  # íŠ¹ì • ëª¨ë¸ë¡œ í‰ê°€
  python evaluate_rag.py --model kakaobank

  # ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ
  python evaluate_rag.py --compare-models e5 kakaobank fine5

  # íŠ¹ì • ì¿¼ë¦¬ë§Œ í‰ê°€
  python evaluate_rag.py --query-ids Q001 Q002

  # ë‚œì´ë„ë³„ í•„í„°ë§
  python evaluate_rag.py --difficulty easy medium
        """
    )
    
    # ê¸°ë³¸ ì˜µì…˜
    parser.add_argument(
        "--model", 
        choices=["e5", "kakaobank", "fine5"],
        default="e5",
        help="ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ (ê¸°ë³¸ê°’: e5)"
    )
    
    parser.add_argument(
        "--enable-generation",
        action="store_true",
        help="ë‹µë³€ ìƒì„± í™œì„±í™” (LLM í•„ìš”)"
    )
    
    parser.add_argument(
        "--top-k",
        type=int,
        default=5,
        help="ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’: 5)"
    )
    
    # í•„í„°ë§ ì˜µì…˜
    parser.add_argument(
        "--query-ids",
        nargs="+",
        help="í‰ê°€í•  ì¿¼ë¦¬ ID ëª©ë¡ (ì˜ˆ: Q001 Q002)"
    )
    
    parser.add_argument(
        "--difficulty",
        nargs="+",
        choices=["easy", "medium", "hard"],
        help="í‰ê°€í•  ë‚œì´ë„ í•„í„°"
    )
    
    parser.add_argument(
        "--query-type",
        nargs="+",
        help="í‰ê°€í•  ì¿¼ë¦¬ íƒ€ì… í•„í„°"
    )
    
    # ë¹„êµ í‰ê°€ ì˜µì…˜
    parser.add_argument(
        "--compare-models",
        nargs="+",
        choices=["e5", "kakaobank", "fine5"],
        help="ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ í‰ê°€"
    )
    
    # ì¶œë ¥ ì˜µì…˜
    parser.add_argument(
        "--output-dir",
        default="evaluation_results",
        help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: evaluation_results)"
    )
    
    parser.add_argument(
        "--no-save",
        action="store_true",
        help="ìƒì„¸ ê²°ê³¼ ì €ì¥í•˜ì§€ ì•ŠìŒ"
    )
    
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥"
    )
    
    args = parser.parse_args()
    
    # ë¡œê¹… ë ˆë²¨ ì„¤ì •
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # ëª¨ë¸ íƒ€ì… ë§¤í•‘
    model_mapping = {
        "e5": EmbeddingModelType.MULTILINGUAL_E5_SMALL,
        "kakaobank": EmbeddingModelType.KAKAOBANK_DEBERTA,
        "fine5": EmbeddingModelType.FINE5_FINANCE
    }
    
    try:
        if args.compare_models:
            # ëª¨ë¸ ë¹„êµ í‰ê°€
            run_model_comparison(args, model_mapping)
        else:
            # ë‹¨ì¼ ëª¨ë¸ í‰ê°€
            run_single_evaluation(args, model_mapping)
            
    except KeyboardInterrupt:
        logger.info("í‰ê°€ê°€ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        logger.error(f"í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


def run_single_evaluation(args, model_mapping):
    """ë‹¨ì¼ ëª¨ë¸ í‰ê°€ ì‹¤í–‰"""
    model_type = model_mapping[args.model]
    
    logger.info(f"RAG í‰ê°€ ì‹œì‘ - ëª¨ë¸: {args.model}")
    logger.info(f"ë‹µë³€ ìƒì„±: {'í™œì„±í™”' if args.enable_generation else 'ë¹„í™œì„±í™”'}")
    logger.info(f"ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜: {args.top_k}")
    
    # í‰ê°€ ì„¤ì •
    config = EvaluationConfig(
        top_k=args.top_k,
        enable_generation=args.enable_generation,
        model_type=model_type,
        output_dir=args.output_dir,
        save_detailed_results=not args.no_save
    )
    
    # í‰ê°€ê¸° ìƒì„±
    evaluator = RAGEvaluator(config=config)
    
    # ì¿¼ë¦¬ í•„í„°ë§
    filtered_queries = filter_queries(evaluator.evaluation_queries, args)
    
    if not filtered_queries:
        logger.warning("í•„í„° ì¡°ê±´ì— ë§ëŠ” ì¿¼ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    logger.info(f"í‰ê°€í•  ì¿¼ë¦¬ ìˆ˜: {len(filtered_queries)}")
    
    # í‰ê°€ ì‹¤í–‰
    results = []
    for i, query_data in enumerate(filtered_queries, 1):
        logger.info(f"[{i}/{len(filtered_queries)}] {query_data['query_id']}: {query_data['query']}")
        
        result = evaluator.evaluate_single_query(query_data)
        results.append(result)
        
        # ì¦‰ì‹œ ê²°ê³¼ ì¶œë ¥
        print(f"  ì ìˆ˜: {result.overall_score:.1f}/100")
        print(f"  ê²€ìƒ‰: Recall={result.retrieval.recall_at_k:.3f}, Precision={result.retrieval.precision_at_k:.3f}")
        print(f"  ì‘ë‹µì‹œê°„: {result.response_time_ms:.0f}ms")
        if result.error_message:
            print(f"  ì—ëŸ¬: {result.error_message}")
        print()
    
    # ìš”ì•½ í†µê³„
    print_summary(results)


def run_model_comparison(args, model_mapping):
    """ëª¨ë¸ ë¹„êµ í‰ê°€ ì‹¤í–‰"""
    model_types = [model_mapping[model] for model in args.compare_models]
    
    logger.info(f"ëª¨ë¸ ë¹„êµ í‰ê°€ ì‹œì‘: {args.compare_models}")
    
    # í‰ê°€ ì„¤ì •
    config = EvaluationConfig(
        top_k=args.top_k,
        enable_generation=args.enable_generation,
        output_dir=args.output_dir,
        save_detailed_results=not args.no_save
    )
    
    # ì²« ë²ˆì§¸ ëª¨ë¸ë¡œ í‰ê°€ê¸° ìƒì„± (ì¿¼ë¦¬ ë¡œë“œìš©)
    evaluator = RAGEvaluator(config=config)
    
    # ì¿¼ë¦¬ í•„í„°ë§
    filtered_queries = filter_queries(evaluator.evaluation_queries, args)
    
    if not filtered_queries:
        logger.warning("í•„í„° ì¡°ê±´ì— ë§ëŠ” ì¿¼ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    logger.info(f"ë¹„êµí•  ì¿¼ë¦¬ ìˆ˜: {len(filtered_queries)}")
    
    # ëª¨ë¸ë³„ í‰ê°€
    comparison_results = {}
    
    for model_name in args.compare_models:
        logger.info(f"ëª¨ë¸ í‰ê°€ ì¤‘: {model_name}")
        
        model_type = model_mapping[model_name]
        config.model_type = model_type
        
        # ìƒˆë¡œìš´ í‰ê°€ê¸° ìƒì„±
        model_evaluator = RAGEvaluator(config=config)
        
        # í‰ê°€ ì‹¤í–‰
        model_results = []
        for query_data in filtered_queries:
            result = model_evaluator.evaluate_single_query(query_data)
            model_results.append(result)
        
        comparison_results[model_name] = model_results
    
    # ë¹„êµ ê²°ê³¼ ì¶œë ¥
    print_comparison_summary(comparison_results)


def filter_queries(queries, args):
    """ì¿¼ë¦¬ í•„í„°ë§"""
    filtered = queries.copy()
    
    # ì¿¼ë¦¬ ID í•„í„°
    if args.query_ids:
        filtered = [q for q in filtered if q['query_id'] in args.query_ids]
    
    # ë‚œì´ë„ í•„í„°
    if args.difficulty:
        filtered = [q for q in filtered if q.get('difficulty') in args.difficulty]
    
    # ì¿¼ë¦¬ íƒ€ì… í•„í„°
    if args.query_type:
        filtered = [q for q in filtered if q.get('query_type') in args.query_type]
    
    return filtered


def print_summary(results):
    """í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    valid_results = [r for r in results if not r.error_message]
    error_count = len(results) - len(valid_results)
    
    print("\n" + "=" * 60)
    print("ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    print(f"ì´ ì¿¼ë¦¬ ìˆ˜: {len(results)}")
    print(f"ì„±ê³µí•œ ì¿¼ë¦¬: {len(valid_results)}")
    print(f"ì‹¤íŒ¨í•œ ì¿¼ë¦¬: {error_count}")
    
    if valid_results:
        avg_score = sum(r.overall_score for r in valid_results) / len(valid_results)
        avg_time = sum(r.response_time_ms for r in valid_results) / len(valid_results)
        
        print(f"í‰ê·  ì ìˆ˜: {avg_score:.2f}/100")
        print(f"í‰ê·  ì‘ë‹µì‹œê°„: {avg_time:.0f}ms")
        
        # ê²€ìƒ‰ ë©”íŠ¸ë¦­ í‰ê· 
        avg_recall = sum(r.retrieval.recall_at_k for r in valid_results) / len(valid_results)
        avg_precision = sum(r.retrieval.precision_at_k for r in valid_results) / len(valid_results)
        avg_mrr = sum(r.retrieval.mrr for r in valid_results) / len(valid_results)
        
        print(f"í‰ê·  Recall@{5}: {avg_recall:.3f}")
        print(f"í‰ê·  Precision@{5}: {avg_precision:.3f}")
        print(f"í‰ê·  MRR: {avg_mrr:.3f}")
        
        # ë‚œì´ë„ë³„ ì„±ëŠ¥
        difficulty_stats = {}
        for result in valid_results:
            diff = result.difficulty
            if diff not in difficulty_stats:
                difficulty_stats[diff] = []
            difficulty_stats[diff].append(result.overall_score)
        
        if difficulty_stats:
            print("\nğŸ“ˆ ë‚œì´ë„ë³„ ì„±ëŠ¥:")
            for difficulty, scores in difficulty_stats.items():
                avg_score = sum(scores) / len(scores)
                print(f"  {difficulty}: {avg_score:.2f} ({len(scores)}ê°œ ì¿¼ë¦¬)")


def print_comparison_summary(comparison_results):
    """ëª¨ë¸ ë¹„êµ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    print("\n" + "=" * 80)
    print("ğŸ“Š ëª¨ë¸ ë¹„êµ ê²°ê³¼")
    print("=" * 80)
    
    # ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½
    print(f"{'ëª¨ë¸':<15} {'í‰ê· ì ìˆ˜':<10} {'í‰ê· ì‘ë‹µì‹œê°„':<15} {'Recall@5':<10} {'Precision@5':<12}")
    print("-" * 80)
    
    for model_name, results in comparison_results.items():
        valid_results = [r for r in results if not r.error_message]
        if valid_results:
            avg_score = sum(r.overall_score for r in valid_results) / len(valid_results)
            avg_time = sum(r.response_time_ms for r in valid_results) / len(valid_results)
            avg_recall = sum(r.retrieval.recall_at_k for r in valid_results) / len(valid_results)
            avg_precision = sum(r.retrieval.precision_at_k for r in valid_results) / len(valid_results)
            
            print(f"{model_name:<15} {avg_score:<10.2f} {avg_time:<15.0f} {avg_recall:<10.3f} {avg_precision:<12.3f}")
    
    # ì¿¼ë¦¬ë³„ ìƒì„¸ ë¹„êµ
    print("\nğŸ“‹ ì¿¼ë¦¬ë³„ ìƒì„¸ ë¹„êµ:")
    print("-" * 80)
    
    # ì²« ë²ˆì§¸ ëª¨ë¸ì˜ ì¿¼ë¦¬ ìˆœì„œë¥¼ ê¸°ì¤€ìœ¼ë¡œ
    first_model = list(comparison_results.keys())[0]
    first_results = comparison_results[first_model]
    
    for result in first_results:
        if result.error_message:
            continue
            
        print(f"[{result.query_id}] {result.query}")
        print("-" * 40)
        
        for model_name, results in comparison_results.items():
            model_result = next((r for r in results if r.query_id == result.query_id), None)
            if model_result and not model_result.error_message:
                print(f"{model_name}: {model_result.overall_score:.1f}ì  "
                      f"(Recall={model_result.retrieval.recall_at_k:.3f}, "
                      f"ì‘ë‹µì‹œê°„={model_result.response_time_ms:.0f}ms)")
        
        print()


if __name__ == "__main__":
    main()
