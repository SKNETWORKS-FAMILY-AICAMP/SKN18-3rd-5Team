#!/usr/bin/env python3
"""
JSONL to Parquet Converter - Optimized Version

JSONL íŒŒì¼ë“¤ì„ íš¨ìœ¨ì ìœ¼ë¡œ Parquet íŒŒì¼ë¡œ ë³€í™˜í•˜ëŠ” ë„êµ¬
- ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê·¹ëŒ€í™”
- ìƒ˜í”Œ ê¸°ë°˜ ìŠ¤í‚¤ë§ˆ ìƒì„±ìœ¼ë¡œ ì²˜ë¦¬ ì†ë„ í–¥ìƒ
- Row Group ê¸°ë°˜ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì•ˆì •ì„± ë³´ì¥
"""

import json
import argparse
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from datetime import datetime


class ParquetConverter:
    """JSONLì„ Parquetìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ìµœì í™”ëœ ì»¨ë²„í„°"""
    
    def __init__(self, compression: str = 'snappy'):
        """
        Args:
            compression: ì••ì¶• ë°©ì‹ ('snappy', 'gzip', 'brotli', 'zstd')
        """
        self.compression = compression
    
    def _optimize_dtypes(self, df: pd.DataFrame) -> pd.DataFrame:
        """DataFrameì˜ ë°ì´í„° íƒ€ì…ì„ ìµœì í™”í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ"""
        
        # ë¬¸ìì—´ ì»¬ëŸ¼ì„ categoryë¡œ ë³€í™˜ (ì¤‘ë³µê°’ì´ ë§ì€ ê²½ìš°)
        category_cols = ['chunk_type', 'doc_type', 'data_category']
        for col in category_cols:
            if col in df.columns:
                df[col] = df[col].astype('category')
        
        # ì •ìˆ˜í˜• ìµœì í™”
        if 'token_count' in df.columns:
            df['token_count'] = pd.to_numeric(df['token_count'], downcast='integer')
        
        if 'merged_count' in df.columns:
            df['merged_count'] = pd.to_numeric(df['merged_count'], downcast='integer')
        
        # fiscal_yearëŠ” nullable intë¡œ
        if 'fiscal_year' in df.columns:
            df['fiscal_year'] = df['fiscal_year'].astype('Int16')
        
        return df
    
    def save_parquet(
        self, 
        df: pd.DataFrame, 
        output_path: Path,
        partition_cols: Optional[List[str]] = None
    ) -> None:
        """DataFrameì„ Parquet íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            df: ì €ì¥í•  DataFrame
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
            partition_cols: íŒŒí‹°ì…˜ ì»¬ëŸ¼ (ì˜ˆ: ['corp_name', 'doc_type'])
        """
        # ë°ì´í„° íƒ€ì… ìµœì í™”
        df = self._optimize_dtypes(df)
        
        if partition_cols:
            # íŒŒí‹°ì…˜ë³„ë¡œ ì €ì¥
            df.to_parquet(
                output_path,
                partition_cols=partition_cols,
                compression=self.compression,
                engine='pyarrow'
            )
        else:
            # ë‹¨ì¼ íŒŒì¼ë¡œ ì €ì¥
            df.to_parquet(
                output_path,
                compression=self.compression,
                engine='pyarrow'
            )
    
    def jsonl_to_dataframe(self, jsonl_files: List[Path]) -> pd.DataFrame:
        """JSONL íŒŒì¼ë“¤ì„ DataFrameìœ¼ë¡œ ë³€í™˜ (ë©”ëª¨ë¦¬ ì§‘ì•½ì )
        
        Args:
            jsonl_files: JSONL íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë³€í™˜ëœ DataFrame
        """
        all_data = []
        
        for file_path in jsonl_files:
            print(f"ì²˜ë¦¬ ì¤‘: {file_path.name}")
            
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_no, line in enumerate(f, 1):
                    try:
                        data = json.loads(line)
                        all_data.append(data)
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸  {file_path.name} Line {line_no} JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue
        
        print(f"ì´ {len(all_data):,}ê°œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
        return pd.DataFrame(all_data)
    
    def jsonl_to_parquet_streaming(
        self, 
        jsonl_files: List[Path], 
        output_path: Path,
        batch_size: int = 10000
    ) -> None:
        """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ JSONLì„ Parquetìœ¼ë¡œ ë³€í™˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        
        Args:
            jsonl_files: JSONL íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            output_path: ì¶œë ¥ Parquet íŒŒì¼ ê²½ë¡œ
            batch_size: ë°°ì¹˜ í¬ê¸° (Row Group í¬ê¸°)
        """
        print(f"ğŸ”„ ìŠ¤íŠ¸ë¦¬ë° ë³€í™˜ ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {batch_size:,})")
        
        # Row Group ê¸°ë°˜ ì ‘ê·¼ë²• ì‚¬ìš©
        self._write_parquet_with_row_groups(jsonl_files, output_path, batch_size)
        
        print(f"\nâœ… ìŠ¤íŠ¸ë¦¬ë° ë³€í™˜ ì™„ë£Œ")
    
    def _write_parquet_with_row_groups(
        self, 
        jsonl_files: List[Path], 
        output_path: Path,
        batch_size: int
    ) -> None:
        """Row Group ê¸°ë°˜ìœ¼ë¡œ Parquet íŒŒì¼ ì‘ì„± (ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± í•´ê²°)"""
        
        # 1ë‹¨ê³„: ì œí•œëœ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ í†µí•© ìŠ¤í‚¤ë§ˆ ìƒì„± (ë©”ëª¨ë¦¬ ì•ˆì „ ëª¨ë“œ)
        print("ğŸ” í†µí•© ìŠ¤í‚¤ë§ˆ ìƒì„± ì¤‘...")
        all_data = []
        total_chunks = 0
        processed_files = 0
        max_sample_for_schema = 30000  # ìŠ¤í‚¤ë§ˆ ìƒì„±ì„ ìœ„í•œ ìµœëŒ€ ìƒ˜í”Œ ìˆ˜
        
        print(f"ğŸ“Š ì´ {len(jsonl_files):,}ê°œ íŒŒì¼ì—ì„œ ìµœëŒ€ {max_sample_for_schema:,}ê°œ ìƒ˜í”Œë¡œ ìŠ¤í‚¤ë§ˆ ìƒì„± (ë©”ëª¨ë¦¬ ì•ˆì „ ëª¨ë“œ)")
        
        for file_idx, file_path in enumerate(jsonl_files):
            # ìƒ˜í”Œ ìˆ˜ì§‘ ì œí•œ
            if total_chunks >= max_sample_for_schema:
                print(f"âœ… ìƒ˜í”Œ ìˆ˜ì§‘ ì™„ë£Œ: {total_chunks:,}ê°œ (ëª©í‘œ: {max_sample_for_schema:,}ê°œ)")
                break
                
            processed_files += 1
            
            # íŒŒì¼ ì²˜ë¦¬ ì§„í–‰ ìƒí™© ì¶œë ¥
            if processed_files % 100 == 0 or processed_files <= 10:
                print(f"  ğŸ“ íŒŒì¼ {processed_files:,}/{len(jsonl_files):,}: {file_path.name} (í˜„ì¬ ìƒ˜í”Œ: {total_chunks:,}ê°œ)")
            
            with open(file_path, 'r', encoding='utf-8') as f:
                file_chunks = 0
                for line_no, line in enumerate(f, 1):
                    if total_chunks >= max_sample_for_schema:
                        break
                        
                    try:
                        data = json.loads(line)
                        all_data.append(data)
                        total_chunks += 1
                        file_chunks += 1
                        
                        # 1,000ê°œë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥
                        if total_chunks % 1000 == 0:
                            print(f"    ğŸ“ˆ ìƒ˜í”Œ ìˆ˜ì§‘ ì§„í–‰: {total_chunks:,}/{max_sample_for_schema:,}ê°œ")
                            
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸  {file_path.name} Line {line_no} JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue
                
                if file_chunks > 0 and processed_files <= 10:
                    print(f"    âœ… {file_path.name}: {file_chunks:,}ê°œ ì²­í¬ ì²˜ë¦¬ë¨")
        
        print(f"âœ… ìŠ¤í‚¤ë§ˆ ìƒ˜í”Œ ìˆ˜ì§‘ ì™„ë£Œ: {total_chunks:,}ê°œ ì²­í¬")
        
        # í†µí•© ìŠ¤í‚¤ë§ˆ ìƒì„± (íš¨ìœ¨ì ì¸ ìƒ˜í”Œ ê¸°ë°˜)
        if all_data:
            # ì´ë¯¸ ì œí•œëœ ìƒ˜í”Œì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            sample_data = all_data
            
            print(f"ğŸ”„ ìŠ¤í‚¤ë§ˆ ìƒì„±ìš© ìƒ˜í”Œ DataFrame ìƒì„± ì¤‘... ({len(sample_data):,}ê°œ ìƒ˜í”Œ)")
            df_sample = pd.DataFrame(sample_data)
            print(f"âœ… ìƒ˜í”Œ DataFrame ìƒì„± ì™„ë£Œ: {len(df_sample):,}í–‰ x {len(df_sample.columns)}ì—´")
            
            print("ğŸ”„ ë°ì´í„° íƒ€ì… ìµœì í™” ì¤‘...")
            df_sample = self._optimize_dtypes(df_sample)
            print("âœ… ë°ì´í„° íƒ€ì… ìµœì í™” ì™„ë£Œ")
            
            print("ğŸ”„ PyArrow ìŠ¤í‚¤ë§ˆ ìƒì„± ì¤‘...")
            unified_schema = pa.Schema.from_pandas(df_sample)
            print(f"âœ… í†µí•© ìŠ¤í‚¤ë§ˆ ìƒì„± ì™„ë£Œ (ìƒ˜í”Œ: {len(sample_data):,}ê°œ)")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del all_data, sample_data, df_sample
            import gc
            gc.collect()
            print("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        else:
            print("âŒ ìŠ¤í‚¤ë§ˆ ìƒì„± ì‹¤íŒ¨: ë°ì´í„° ì—†ìŒ")
            return
        
        # 2ë‹¨ê³„: í†µí•© ìŠ¤í‚¤ë§ˆë¡œ ParquetWriter ìƒì„±
        writer = pq.ParquetWriter(
            str(output_path),
            unified_schema,
            compression=self.compression,
            use_dictionary=True,
            write_statistics=True
        )
        
        total_processed = 0
        processed_files = 0
        
        print(f"\nğŸ”„ ë°ì´í„° ë³€í™˜ ì‹œì‘ (ì´ {len(jsonl_files):,}ê°œ íŒŒì¼)")
        
        try:
            for file_idx, file_path in enumerate(jsonl_files):
                processed_files += 1
                
                # íŒŒì¼ ì²˜ë¦¬ ì§„í–‰ ìƒí™© ì¶œë ¥
                if processed_files % 50 == 0 or processed_files <= 10:
                    print(f"  ğŸ“ íŒŒì¼ {processed_files:,}/{len(jsonl_files):,}: {file_path.name}")
                
                with open(file_path, 'r', encoding='utf-8') as f:
                    batch_data = []
                    file_chunks = 0
                    
                    for line_no, line in enumerate(f, 1):
                        try:
                            data = json.loads(line)
                            batch_data.append(data)
                            file_chunks += 1
                            
                            # ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ Row Groupìœ¼ë¡œ ì‘ì„±
                            if len(batch_data) >= batch_size:
                                df = pd.DataFrame(batch_data)
                                df = self._optimize_dtypes(df)
                                
                                # ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„±ì„ ìœ„í•´ í†µí•© ìŠ¤í‚¤ë§ˆ ì ìš©
                                table = pa.Table.from_pandas(df, schema=unified_schema)
                                writer.write_table(table)
                                
                                total_processed += len(batch_data)
                                
                                # 10,000ê°œë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥
                                if total_processed % 10000 == 0:
                                    print(f"    ğŸ“ˆ ì „ì²´ ì§„í–‰: {total_processed:,}ê°œ ì²­í¬ ì²˜ë¦¬ë¨")
                                
                                batch_data = []
                                
                        except json.JSONDecodeError as e:
                            print(f"âš ï¸  {file_path.name} Line {line_no} JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                            continue
                        except Exception as e:
                            print(f"âš ï¸  ìŠ¤í‚¤ë§ˆ ì˜¤ë¥˜: {e}")
                            # ìŠ¤í‚¤ë§ˆ ì˜¤ë¥˜ ì‹œ í•´ë‹¹ ë°°ì¹˜ ê±´ë„ˆë›°ê¸°
                            batch_data = []
                            continue
                    
                    # íŒŒì¼ ëì˜ ë‚¨ì€ ë°ì´í„° ì²˜ë¦¬
                    if batch_data:
                        try:
                            df = pd.DataFrame(batch_data)
                            df = self._optimize_dtypes(df)
                            table = pa.Table.from_pandas(df, schema=unified_schema)
                            writer.write_table(table)
                            
                            total_processed += len(batch_data)
                            
                        except Exception as e:
                            print(f"âš ï¸  ë§ˆì§€ë§‰ ë°°ì¹˜ ìŠ¤í‚¤ë§ˆ ì˜¤ë¥˜: {e}")
                    
                    # íŒŒì¼ë³„ ì²˜ë¦¬ ê²°ê³¼ ì¶œë ¥ (ì²˜ìŒ 10ê°œ íŒŒì¼ë§Œ)
                    if processed_files <= 10:
                        print(f"    âœ… {file_path.name}: {file_chunks:,}ê°œ ì²­í¬ ì²˜ë¦¬ë¨")
        
        finally:
            # Writer ë‹«ê¸°
            writer.close()
            print(f"âœ… ì´ {total_processed:,}ê°œ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ")
    
    def get_parquet_stats(self, parquet_path: Path) -> Optional[Dict[str, Any]]:
        """Parquet íŒŒì¼ì˜ í†µê³„ ì •ë³´ ë°˜í™˜
        
        Args:
            parquet_path: Parquet íŒŒì¼ ê²½ë¡œ
            
        Returns:
            í†µê³„ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        try:
            parquet_file = pq.ParquetFile(parquet_path)
            metadata = parquet_file.metadata
            
            return {
                'num_rows': metadata.num_rows,
                'num_columns': metadata.num_columns,
                'num_row_groups': metadata.num_row_groups,
                'serialized_size': metadata.serialized_size,
                'created_by': metadata.created_by,
                'schema': metadata.schema.to_arrow_schema()
            }
        except Exception as e:
            print(f"âš ï¸  í†µê³„ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None


def convert_jsonl_to_parquet_streaming(
    input_dir: Path, 
    output_file: Path,
    compression: str = 'snappy',
    batch_size: int = 10000
) -> None:
    """
    JSONL íŒŒì¼ë“¤ì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ Parquetìœ¼ë¡œ ë³€í™˜
    
    Args:
        input_dir: JSONL íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
        output_file: ì¶œë ¥ Parquet íŒŒì¼ ê²½ë¡œ
        compression: ì••ì¶• ë°©ì‹
        batch_size: ë°°ì¹˜ í¬ê¸°
    """
    print("=" * 80)
    print("JSONL to Parquet Converter - Streaming Mode (Memory Efficient)")
    print("=" * 80)
    print(f"ğŸ“ ì…ë ¥ ë””ë ‰í† ë¦¬: {input_dir}")
    print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_file}")
    print(f"ğŸ—œï¸  ì••ì¶• ë°©ì‹: {compression}")
    print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {batch_size:,} (ë°°ì¹˜ í¬ê¸°ê°€ í´ìˆ˜ë¡ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€, ì²˜ë¦¬ ì†ë„ í–¥ìƒ)")
    print("=" * 80)
    print()
    
    # JSONL íŒŒì¼ ëª©ë¡
    jsonl_files = sorted(input_dir.glob("*_chunks.jsonl"))
    
    if not jsonl_files:
        print("âŒ JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“„ ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {len(jsonl_files):,}ê°œ")
    print()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    # ë³€í™˜ ì‹¤í–‰
    converter = ParquetConverter(compression=compression)
    
    # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë³€í™˜
    converter.jsonl_to_parquet_streaming(jsonl_files, output_file, batch_size)
    
    # í†µê³„ ì •ë³´
    stats = converter.get_parquet_stats(output_file)
    if stats:
        print(f"\nğŸ“Š Parquet í†µê³„:")
        print(f"   í–‰ ìˆ˜: {stats['num_rows']:,}")
        print(f"   ì—´ ìˆ˜: {stats['num_columns']}")
        print(f"   Row Groups: {stats['num_row_groups']}")
        print(f"   ì••ì¶• í›„ í¬ê¸°: {stats['serialized_size'] / (1024**2):.2f} MB")


def process_to_single_parquet(
    input_dir: Path, 
    output_file: Path,
    compression: str = 'snappy'
) -> None:
    """
    ì „ì²´ JSONL íŒŒì¼ì„ í•˜ë‚˜ì˜ Parquet íŒŒì¼ë¡œ ë³€í™˜
    
    Args:
        input_dir: JSONL íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
        output_file: ì¶œë ¥ Parquet íŒŒì¼ ê²½ë¡œ
        compression: ì••ì¶• ë°©ì‹
    """
    print("=" * 80)
    print("JSONL to Parquet Converter - Single File Mode")
    print("=" * 80)
    print(f"ğŸ“ ì…ë ¥ ë””ë ‰í† ë¦¬: {input_dir}")
    print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_file}")
    print(f"ğŸ—œï¸  ì••ì¶• ë°©ì‹: {compression}")
    print("=" * 80)
    print()
    
    # JSONL íŒŒì¼ ëª©ë¡
    jsonl_files = sorted(input_dir.glob("*_chunks.jsonl"))
    
    if not jsonl_files:
        print("âŒ JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“„ ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {len(jsonl_files):,}ê°œ")
    print()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    # ë³€í™˜ ì‹¤í–‰
    converter = ParquetConverter(compression=compression)
    
    # DataFrameìœ¼ë¡œ ë³€í™˜ í›„ ì €ì¥
    df = converter.jsonl_to_dataframe(jsonl_files)
    converter.save_parquet(df, output_file)
    
    # í†µê³„ ì •ë³´
    stats = converter.get_parquet_stats(output_file)
    if stats:
        print(f"\nğŸ“Š Parquet í†µê³„:")
        print(f"   í–‰ ìˆ˜: {stats['num_rows']:,}")
        print(f"   ì—´ ìˆ˜: {stats['num_columns']}")
        print(f"   Row Groups: {stats['num_row_groups']}")
        print(f"   ì••ì¶• í›„ í¬ê¸°: {stats['serialized_size'] / (1024**2):.2f} MB")


def process_with_partitioning(
    input_dir: Path, 
    output_dir: Path,
    partition_col: str,
    compression: str = 'snappy'
) -> None:
    """
    íŒŒí‹°ì…”ë‹ì„ ì ìš©í•˜ì—¬ Parquet íŒŒì¼ë“¤ë¡œ ë³€í™˜
    
    Args:
        input_dir: JSONL íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        partition_col: íŒŒí‹°ì…˜ ê¸°ì¤€ ì»¬ëŸ¼
        compression: ì••ì¶• ë°©ì‹
    """
    print("=" * 80)
    print(f"JSONL to Parquet Converter - Partitioned Mode ({partition_col})")
    print("=" * 80)
    print(f"ğŸ“ ì…ë ¥ ë””ë ‰í† ë¦¬: {input_dir}")
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"ğŸ—œï¸  ì••ì¶• ë°©ì‹: {compression}")
    print(f"ğŸ“Š íŒŒí‹°ì…˜ ê¸°ì¤€: {partition_col}")
    print("=" * 80)
    print()
    
    # JSONL íŒŒì¼ ëª©ë¡
    jsonl_files = sorted(input_dir.glob("*_chunks.jsonl"))
    
    if not jsonl_files:
        print("âŒ JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“„ ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {len(jsonl_files):,}ê°œ")
    print()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ë³€í™˜ ì‹¤í–‰
    converter = ParquetConverter(compression=compression)
    
    # DataFrameìœ¼ë¡œ ë³€í™˜ í›„ íŒŒí‹°ì…”ë‹í•˜ì—¬ ì €ì¥
    df = converter.jsonl_to_dataframe(jsonl_files)
    
    if partition_col not in df.columns:
        print(f"âŒ íŒŒí‹°ì…˜ ì»¬ëŸ¼ '{partition_col}'ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
        return
    
    converter.save_parquet(df, output_dir, partition_cols=[partition_col])
    
    print(f"\nâœ… íŒŒí‹°ì…”ë‹ ì™„ë£Œ: {output_dir}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="JSONL íŒŒì¼ë“¤ì„ Parquetìœ¼ë¡œ ë³€í™˜",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ë‹¨ì¼ Parquet íŒŒì¼ ìƒì„±
  python jsonl_to_parquet.py
  
  # ê¸°ì—…ë³„ë¡œ íŒŒí‹°ì…”ë‹
  python jsonl_to_parquet.py --partition corp_name
  
  # ì••ì¶• ë°©ì‹ ë³€ê²½
  python jsonl_to_parquet.py --compression zstd
        """
    )
    
    parser.add_argument(
        '--partition',
        type=str,
        choices=['none', 'corp_name', 'doc_type', 'data_category'],
        default='none',
        help='íŒŒí‹°ì…˜ ê¸°ì¤€ (ê¸°ë³¸ê°’: none)'
    )
    
    parser.add_argument(
        '--compression',
        type=str,
        choices=['snappy', 'gzip', 'brotli', 'zstd'],
        default='snappy',
        help='ì••ì¶• ë°©ì‹ (ê¸°ë³¸ê°’: snappy)'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=50000,
        help='ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°ì ˆ, ê¸°ë³¸ê°’: 50000)'
    )
    
    parser.add_argument(
        '--streaming',
        action='store_true',
        default=True,
        help='ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì‚¬ìš© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì , ê¸°ë³¸ê°’: True)'
    )
    
    args = parser.parse_args()
    
    # ê²½ë¡œ ì„¤ì •
    script_dir = Path(__file__).parent  # service/etl/loader
    etl_dir = script_dir.parent  # service/etl
    service_dir = etl_dir.parent  # service
    project_root = service_dir.parent  # project root
    data_dir = project_root / "data"
    
    # ì…ë ¥/ì¶œë ¥ ê²½ë¡œ
    input_dir = data_dir / "transform" / "final"
    parquet_dir = data_dir / "parquet"
    
    # íŒŒí‹°ì…”ë‹ ì²˜ë¦¬
    if args.partition == 'none':
        # ë‹¨ì¼ íŒŒì¼ë¡œ ë³€í™˜
        output_file = parquet_dir / "chunks.parquet"
        
        if args.streaming:
            convert_jsonl_to_parquet_streaming(
                input_dir, 
                output_file, 
                args.compression, 
                args.batch_size
            )
        else:
            process_to_single_parquet(
                input_dir, 
                output_file, 
                args.compression
            )
    else:
        # íŒŒí‹°ì…”ë‹í•˜ì—¬ ë³€í™˜
        output_dir = parquet_dir / f"chunks_partitioned_{args.partition}"
        process_with_partitioning(
            input_dir, 
            output_dir, 
            args.partition, 
            args.compression
        )


if __name__ == "__main__":
    main()