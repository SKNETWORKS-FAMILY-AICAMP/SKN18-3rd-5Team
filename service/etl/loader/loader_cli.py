#!/usr/bin/env python3
"""
JSONL Loader CLI - JSONL íŒŒì¼ ë¡œë”© ë° ì„ë² ë”© ìƒì„± í†µí•© CLI

ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ë‹¨ê³„ë³„ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.
"""

import argparse
import sys
import logging
import time
from pathlib import Path

# ë¡œì»¬ ëª¨ë“ˆ import
from system_manager import (
    check_docker_compose, start_docker_compose, stop_docker_compose,
    create_schema, drop_schema, check_schema, system_health_check, reset_system
)
from jsonl_to_postgres import load_jsonl_files, get_loading_stats, clear_data

# RAG ì‹œìŠ¤í…œ ëª¨ë“ˆ import
import sys
from pathlib import Path
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from service.rag.vectorstore.pgvector_store import PgVectorStore
from service.rag.models.loader import ModelFactory, EmbeddingModelType
from config.vector_database import get_vector_db_config

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# ëª¨ë“  í•¨ìˆ˜ëŠ” ì´ì œ ë³„ë„ ëª¨ë“ˆì—ì„œ importë¨

# ============================================================================
# RAG ì‹œìŠ¤í…œ ê´€ë¦¬ í•¨ìˆ˜ë“¤
# ============================================================================

def test_database_connection():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ” ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
    try:
        vector_store = PgVectorStore()
        if vector_store.is_connected():
            logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ!")
            
            # í…Œì´ë¸” ì •ë³´ ì¡°íšŒ
            with vector_store.conn.cursor() as cursor:
                cursor.execute("""
                    SELECT table_name, 
                           (SELECT COUNT(*) FROM information_schema.columns 
                            WHERE table_name = t.table_name) as column_count
                    FROM information_schema.tables t
                    WHERE table_schema = 'public' 
                    AND table_name LIKE '%embedding%' OR table_name = 'chunks'
                    ORDER BY table_name
                """)
                
                tables = cursor.fetchall()
                logger.info("ğŸ“Š ê´€ë ¨ í…Œì´ë¸” í˜„í™©:")
                for table_name, col_count in tables:
                    logger.info(f"  â€¢ {table_name}: {col_count}ê°œ ì»¬ëŸ¼")
            
            return True
        else:
            logger.error("âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨!")
            return False
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def truncate_documents():
    """ë¬¸ì„œ í…Œì´ë¸” ë°ì´í„° ì‚­ì œ (chunks í…Œì´ë¸”)"""
    logger.info("ğŸ—‘ï¸ ë¬¸ì„œ í…Œì´ë¸” ë°ì´í„° ì‚­ì œ ì¤‘...")
    try:
        vector_store = PgVectorStore()
        with vector_store.conn.cursor() as cursor:
            # chunks í…Œì´ë¸” ë°ì´í„° ì‚­ì œ (CASCADEë¡œ ê´€ë ¨ ì„ë² ë”© í…Œì´ë¸”ë„ í•¨ê»˜ ì‚­ì œ)
            cursor.execute("TRUNCATE TABLE chunks RESTART IDENTITY CASCADE")
            vector_store.conn.commit()
        
        logger.info("âœ… ë¬¸ì„œ í…Œì´ë¸” ë°ì´í„° ì‚­ì œ ì™„ë£Œ!")
        return True
    except Exception as e:
        logger.error(f"âŒ ë¬¸ì„œ í…Œì´ë¸” ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {e}")
        return False

def truncate_vectors():
    """ë²¡í„° í…Œì´ë¸” ë°ì´í„° ì‚­ì œ (ì„ë² ë”© í…Œì´ë¸”ë“¤)"""
    logger.info("ğŸ—‘ï¸ ë²¡í„° í…Œì´ë¸” ë°ì´í„° ì‚­ì œ ì¤‘...")
    try:
        vector_store = PgVectorStore()
        config = get_vector_db_config()
        
        with vector_store.conn.cursor() as cursor:
            # ëª¨ë“  ì„ë² ë”© í…Œì´ë¸” ë°ì´í„° ì‚­ì œ
            for model_type in EmbeddingModelType:
                table_name = config.get_table_name(model_type)
                cursor.execute(f"TRUNCATE TABLE {table_name} RESTART IDENTITY CASCADE")
                logger.info(f"  â€¢ {table_name} í…Œì´ë¸” ë°ì´í„° ì‚­ì œ")
            
            vector_store.conn.commit()
        
        logger.info("âœ… ë²¡í„° í…Œì´ë¸” ë°ì´í„° ì‚­ì œ ì™„ë£Œ!")
        return True
    except Exception as e:
        logger.error(f"âŒ ë²¡í„° í…Œì´ë¸” ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {e}")
        return False

def load_documents(jsonl_dir: Path, batch_size: int = 1000):
    """ë¬¸ì„œ ë¡œë“œ (JSONL íŒŒì¼ì„ chunks í…Œì´ë¸”ì— ë¡œë“œ)"""
    logger.info(f"ğŸ“„ ë¬¸ì„œ ë¡œë“œ ì‹œì‘: {jsonl_dir}")
    try:
        success = load_jsonl_files(jsonl_dir, batch_size)
        if success:
            logger.info("âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ!")
        else:
            logger.error("âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨!")
        return success
    except Exception as e:
        logger.error(f"âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

def download_models(model: str = "all"):
    """ì„ë² ë”© ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
    logger.info(f"ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {model}")
    
    try:
        from sentence_transformers import SentenceTransformer
        
        models_to_download = []
        if model == "all":
            models_to_download = ["e5", "kakaobank", "fine5"]
        else:
            models_to_download = [model]
        
        for model_name in models_to_download:
            if model_name == "e5":
                logger.info("ğŸ“¥ E5 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                SentenceTransformer('intfloat/multilingual-e5-small')
                logger.info("âœ… E5 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
            elif model_name == "kakaobank":
                logger.info("ğŸ“¥ KakaoBank ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                SentenceTransformer('kakaobank/kf-deberta-base')
                logger.info("âœ… KakaoBank ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
            elif model_name == "fine5":
                logger.info("ğŸ“¥ FinE5 ëª¨ë¸ ì„¤ì • ì¤‘...")
                logger.info("â„¹ï¸ FinE5ëŠ” API ì „ìš© ëª¨ë¸ì…ë‹ˆë‹¤. AbaciNLP API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                
                # API í‚¤ í™•ì¸
                import os
                from dotenv import load_dotenv
                load_dotenv()
                
                api_key = os.getenv('FIN_E5_API_KEY')
                if not api_key:
                    logger.warning("âš ï¸ FIN_E5_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    logger.info("â„¹ï¸ .env íŒŒì¼ì— FIN_E5_API_KEY=your_api_key ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
                else:
                    logger.info(f"âœ… API í‚¤ í™•ì¸ë¨: {api_key[:10]}...")
                    
                    # API ì—°ê²° í…ŒìŠ¤íŠ¸
                    try:
                        from service.rag.models.fine5_api_encoder import FinE5APIEncoder
                        encoder = FinE5APIEncoder(api_key=api_key, model_name='abacinlp-text-v1')
                        test_embedding = encoder.encode_query("í…ŒìŠ¤íŠ¸")
                        logger.info(f"âœ… FinE5 API ì—°ê²° ì„±ê³µ! ì„ë² ë”© ì°¨ì›: {len(test_embedding)}")
                    except Exception as e:
                        logger.error(f"âŒ FinE5 API ì—°ê²° ì‹¤íŒ¨: {e}")
                        logger.info("â„¹ï¸ API í‚¤ë¥¼ í™•ì¸í•˜ê³  AbaciNLP ì„œë¹„ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                
                logger.info("âœ… FinE5 ëª¨ë¸ ì„¤ì • ì™„ë£Œ")
        
        logger.info("ğŸ‰ ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False


def load_vectors(model: str = "e5", batch_size: int = 100, limit: int = None):
    """ë²¡í„° ë¡œë“œ (ì„ë² ë”© ìƒì„± ë° ì €ì¥)"""
    logger.info(f"ğŸ§  ë²¡í„° ë¡œë“œ ì‹œì‘: {model} ëª¨ë¸")
    try:
        # ëª¨ë¸ íƒ€ì… ì„¤ì •
        if model.lower() == "e5":
            model_type = EmbeddingModelType.MULTILINGUAL_E5_SMALL
        elif model.lower() == "kakaobank":
            model_type = EmbeddingModelType.KAKAOBANK_DEBERTA
        elif model.lower() == "fine5":
            model_type = EmbeddingModelType.FINE5_FINANCE
        else:
            logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model}")
            return False
        
        # ì„ë² ë”© ìƒì„±ê¸° ì‹¤í–‰
        import subprocess
        from pathlib import Path
        
        # ëª¨ë¸ëª…ì„ ì§§ì€ í˜•íƒœë¡œ ë³€í™˜
        model_short = model.lower()
        
        command = f"""
            cd {Path(__file__).parent} && 
            python embeddings.py --model {model_short} --batch-size {batch_size}
        """
        
        if limit:
            command += f" --limit {limit}"
        
        logger.info(f"ğŸš€ ì„ë² ë”© ìƒì„± ëª…ë ¹ì–´: {command}")
        
        # ì‹¤ì‹œê°„ ë¡œê·¸ë¥¼ ìœ„í•´ capture_output=Falseë¡œ ì„¤ì •
        result = subprocess.run(command, shell=True, check=True)
        logger.info("âœ… ë²¡í„° ë¡œë“œ ì™„ë£Œ!")
        
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"âŒ ë²¡í„° ë¡œë“œ ì‹¤íŒ¨: {e.stderr}")
        return False
    except Exception as e:
        logger.error(f"âŒ ë²¡í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

def show_vector_stats():
    """ë²¡í„° í†µê³„ ì¡°íšŒ"""
    logger.info("ğŸ“Š ë²¡í„° í†µê³„ ì¡°íšŒ ì¤‘...")
    try:
        vector_store = PgVectorStore()
        config = get_vector_db_config()
        
        with vector_store.conn.cursor() as cursor:
            # ì „ì²´ ì²­í¬ ìˆ˜
            cursor.execute("SELECT COUNT(*) FROM chunks")
            total_chunks = cursor.fetchone()[0]
            
            logger.info(f"ğŸ“„ ì „ì²´ ì²­í¬ ìˆ˜: {total_chunks:,}")
            
            # ê° ëª¨ë¸ë³„ ì„ë² ë”© ìˆ˜
            for model_type in EmbeddingModelType:
                table_name = config.get_table_name(model_type)
                try:
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                    embedding_count = cursor.fetchone()[0]
                    logger.info(f"ğŸ§  {model_type.value}: {embedding_count:,}ê°œ ì„ë² ë”©")
                except Exception as e:
                    logger.info(f"ğŸ§  {model_type.value}: í…Œì´ë¸” ì—†ìŒ")
        
        return True
    except Exception as e:
        logger.error(f"âŒ ë²¡í„° í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return False




def generate_embeddings(model: str = "intfloat/multilingual-e5-small"):
    """ì„ë² ë”© ìƒì„± (CLIìš©)"""
    import subprocess
    from pathlib import Path
    
    logger.info(f"ğŸ§  ì„ë² ë”© ìƒì„±: {model}")
    
    # ì„ë² ë”© ìƒì„±ê¸° ì‹¤í–‰
    command = f"""
        cd {Path(__file__).parent} && 
        python embeddings.py --model {model}
    """
    
    try:
        result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
        logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        if result.stdout:
            logger.info(f"ì¶œë ¥: {result.stdout}")
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e.stderr}")
        return False


def run_full_pipeline(
    batch_size: int = 1000,
    embedding_model: str = "intfloat/multilingual-e5-small",
    skip_embeddings: bool = False
):
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    logger.info("ğŸš€ JSONL RAG íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    start_time = time.time()
    
    # 1ë‹¨ê³„: Docker Compose í™•ì¸
    if not check_docker_compose():
        logger.error("Docker Compose í™•ì¸ ì‹¤íŒ¨")
        return False
    
    # 2ë‹¨ê³„: ìŠ¤í‚¤ë§ˆ ìƒì„±
    if not create_schema():
        logger.error("ìŠ¤í‚¤ë§ˆ ìƒì„± ì‹¤íŒ¨")
        return False
    
    # 3ë‹¨ê³„: JSONL íŒŒì¼ ë¡œë”©
    jsonl_dir = Path(__file__).parent.parent.parent.parent / "data" / "transform" / "final"
    if not load_jsonl_files(jsonl_dir, batch_size):
        logger.error("JSONL íŒŒì¼ ë¡œë”© ì‹¤íŒ¨")
        return False
    
    # 4ë‹¨ê³„: ì„ë² ë”© ìƒì„± (ì„ íƒì )
    if not skip_embeddings:
        if not generate_embeddings(embedding_model):
            logger.error("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
            return False
    
    end_time = time.time()
    elapsed_time = end_time - start_time
    
    logger.info(f"ğŸ‰ JSONL ë¡œë”© íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    logger.info(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {elapsed_time/60:.1f}ë¶„")
    
    return True


def main():
    """ë©”ì¸ CLI í•¨ìˆ˜"""
    
    parser = argparse.ArgumentParser(
        description='JSONL Loader CLI - JSONL íŒŒì¼ ë¡œë”© ë° ì„ë² ë”© ìƒì„±',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬
  python loader_cli.py db test                     # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸
  python loader_cli.py db list                     # í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ
  python loader_cli.py db create                   # ìŠ¤í‚¤ë§ˆ ìƒì„±
  
        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        python loader_cli.py download                    # ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        python loader_cli.py download --model e5         # E5 ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ
        python loader_cli.py download --model kakaobank  # KakaoBank ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ
        python loader_cli.py download --model fine5      # FinE5 ëª¨ë¸ ì„¤ì • (API ì „ìš©)
        
        # ë°ì´í„° ì‚­ì œ
        python loader_cli.py truncate doc                # ë¬¸ì„œ í…Œì´ë¸” ë°ì´í„° ì‚­ì œ
        python loader_cli.py truncate vector             # ì„ë² ë”© í…Œì´ë¸” ë°ì´í„° ì‚­ì œ
        python loader_cli.py truncate all                # ëª¨ë“  í…Œì´ë¸” ë°ì´í„° ì‚­ì œ
  
        # ë°ì´í„° ë¡œë“œ
        python loader_cli.py load doc                    # ë¬¸ì„œ ë¡œë“œ
        python loader_cli.py load vector --model e5      # ë²¡í„° ë¡œë“œ (E5 ëª¨ë¸)
        python loader_cli.py load vector --model kakaobank --limit 1000  # KakaoBank ëª¨ë¸ë¡œ 1000ê°œ ì œí•œ
        
        """
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='ìƒì„¸ ë¡œê·¸ ì¶œë ¥'
    )
    
    subparsers = parser.add_subparsers(dest='command', help='ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´')
    
    # ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬
    db_parser = subparsers.add_parser('db', help='ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬')
    db_subparsers = db_parser.add_subparsers(dest='db_command', help='ë°ì´í„°ë² ì´ìŠ¤ ëª…ë ¹ì–´')
    db_subparsers.add_parser('test', help='ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸')
    db_subparsers.add_parser('list', help='í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ')
    db_subparsers.add_parser('create', help='ìŠ¤í‚¤ë§ˆ ìƒì„±')
    
    # truncate ëª…ë ¹ì–´
    truncate_parser = subparsers.add_parser('truncate', help='ë°ì´í„° ì‚­ì œ (TRUNCATE)')
    truncate_subparsers = truncate_parser.add_subparsers(dest='truncate_command', help='ì‚­ì œ ëª…ë ¹ì–´')
    truncate_subparsers.add_parser('doc', help='ë¬¸ì„œ í…Œì´ë¸” ë°ì´í„° ì‚­ì œ')
    truncate_subparsers.add_parser('vector', help='ì„ë² ë”© í…Œì´ë¸” ë°ì´í„° ì‚­ì œ')
    truncate_subparsers.add_parser('all', help='ëª¨ë“  í…Œì´ë¸” ë°ì´í„° ì‚­ì œ')
    
    # download ëª…ë ¹ì–´
    download_parser = subparsers.add_parser('download', help='ì„ë² ë”© ëª¨ë¸ ë‹¤ìš´ë¡œë“œ')
    download_parser.add_argument('--model', choices=['e5', 'kakaobank', 'fine5', 'all'], default='all', help='ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸')
    
    # ë¡œë“œ ëª…ë ¹ì–´
    load_parser = subparsers.add_parser('load', help='ë°ì´í„° ë¡œë“œ')
    load_subparsers = load_parser.add_subparsers(dest='load_command', help='ë¡œë“œ ëª…ë ¹ì–´')
    
    # ë¬¸ì„œ ë¡œë“œ
    load_doc_parser = load_subparsers.add_parser('doc', help='ë¬¸ì„œ ë¡œë“œ')
    load_doc_parser.add_argument('--jsonl-dir', type=Path, help='JSONL íŒŒì¼ ë””ë ‰í† ë¦¬')
    load_doc_parser.add_argument('--batch-size', type=int, default=1000, help='ë°°ì¹˜ í¬ê¸°')
    
    # ë²¡í„° ë¡œë“œ (ì„ë² ë”© ìƒì„±)
    load_vector_parser = load_subparsers.add_parser('vector', help='ë²¡í„° ë¡œë“œ (ì„ë² ë”© ìƒì„±)')
    load_vector_parser.add_argument('--model', choices=['e5', 'kakaobank', 'fine5'], default='e5', help='ì„ë² ë”© ëª¨ë¸')
    load_vector_parser.add_argument('--batch-size', type=int, default=100, help='ë°°ì¹˜ í¬ê¸°')
    load_vector_parser.add_argument('--limit', type=int, help='ì²˜ë¦¬í•  ì²­í¬ ìˆ˜ ì œí•œ')
    
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    try:
        if args.command == 'db':
            if args.db_command == 'test':
                success = test_database_connection()
            elif args.db_command == 'list':
                success = show_vector_stats()
            elif args.db_command == 'create':
                success = create_schema()
            else:
                db_parser.print_help()
                return
        elif args.command == 'truncate':
            if args.truncate_command == 'doc':
                success = truncate_documents()
            elif args.truncate_command == 'vector':
                success = truncate_vectors()
            elif args.truncate_command == 'all':
                # ëª¨ë“  í…Œì´ë¸” ë°ì´í„° ì‚­ì œ
                success1 = truncate_documents()
                success2 = truncate_vectors()
                success = success1 and success2
            else:
                truncate_parser.print_help()
                return
        elif args.command == 'download':
            success = download_models(args.model)
        elif args.command == 'load':
            if args.load_command == 'doc':
                # JSONL ë””ë ‰í† ë¦¬ ì„¤ì •
                if args.jsonl_dir:
                    jsonl_dir = args.jsonl_dir
                else:
                    jsonl_dir = Path(__file__).parent.parent.parent.parent / "data" / "transform" / "final"
                success = load_documents(jsonl_dir, args.batch_size)
            elif args.load_command == 'vector':
                success = load_vectors(args.model, args.batch_size, args.limit)
            else:
                load_parser.print_help()
                return
        else:
            parser.print_help()
            return
            
    except Exception as e:
        logger.error(f"ëª…ë ¹ì–´ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        sys.exit(1)
    
    if success:
        sys.exit(0)
    else:
        sys.exit(1)


if __name__ == "__main__":
    main()
