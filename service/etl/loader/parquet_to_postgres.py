#!/usr/bin/env python3
"""
Parquet to PostgreSQL Loader

Parquet íŒŒì¼ì˜ ì²­í¬ ë°ì´í„°ë¥¼ PostgreSQLì˜ vector_db ìŠ¤í‚¤ë§ˆë¡œ ë¡œë“œí•˜ëŠ” ë„êµ¬
- document_sources: ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
- document_chunks: ì²­í¬ ë°ì´í„° (natural_text, metadata ë“±)
"""

import sys
import argparse
from pathlib import Path
from typing import Optional, Dict, Any, List, Set
import pandas as pd
import psycopg2
from psycopg2.extras import execute_values
from psycopg2 import sql
import json
from datetime import datetime


class ParquetToPostgresLoader:
    """Parquet íŒŒì¼ì„ PostgreSQLë¡œ ë¡œë“œí•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(
        self,
        host: str = "localhost",
        port: int = 5432,
        database: str = "postgres",
        user: str = "postgres",
        password: str = "postgres"
    ):
        """
        Args:
            host: PostgreSQL í˜¸ìŠ¤íŠ¸
            port: PostgreSQL í¬íŠ¸
            database: ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„
            user: ì‚¬ìš©ì ì´ë¦„
            password: ë¹„ë°€ë²ˆí˜¸
        """
        self.connection_params = {
            'host': host,
            'port': port,
            'database': database,
            'user': user,
            'password': password
        }
        self.conn = None
        self.cursor = None

    def connect(self) -> bool:
        """PostgreSQL ì—°ê²°"""
        try:
            self.conn = psycopg2.connect(**self.connection_params)
            self.cursor = self.conn.cursor()
            print(f"âœ… PostgreSQL ì—°ê²° ì„±ê³µ: {self.connection_params['host']}:{self.connection_params['port']}/{self.connection_params['database']}")
            return True
        except Exception as e:
            print(f"âŒ PostgreSQL ì—°ê²° ì‹¤íŒ¨: {e}")
            return False

    def disconnect(self):
        """PostgreSQL ì—°ê²° ì¢…ë£Œ"""
        if self.cursor:
            self.cursor.close()
        if self.conn:
            self.conn.close()
        print("âœ… PostgreSQL ì—°ê²° ì¢…ë£Œ")

    def load_parquet(self, parquet_path: Path) -> Optional[pd.DataFrame]:
        """Parquet íŒŒì¼ ë¡œë“œ

        Args:
            parquet_path: Parquet íŒŒì¼ ê²½ë¡œ

        Returns:
            DataFrame ë˜ëŠ” None
        """
        try:
            print(f"ğŸ“– Parquet íŒŒì¼ ë¡œë“œ ì¤‘: {parquet_path}")
            df = pd.read_parquet(parquet_path, engine='pyarrow')
            print(f"âœ… {len(df):,}ê°œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
            print(f"   ì»¬ëŸ¼: {list(df.columns)}")
            return df
        except Exception as e:
            print(f"âŒ Parquet íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def extract_document_sources(self, df: pd.DataFrame) -> pd.DataFrame:
        """ì²­í¬ ë°ì´í„°í”„ë ˆì„ì—ì„œ ë¬¸ì„œ ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ

        Args:
            df: ì²­í¬ DataFrame

        Returns:
            ë¬¸ì„œ ì†ŒìŠ¤ DataFrame (unique doc_id)
        """
        # metadata ì»¬ëŸ¼ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ
        metadata_df = pd.json_normalize(df['metadata'])

        # doc_id ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ì—¬ unique ë¬¸ì„œ ìƒì„±
        sources_data = {
            'doc_id': df['doc_id'],
            'corp_name': metadata_df['corp_name'] if 'corp_name' in metadata_df.columns else None,
            'document_name': metadata_df['document_name'] if 'document_name' in metadata_df.columns else None,
            'rcept_dt': metadata_df['rcept_dt'] if 'rcept_dt' in metadata_df.columns else None,
            'doc_type': metadata_df['doc_type'] if 'doc_type' in metadata_df.columns else None,
            'data_category': metadata_df['data_category'] if 'data_category' in metadata_df.columns else None,
            'fiscal_year': metadata_df['fiscal_year'] if 'fiscal_year' in metadata_df.columns else None,
        }

        sources_df = pd.DataFrame(sources_data)

        # doc_id ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ì²« ë²ˆì§¸ ê°’ ìœ ì§€)
        sources_df = sources_df.drop_duplicates(subset=['doc_id'], keep='first')

        print(f"ğŸ“„ ë¬¸ì„œ ì†ŒìŠ¤: {len(sources_df):,}ê°œ unique ë¬¸ì„œ")

        return sources_df

    def insert_document_sources(self, sources_df: pd.DataFrame, batch_size: int = 1000) -> Dict[str, int]:
        """ë¬¸ì„œ ì†ŒìŠ¤ë¥¼ vector_db.document_sources í…Œì´ë¸”ì— ì‚½ì…

        Args:
            sources_df: ë¬¸ì„œ ì†ŒìŠ¤ DataFrame
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            doc_id â†’ database id ë§¤í•‘
        """
        print("\n" + "=" * 80)
        print("ğŸ“ ë¬¸ì„œ ì†ŒìŠ¤ ì‚½ì… ì¤‘...")
        print("=" * 80)

        doc_id_map = {}

        try:
            # ê¸°ì¡´ doc_id ì¡°íšŒ
            self.cursor.execute("SELECT doc_id, id FROM vector_db.document_sources")
            existing_docs = {row[0]: row[1] for row in self.cursor.fetchall()}
            print(f"   ê¸°ì¡´ ë¬¸ì„œ: {len(existing_docs):,}ê°œ")

            # ì‹ ê·œ ë¬¸ì„œë§Œ í•„í„°ë§
            new_sources = sources_df[~sources_df['doc_id'].isin(existing_docs.keys())]
            print(f"   ì‹ ê·œ ë¬¸ì„œ: {len(new_sources):,}ê°œ")

            if len(new_sources) == 0:
                print("âœ… ëª¨ë“  ë¬¸ì„œê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤")
                return existing_docs

            # ë°°ì¹˜ ì‚½ì…
            insert_query = """
                INSERT INTO vector_db.document_sources
                (doc_id, corp_name, document_name, rcept_dt, doc_type, data_category, fiscal_year, metadata)
                VALUES %s
                ON CONFLICT (doc_id) DO NOTHING
                RETURNING id, doc_id
            """

            total_inserted = 0

            for i in range(0, len(new_sources), batch_size):
                batch = new_sources.iloc[i:i+batch_size]

                # ë°ì´í„° ì¤€ë¹„
                values = [
                    (
                        row['doc_id'],
                        row['corp_name'],
                        row['document_name'],
                        row['rcept_dt'],
                        row['doc_type'],
                        row['data_category'],
                        int(row['fiscal_year']) if pd.notna(row['fiscal_year']) else None,
                        json.dumps({})  # ê¸°ë³¸ ë¹ˆ ë©”íƒ€ë°ì´í„°
                    )
                    for _, row in batch.iterrows()
                ]

                # ì‚½ì… ë° ID ë°˜í™˜
                execute_values(self.cursor, insert_query, values)
                inserted = self.cursor.fetchall()

                # doc_id â†’ id ë§¤í•‘ ì¶”ê°€
                for db_id, doc_id in inserted:
                    doc_id_map[doc_id] = db_id

                total_inserted += len(inserted)

                if (i + batch_size) % 10000 == 0:
                    print(f"   ì§„í–‰: {i + batch_size:,}/{len(new_sources):,}")

            # ì»¤ë°‹
            self.conn.commit()
            print(f"âœ… {total_inserted:,}ê°œ ë¬¸ì„œ ì†ŒìŠ¤ ì‚½ì… ì™„ë£Œ")

            # ê¸°ì¡´ + ì‹ ê·œ ë§¤í•‘ ë³‘í•©
            doc_id_map.update(existing_docs)

            return doc_id_map

        except Exception as e:
            self.conn.rollback()
            print(f"âŒ ë¬¸ì„œ ì†ŒìŠ¤ ì‚½ì… ì‹¤íŒ¨: {e}")
            raise

    def insert_document_chunks(
        self,
        chunks_df: pd.DataFrame,
        batch_size: int = 1000,
        skip_duplicates: bool = True
    ) -> int:
        """ë¬¸ì„œ ì²­í¬ë¥¼ vector_db.document_chunks í…Œì´ë¸”ì— ì‚½ì…

        Args:
            chunks_df: ì²­í¬ DataFrame
            batch_size: ë°°ì¹˜ í¬ê¸°
            skip_duplicates: ì¤‘ë³µ ìŠ¤í‚µ ì—¬ë¶€

        Returns:
            ì‚½ì…ëœ ì²­í¬ ìˆ˜
        """
        print("\n" + "=" * 80)
        print("ğŸ“ ë¬¸ì„œ ì²­í¬ ì‚½ì… ì¤‘...")
        print("=" * 80)

        try:
            # ê¸°ì¡´ chunk_id ì¡°íšŒ (skip_duplicatesê°€ Trueì¸ ê²½ìš°)
            existing_chunks = set()
            if skip_duplicates:
                self.cursor.execute("SELECT chunk_id FROM vector_db.document_chunks")
                existing_chunks = {row[0] for row in self.cursor.fetchall()}
                print(f"   ê¸°ì¡´ ì²­í¬: {len(existing_chunks):,}ê°œ")

            # ì‹ ê·œ ì²­í¬ë§Œ í•„í„°ë§
            if skip_duplicates:
                new_chunks = chunks_df[~chunks_df['chunk_id'].isin(existing_chunks)]
                print(f"   ì‹ ê·œ ì²­í¬: {len(new_chunks):,}ê°œ")
            else:
                new_chunks = chunks_df

            if len(new_chunks) == 0:
                print("âœ… ëª¨ë“  ì²­í¬ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤")
                return 0

            # ë°°ì¹˜ ì‚½ì…
            insert_query = """
                INSERT INTO vector_db.document_chunks
                (chunk_id, doc_id, chunk_type, section_path, structured_data, natural_text,
                 corp_name, document_name, rcept_dt, next_context, doc_type, data_category,
                 fiscal_year, keywords, token_count, metadata)
                VALUES %s
                ON CONFLICT (chunk_id) DO NOTHING
            """

            total_inserted = 0

            for i in range(0, len(new_chunks), batch_size):
                batch = new_chunks.iloc[i:i+batch_size]

                # ë°ì´í„° ì¤€ë¹„
                values = []
                for _, row in batch.iterrows():
                    # metadataì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ
                    metadata = row['metadata'] if isinstance(row['metadata'], dict) else json.loads(row['metadata'])

                    values.append((
                        row['chunk_id'],
                        row['doc_id'],
                        row['chunk_type'],
                        row.get('section_path', ''),
                        json.dumps(row.get('structured_data', {})),
                        row['natural_text'],
                        metadata.get('corp_name'),
                        metadata.get('document_name'),
                        metadata.get('rcept_dt'),
                        metadata.get('next_context'),
                        metadata.get('doc_type'),
                        metadata.get('data_category'),
                        int(metadata['fiscal_year']) if metadata.get('fiscal_year') and pd.notna(metadata.get('fiscal_year')) else None,
                        metadata.get('keywords', []),
                        int(metadata['token_count']) if metadata.get('token_count') else None,
                        json.dumps(metadata)
                    ))

                # ì‚½ì…
                execute_values(self.cursor, insert_query, values)
                total_inserted += len(values)

                if (i + batch_size) % 10000 == 0 or i == 0:
                    print(f"   ì§„í–‰: {i + batch_size:,}/{len(new_chunks):,} ì²­í¬ ì‚½ì…ë¨")

            # ì»¤ë°‹
            self.conn.commit()
            print(f"âœ… {total_inserted:,}ê°œ ì²­í¬ ì‚½ì… ì™„ë£Œ")

            return total_inserted

        except Exception as e:
            self.conn.rollback()
            print(f"âŒ ì²­í¬ ì‚½ì… ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            raise

    def get_statistics(self):
        """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ"""
        print("\n" + "=" * 80)
        print("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ í†µê³„")
        print("=" * 80)

        try:
            # ë¬¸ì„œ ì†ŒìŠ¤ ìˆ˜
            self.cursor.execute("SELECT COUNT(*) FROM vector_db.document_sources")
            doc_count = self.cursor.fetchone()[0]
            print(f"   ë¬¸ì„œ ì†ŒìŠ¤: {doc_count:,}ê°œ")

            # ì²­í¬ ìˆ˜
            self.cursor.execute("SELECT COUNT(*) FROM vector_db.document_chunks")
            chunk_count = self.cursor.fetchone()[0]
            print(f"   ì²­í¬: {chunk_count:,}ê°œ")

            # ê¸°ì—…ë³„ í†µê³„
            self.cursor.execute("""
                SELECT corp_name, COUNT(*) as chunk_count
                FROM vector_db.document_chunks
                GROUP BY corp_name
                ORDER BY chunk_count DESC
                LIMIT 10
            """)
            print("\n   ê¸°ì—…ë³„ ì²­í¬ ìˆ˜ (TOP 10):")
            for corp_name, count in self.cursor.fetchall():
                print(f"      {corp_name}: {count:,}ê°œ")

            # ë¬¸ì„œ ìœ í˜•ë³„ í†µê³„
            self.cursor.execute("""
                SELECT doc_type, COUNT(*) as chunk_count
                FROM vector_db.document_chunks
                GROUP BY doc_type
                ORDER BY chunk_count DESC
            """)
            print("\n   ë¬¸ì„œ ìœ í˜•ë³„ ì²­í¬ ìˆ˜:")
            for doc_type, count in self.cursor.fetchall():
                print(f"      {doc_type}: {count:,}ê°œ")

        except Exception as e:
            print(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="Parquet íŒŒì¼ì„ PostgreSQL vector_db ìŠ¤í‚¤ë§ˆë¡œ ë¡œë“œ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê¸°ë³¸ ë¡œë“œ
  python parquet_to_postgres.py

  # ì»¤ìŠ¤í…€ ë°ì´í„°ë² ì´ìŠ¤
  python parquet_to_postgres.py --host localhost --port 5432 --database mydb --user myuser --password mypass

  # ë°°ì¹˜ í¬ê¸° ì¡°ì •
  python parquet_to_postgres.py --batch-size 5000
        """
    )

    parser.add_argument('--host', type=str, default='localhost', help='PostgreSQL í˜¸ìŠ¤íŠ¸')
    parser.add_argument('--port', type=int, default=5432, help='PostgreSQL í¬íŠ¸')
    parser.add_argument('--database', type=str, default='postgres', help='ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„')
    parser.add_argument('--user', type=str, default='postgres', help='ì‚¬ìš©ì ì´ë¦„')
    parser.add_argument('--password', type=str, default='postgres', help='ë¹„ë°€ë²ˆí˜¸')
    parser.add_argument('--batch-size', type=int, default=1000, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--parquet-file', type=str, help='Parquet íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: data/parquet/chunks.parquet)')

    args = parser.parse_args()

    # ê²½ë¡œ ì„¤ì •
    if args.parquet_file:
        parquet_path = Path(args.parquet_file)
    else:
        script_dir = Path(__file__).parent
        project_root = script_dir.parent.parent.parent
        parquet_path = project_root / "data" / "parquet" / "chunks.parquet"

    if not parquet_path.exists():
        print(f"âŒ Parquet íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {parquet_path}")
        print(f"   ë¨¼ì € jsonl_to_parquet.pyë¥¼ ì‹¤í–‰í•˜ì—¬ Parquet íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”")
        sys.exit(1)

    # ë¡œë” ìƒì„±
    loader = ParquetToPostgresLoader(
        host=args.host,
        port=args.port,
        database=args.database,
        user=args.user,
        password=args.password
    )

    try:
        # ì—°ê²°
        if not loader.connect():
            sys.exit(1)

        # Parquet íŒŒì¼ ë¡œë“œ
        df = loader.load_parquet(parquet_path)
        if df is None:
            sys.exit(1)

        # ë¬¸ì„œ ì†ŒìŠ¤ ì¶”ì¶œ ë° ì‚½ì…
        sources_df = loader.extract_document_sources(df)
        doc_id_map = loader.insert_document_sources(sources_df, batch_size=args.batch_size)

        # ì²­í¬ ì‚½ì…
        inserted_count = loader.insert_document_chunks(df, batch_size=args.batch_size)

        # í†µê³„ ì¶œë ¥
        loader.get_statistics()

        print("\n" + "=" * 80)
        print("âœ… ë¡œë”© ì™„ë£Œ!")
        print("=" * 80)

    except Exception as e:
        print(f"\nâŒ ë¡œë”© ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        loader.disconnect()


if __name__ == "__main__":
    main()
