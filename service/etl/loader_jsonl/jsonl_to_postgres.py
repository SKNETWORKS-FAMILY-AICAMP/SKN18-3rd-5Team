#!/usr/bin/env python3
"""
JSONL to PostgreSQL Loader - JSONL íŒŒì¼ì„ ì§ì ‘ PostgreSQLì— ë¡œë“œ

JSONL íŒŒì¼ë“¤ì„ PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ì— ì§ì ‘ ë¡œë“œí•˜ëŠ” ë„êµ¬
- JSONL íŒŒì¼ ì§ì ‘ ì½ê¸°
- ì„ë² ë”© ìƒì„± ë° ì €ì¥
- pgvectorë¥¼ í†µí•œ ë²¡í„° ê²€ìƒ‰ ì§€ì›
"""

import json
import argparse
import logging
import time
from pathlib import Path
from typing import List, Optional, Dict, Any, Generator
import pandas as pd
import psycopg2
from psycopg2.extras import execute_values
import os
from datetime import datetime

# ë¡œê¹… ì„¤ì • - ê°„ë‹¨í•˜ê²Œ ì„¤ì •
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    force=True
)
logger = logging.getLogger(__name__)


class JSONLToPostgresLoader:
    """JSONL íŒŒì¼ì„ PostgreSQLì— ë¡œë“œí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(
        self, 
        db_config: Dict[str, str],
        batch_size: int = 1000
    ):
        """
        Args:
            db_config: PostgreSQL ì—°ê²° ì„¤ì •
            batch_size: ë°°ì¹˜ í¬ê¸°
        """
        self.db_config = db_config
        self.batch_size = batch_size
        self.conn = None
        
    def _connect_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        try:
            self.conn = psycopg2.connect(**self.db_config)
            self.conn.autocommit = True
            logger.info("PostgreSQL ì—°ê²° ì„±ê³µ")
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
    
    
    def _create_tables(self):
        """í•„ìš”í•œ í…Œì´ë¸” ìƒì„±"""
        cursor = self.conn.cursor()
        
        # pgvector í™•ì¥ í™œì„±í™”
        cursor.execute("CREATE EXTENSION IF NOT EXISTS vector;")
        
        # ì²­í¬ í…Œì´ë¸” ìƒì„±
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS chunks (
                id SERIAL PRIMARY KEY,
                chunk_id VARCHAR(255) UNIQUE NOT NULL,
                doc_id VARCHAR(255),
                chunk_type VARCHAR(50),
                section_path TEXT,
                natural_text TEXT,
                structured_data JSONB,
                metadata JSONB,
                token_count INTEGER,
                merged_count INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
        """)
        
        
        # ì¸ë±ìŠ¤ ìƒì„±
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_chunks_corp_name ON chunks ((metadata->>'corp_name'));")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_chunks_doc_type ON chunks ((metadata->>'doc_type'));")
        
        cursor.close()
        logger.info("í…Œì´ë¸” ë° ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    # íŒŒì¼ë³„ ì§„í–‰ ìƒí™© ì¶”ì , ì§„í–‰ë¥  í‘œì‹œ
    def _read_jsonl_files(self, jsonl_dir: Path) -> Generator[Tuple[Dict[str, Any], Path, int, int], None, None]:
        """JSONL íŒŒì¼ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì½ê¸°"""
        jsonl_files = sorted(jsonl_dir.glob("*_chunks.jsonl"))
        total_files = len(jsonl_files)
        print(f"ğŸ“‚ ì²˜ë¦¬í•  JSONL íŒŒì¼ ìˆ˜: {total_files}ê°œ")
        logger.info(f"ğŸ“‚ ì²˜ë¦¬í•  JSONL íŒŒì¼ ìˆ˜: {total_files}ê°œ")
        import sys
        sys.stdout.flush()
        
        for file_idx, file_path in enumerate(jsonl_files, 1):
            print(f"({file_idx}/{total_files}): {file_path.name}")
            sys.stdout.flush()
            chunk_count = 0
            
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_no, line in enumerate(f, 1):
                    try:
                        data = json.loads(line.strip())
                        chunk_count += 1
                        
                        
                        yield data, file_path, file_idx, total_files
                    except json.JSONDecodeError as e:
                        logger.warning(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨ {file_path.name}:{line_no}: {e}")
                        continue
    
    def _insert_chunks_batch(self, chunks_batch: List[Dict[str, Any]]):
        """ì²­í¬ ë°°ì¹˜ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì‚½ì…"""
        cursor = self.conn.cursor()
        
        # ì¤‘ë³µ ì œê±°: chunk_id ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
        seen_chunk_ids = set()
        unique_chunks = []
        for chunk in chunks_batch:
            chunk_id = chunk.get('chunk_id')
            if chunk_id and chunk_id not in seen_chunk_ids:
                seen_chunk_ids.add(chunk_id)
                unique_chunks.append(chunk)
        
        logger.info(f"ì¤‘ë³µ ì œê±°: {len(chunks_batch)} â†’ {len(unique_chunks)} ì²­í¬")
        
        # ì²­í¬ ë°ì´í„° ì¤€ë¹„
        chunk_data = []
        for chunk in unique_chunks:
            chunk_data.append((
                chunk.get('chunk_id'),
                chunk.get('doc_id'),
                chunk.get('chunk_type'),
                chunk.get('section_path'),
                chunk.get('natural_text'),
                json.dumps(chunk.get('structured_data', {})),
                json.dumps(chunk.get('metadata', {})),
                chunk.get('token_count'),
                chunk.get('merged_count')
            ))
        
        # ì²­í¬ ì‚½ì… (ì¤‘ë³µ ì‹œ ë¬´ì‹œ) - ê°œë³„ INSERTë¡œ ì²˜ë¦¬
        insert_query = """
            INSERT INTO chunks (chunk_id, doc_id, chunk_type, section_path, natural_text, 
                              structured_data, metadata, token_count, merged_count)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (chunk_id) DO NOTHING
        """
        
        for chunk_row in chunk_data:
            try:
                cursor.execute(insert_query, chunk_row)
            except Exception as e:
                logger.warning(f"ì²­í¬ ì‚½ì… ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
                continue
        cursor.close()
    
    
    
    def load_jsonl_to_postgres(self, jsonl_dir: Path):
        """JSONL íŒŒì¼ë“¤ì„ PostgreSQLì— ë¡œë“œ"""
        logger.info("JSONL to PostgreSQL ë¡œë”© ì‹œì‘ (ì²­í¬ ë°ì´í„°ë§Œ)")
        logger.info(f"ğŸ“ ëŒ€ìƒ ë””ë ‰í† ë¦¬: {jsonl_dir}")
        logger.info(f"ğŸ”§ ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
        
        # ì´ˆê¸°í™”
        print("ğŸ”Œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘...")
        logger.info("ğŸ”Œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘...")
        self._connect_db()
        print("ğŸ“‹ í…Œì´ë¸” ìƒì„± ì¤‘...")
        logger.info("ğŸ“‹ í…Œì´ë¸” ìƒì„± ì¤‘...")
        self._create_tables()
        
        # í†µê³„ ë³€ìˆ˜
        total_chunks = 0
        start_time = time.time()
        
        # ë°°ì¹˜ ì²˜ë¦¬
        chunks_batch = []
        
        try:
            print("ğŸ“– JSONL íŒŒì¼ ì½ê¸° ì‹œì‘...")
            logger.info("ğŸ“– JSONL íŒŒì¼ ì½ê¸° ì‹œì‘...")
            import sys
            sys.stdout.flush()
            last_file_idx = 0
            total_files_count = 0
            # tqdm ëŒ€ì‹  ì§ì ‘ ì§„í–‰ ìƒí™© í‘œì‹œ
            for chunk, file_path, file_idx, total_files in self._read_jsonl_files(jsonl_dir):
                chunks_batch.append(chunk)
                total_chunks += 1
                last_file_idx = file_idx
                total_files_count = total_files
                
                # 1,000ê°œë§ˆë‹¤ ì „ì²´ ì§„í–‰ ìƒí™© ë¡œê·¸
                if total_chunks % 5000 == 0:
                    elapsed_time = time.time() - start_time
                    rate = total_chunks / elapsed_time if elapsed_time > 0 else 0
                    print(f"    ì „ì²´ ì§„í–‰: {total_chunks:,}ê°œ ì²­í¬ ì²˜ë¦¬ë¨ (ì†ë„: {rate:.1f} ì²­í¬/ì´ˆ)")
                    import sys
                    sys.stdout.flush()
                
                # ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ ì²˜ë¦¬
                if len(chunks_batch) >= self.batch_size:
                    try:
                        self._insert_chunks_batch(chunks_batch)
                        
                        # ë°°ì¹˜ ì»¤ë°‹
                        self.conn.commit()
                        
                        # íŒŒì¼ ì²˜ë¦¬ ì§„í–‰ë¥  ê³„ì‚°
                        current_file_idx = last_file_idx
                        file_progress = (current_file_idx / total_files_count) * 100 if total_files_count else 0
                        
                        # ì „ì²´ ì²­í¬ ìˆ˜ ì¶”ì • (íŒŒì¼ë‹¹ í‰ê·  1000ê°œ ì²­í¬ë¡œ ì¶”ì •)
                        estimated_total_chunks = total_files_count * 1000
                        chunk_progress = (total_chunks / estimated_total_chunks) * 100 if estimated_total_chunks else 0
                        
                        print(f"ğŸ“Š íŒŒì¼ ì§„í–‰: {current_file_idx}/{total_files_count} ({file_progress:.1f}%) | ì²­í¬: ({total_chunks:,}/{estimated_total_chunks:,}) {chunk_progress:.1f}%")
                        import sys
                        sys.stdout.flush()
                        
                    except Exception as batch_error:
                        self.conn.rollback()
                        logger.error(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨ (ë¡¤ë°±): {batch_error}")
                        raise
                    
                    chunks_batch = []
            
            # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
            if chunks_batch:
                batch_start_time = time.time()
                try:
                    logger.info(f"ğŸ”„ ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(chunks_batch)}ê°œ ì²­í¬")
                    
                    logger.info("ğŸ’¾ ì²­í¬ ë°ì´í„°ë² ì´ìŠ¤ ì‚½ì… ì¤‘...")
                    self._insert_chunks_batch(chunks_batch)
                    
                    # ë§ˆì§€ë§‰ ë°°ì¹˜ ì»¤ë°‹
                    self.conn.commit()
                    
                except Exception as batch_error:
                    self.conn.rollback()
                    logger.error(f"âŒ ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨ (ë¡¤ë°±): {batch_error}")
                    raise
            
            total_time = time.time() - start_time
            print(f"    ë¡œë”© ì™„ë£Œ: ì´ {total_chunks:,}ê°œ ì²­í¬ ì²˜ë¦¬ (ì´ ì†Œìš”ì‹œê°„: {total_time/60:.1f}ë¶„)")
            logger.info(f"   ë¡œë”© ì™„ë£Œ: ì´ {total_chunks:,}ê°œ ì²­í¬ ì²˜ë¦¬ (ì´ ì†Œìš”ì‹œê°„: {total_time/60:.1f}ë¶„)")
            import sys
            sys.stdout.flush()
            
        except Exception as e:
            self.conn.rollback()
            logger.error(f"ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
        finally:
            if self.conn:
                self.conn.close()
    
    def get_stats(self) -> Dict[str, Any]:
        """ë¡œë”© í†µê³„ ì¡°íšŒ"""
        if not self.conn:
            self._connect_db()
        
        cursor = self.conn.cursor()
        
        # ì²­í¬ ìˆ˜ ì¡°íšŒ
        cursor.execute("SELECT COUNT(*) FROM chunks;")
        chunk_count = cursor.fetchone()[0]
        
        # ëª¨ë¸ë³„ ì„ë² ë”© ìˆ˜ ì¡°íšŒ
        model_name = self.embedding_model.replace("/", "_").replace("-", "_")
        cursor.execute(f"SELECT COUNT(*) FROM embeddings_{model_name};")
        embedding_count = cursor.fetchone()[0]
        
        # ê¸°ì—…ë³„ í†µê³„
        cursor.execute("""
            SELECT metadata->>'corp_name' as corp_name, COUNT(*) as count
            FROM chunks 
            WHERE metadata->>'corp_name' IS NOT NULL
            GROUP BY metadata->>'corp_name'
            ORDER BY count DESC
            LIMIT 10;
        """)
        corp_stats = cursor.fetchall()
        
        cursor.close()
        
        return {
            'total_chunks': chunk_count,
            'total_embeddings': embedding_count,
            'model_name': self.embedding_model,
            'top_corporations': corp_stats
        }


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="JSONL íŒŒì¼ë“¤ì„ PostgreSQLì— ë¡œë“œ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ê¸°ë³¸ ì‚¬ìš©
  python jsonl_to_postgres.py --jsonl-dir data/transform/final
  
  # ë°°ì¹˜ í¬ê¸° ì¡°ì •
  python jsonl_to_postgres.py --jsonl-dir data/transform/final --batch-size 500
  
  # ì²­í¬ ë°ì´í„°ë§Œ ë¡œë“œ (ì„ë² ë”©ì€ ë³„ë„ë¡œ ìƒì„±)
  python jsonl_to_postgres.py --jsonl-dir data/transform/final
        """
    )
    
    parser.add_argument(
        '--jsonl-dir',
        type=Path,
        required=True,
        help='JSONL íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=1000,
        help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 1000)'
    )
    
    
    parser.add_argument(
        '--db-host',
        type=str,
        default=os.getenv('POSTGRES_HOST', 'localhost'),
        help='PostgreSQL í˜¸ìŠ¤íŠ¸'
    )
    
    parser.add_argument(
        '--db-port',
        type=str,
        default=os.getenv('POSTGRES_PORT', '5432'),
        help='PostgreSQL í¬íŠ¸'
    )
    
    parser.add_argument(
        '--db-name',
        type=str,
        default=os.getenv('POSTGRES_DB', 'skn_project'),
        help='ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„'
    )
    
    parser.add_argument(
        '--db-user',
        type=str,
        default=os.getenv('POSTGRES_USER', 'postgres'),
        help='ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©ì'
    )
    
    parser.add_argument(
        '--db-password',
        type=str,
        default=os.getenv('POSTGRES_PASSWORD', 'post1234'),
        help='ë°ì´í„°ë² ì´ìŠ¤ ë¹„ë°€ë²ˆí˜¸'
    )
    
    args = parser.parse_args()
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
    db_config = {
        'host': args.db_host,
        'port': args.db_port,
        'database': args.db_name,
        'user': args.db_user,
        'password': args.db_password
    }
    
    # ë¡œë” ìƒì„± ë° ì‹¤í–‰
    loader = JSONLToPostgresLoader(
        db_config=db_config,
        batch_size=args.batch_size
    )
    
    try:
        loader.load_jsonl_to_postgres(args.jsonl_dir)
        
        # í†µê³„ ì¶œë ¥
        stats = loader.get_stats()
        print(f"\nğŸ“Š ë¡œë”© í†µê³„:")
        print(f"   ì´ ì²­í¬ ìˆ˜: {stats['total_chunks']:,}")
        print(f"   ì´ ì„ë² ë”© ìˆ˜: {stats['total_embeddings']:,}")
        print(f"   ì‚¬ìš© ëª¨ë¸: {stats['model_name']}")
        print(f"   ìƒìœ„ ê¸°ì—…ë“¤:")
        for corp_name, count in stats['top_corporations']:
            print(f"     - {corp_name}: {count:,}ê°œ")
            
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return 1
    
    return 0


# CLI ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def load_jsonl_files(jsonl_dir: str, batch_size: int = 1000):
    """JSONL íŒŒì¼ ë¡œë”© (CLIìš©) - ì§ì ‘ í˜¸ì¶œ"""
    from pathlib import Path
    
    print(f"ğŸ“ JSONL íŒŒì¼ ë¡œë”©: {jsonl_dir}")
    logger.info(f"ğŸ“ JSONL íŒŒì¼ ë¡œë”©: {jsonl_dir}")
    
    try:
        # ê¸°ë³¸ DB ì„¤ì •
        db_config = {
            'host': 'localhost',
            'port': '5432',
            'database': 'skn_project',
            'user': 'postgres',
            'password': 'post1234'
        }
        
        # ì§ì ‘ í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±í•˜ì—¬ ë¡œë”©
        loader = JSONLToPostgresLoader(db_config=db_config, batch_size=batch_size)
        loader.load_jsonl_to_postgres(Path(jsonl_dir))
        print("âœ… JSONL íŒŒì¼ ë¡œë”© ì™„ë£Œ")
        logger.info("âœ… JSONL íŒŒì¼ ë¡œë”© ì™„ë£Œ")
        return True
    except Exception as e:
        print(f"âŒ JSONL íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
        logger.error(f"âŒ JSONL íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
        return False


def get_loading_stats():
    """ë¡œë”© í†µê³„ ì¡°íšŒ (CLIìš©)"""
    import subprocess
    
    logger.info("ğŸ“Š ë¡œë”© í†µê³„ ì¡°íšŒ")
    
    try:
        # ì²­í¬ ìˆ˜ ì¡°íšŒ
        result = subprocess.run(
            ["docker", "exec", "SKN18-3rd", "psql", "-U", "postgres", "-d", "skn_project", "-c", 
             "SELECT COUNT(*) as chunk_count FROM chunks;"],
            capture_output=True,
            text=True,
            check=True
        )
        
        logger.info("ğŸ“‹ ì²­í¬ í†µê³„:")
        logger.info(result.stdout)
        
        # ì„ë² ë”© í…Œì´ë¸”ë³„ í†µê³„
        embedding_tables = ["embeddings_multilingual_e5_small", "embeddings_kakaobank", "embeddings_fine5"]
        
        for table in embedding_tables:
            try:
                result = subprocess.run(
                    ["docker", "exec", "SKN18-3rd", "psql", "-U", "postgres", "-d", "skn_project", "-c", 
                     f"SELECT COUNT(*) as {table}_count FROM {table};"],
                    capture_output=True,
                    text=True,
                    check=True
                )
                logger.info(f"ğŸ“‹ {table} í†µê³„:")
                logger.info(result.stdout)
            except subprocess.CalledProcessError:
                logger.info(f"ğŸ“‹ {table}: í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ë¹„ì–´ìˆìŒ")
        
        return True
        
    except subprocess.CalledProcessError as e:
        logger.error(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e.stderr}")
        return False


def clear_data():
    """ë°ì´í„° ì‚­ì œ (CLIìš©)"""
    import subprocess
    
    logger.warning("âš ï¸  ë°ì´í„° ì‚­ì œ (ëª¨ë“  ë°ì´í„°ê°€ ì‚­ì œë©ë‹ˆë‹¤)")
    
    try:
        # ì„ë² ë”© í…Œì´ë¸” ì‚­ì œ
        embedding_tables = ["embeddings_multilingual_e5_small", "embeddings_kakaobank", "embeddings_fine5"]
        
        for table in embedding_tables:
            result = subprocess.run(
                ["docker", "exec", "SKN18-3rd", "psql", "-U", "postgres", "-d", "skn_project", "-c", 
                 f"TRUNCATE TABLE {table} CASCADE;"],
                capture_output=True,
                text=True,
                check=True
            )
        
        # ì²­í¬ í…Œì´ë¸” ì‚­ì œ
        result = subprocess.run(
            ["docker", "exec", "SKN18-3rd", "psql", "-U", "postgres", "-d", "skn_project", "-c", 
             "TRUNCATE TABLE chunks CASCADE;"],
            capture_output=True,
            text=True,
            check=True
        )
        
        logger.info("âœ… ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
        return True
        
    except subprocess.CalledProcessError as e:
        logger.error(f"âŒ ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {e.stderr}")
        return False


if __name__ == "__main__":
    exit(main())
