#!/usr/bin/env python3
"""
====================================================================================
Transform Pipeline - Step 1: êµ¬ì¡°í™” ë° 1ì°¨ ì²­í‚¹
====================================================================================

[íŒŒì´í”„ë¼ì¸ ìˆœì„œ]
1. parser.py      â†’ ë§ˆí¬ë‹¤ìš´ì„ êµ¬ì¡°í™”ëœ ì²­í¬ë¡œ ë³€í™˜
2. normalizer.py â†’ ë°ì´í„° ì •ê·œí™” ë° ìì—°ì–´ í’ˆì§ˆ ê°œì„ 
3. chunker.py         â†’ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ë° ë©”íƒ€ë°ì´í„° ê°•í™”

[ì´ íŒŒì¼ì˜ ì—­í• ]
- ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ í…Œì´ë¸”/í…ìŠ¤íŠ¸ ë‹¨ìœ„ë¡œ íŒŒì‹±
- í…Œì´ë¸” í–‰ì„ ê¸°ë³¸ ìì—°ì–´ë¡œ ë³€í™˜ (ìµœì†Œí•œì˜ ì²˜ë¦¬ë§Œ)
- ì„¹ì…˜ ê²½ë¡œ ë° ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
- ì›ë³¸ ë°ì´í„° ë³´ì¡´ (structured_data)

[ì…ë ¥]
- data/markdown/*.md (XMLì—ì„œ ë³€í™˜ëœ ë§ˆí¬ë‹¤ìš´)

[ì¶œë ¥]
- data/transform/parser/*_chunks.jsonl (1ì°¨ ì²­í¬)
====================================================================================
"""

import re
import sys
import json
from pathlib import Path
from typing import Dict, Any, List

# ê³µí†µ ëª¨ë“ˆ
from utils import Chunk, write_jsonl, get_transform_paths


# ==========================================
# Table -> Natural Language (Convert)
# ==========================================


class TableRowConverter:
    """í…Œì´ë¸” í–‰ì„ ìì—°ì–´ë¡œ ë³€í™˜"""
    
    @staticmethod
    def convert(headers: List[str], row: List[str], section_path: str) -> str:
        """
        í…Œì´ë¸” í–‰ì„ ìì—°ì–´ë¡œ ë³€í™˜
        ì„¹ì…˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•œ ë³€í™˜
        """
        
        # ë¹ˆ ë°ì´í„° ì²´í¬
        non_empty_values = [v for v in row if v and v.strip() and v != '-']
        if not non_empty_values:
            return ""
        
        # ë³´ê³ ì„œ(ì‚¬ì—…ë³´ê³ ì„œ, ë¶„ê¸°ë³´ê³ ì„œ, ì£¼ìš”ì‚¬í•­ë³´ê³ ì„œ(+ ìê¸°ì·¨ë“ì£¼ì‹ê²°ì •ë³´ê³ ì„œ)) -> í…Œì´ë¸” ìœ í˜• íŒŒì•… í•„ìš”.

        # ì„¹ì…˜ë³„ íŠ¹í™” ë³€í™˜
        if "ì£¼ì‹ì˜ ì´ìˆ˜" in section_path:
            return TableRowConverter._convert_stock_table(headers, row) # ì£¼ì‹ í…Œì´ë¸”
        elif "ì¬ë¬´" in section_path or "ì†ìµ" in section_path:
            return TableRowConverter._convert_financial_table(headers, row) # ì¬ë¬´ í…Œì´ë¸”
        else:
            return TableRowConverter._convert_generic_table(headers, row) # ì¼ë°˜ í…Œì´ë¸”
    
    # ==========================================
    # 1. ì£¼ì‹ ì—´ í…Œì´ë¸”
    # ==========================================
    @staticmethod
    def _convert_stock_table(headers: List[str], row: List[str]) -> str:
        """ì£¼ì‹ í…Œì´ë¸” ì „ìš© ë³€í™˜ (ê¸°ë³¸ í˜•íƒœë§Œ)"""
        
        # êµ¬ ë¶„ ì—´ ì°¾ê¸°
        gubun_indices = [i for i, h in enumerate(headers) if 'êµ¬ ë¶„' in h]
        
        # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
        categories = []
        for idx in gubun_indices:
            if idx < len(row) and row[idx] and row[idx] != '-':
                categories.append(row[idx])
        
        # ë°ì´í„° ì¶”ì¶œ
        data_map = {}
        for i, header in enumerate(headers):
            if 'êµ¬ ë¶„' not in header and i < len(row) and row[i] and row[i] != '-':
                value = row[i].strip()
                header_clean = header.strip()
                
                if value:
                    data_map[header_clean] = value
        
        # ìì—°ì–´ ìƒì„± (ê°„ê²°í•˜ê²Œ, ì¡°ì‚¬ ì—†ì´)
        if categories:
            category_text = " - ".join(categories)
            
            if data_map:
                # "í‚¤: ê°’" í˜•íƒœë¡œ ê°„ê²°í•˜ê²Œ (ì¡°ì‚¬ ì œê±°)
                data_items = [f"{k} {v}" for k, v in data_map.items()]
                return f"{category_text}: {', '.join(data_items)}"
            else:
                return category_text
        
        return ""
    

    # ==========================================
    # 2. ì¬ë¬´ ì—´ í…Œì´ë¸”
    # ==========================================
    @staticmethod
    def _convert_financial_table(headers: List[str], row: List[str]) -> str:
        """ì¬ë¬´ í…Œì´ë¸” ì „ìš© ë³€í™˜ (ê¸°ë³¸ í˜•íƒœë§Œ, ë‹¨ìœ„ ë³€í™˜ì€ normalizerì—ì„œ)"""
        
        # ì²« ë²ˆì§¸ ì—´ì€ ë³´í†µ í•­ëª©ëª…
        if not row or not row[0] or row[0] == '-':
            return ""
        
        item_name = row[0].strip()
        
        # ë‚˜ë¨¸ì§€ ì—´ì€ ê°’ (ê°„ê²°í•˜ê²Œ, ì¡°ì‚¬ ì—†ì´)
        values = []
        for i in range(1, min(len(headers), len(row))):
            if row[i] and row[i] != '-':
                header = headers[i].strip()
                value = row[i].strip()
                # "í•­ëª©: ê°’" í˜•íƒœë¡œ ê°„ê²°í•˜ê²Œ
                values.append(f"{header} {value}")
        
        if values:
            # "ê³¼ëª©ëª… - í•­ëª©1 ê°’1, í•­ëª©2 ê°’2" í˜•íƒœ
            return f"{item_name} - {', '.join(values)}"
        else:
            return item_name
    
    # ==========================================
    # 3. ì¼ë°˜ í…Œì´ë¸”
    # ==========================================
    @staticmethod
    def _convert_generic_table(headers: List[str], row: List[str]) -> str:
        """ì¼ë°˜ í…Œì´ë¸” ë³€í™˜ (ê°„ê²°í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ)"""
        
        items = []
        for i in range(min(len(headers), len(row))):
            if row[i] and row[i] != '-' and row[i].strip():
                header = headers[i].strip()
                value = row[i].strip()
                
                if header and value:
                    # "í‚¤: ê°’" í˜•íƒœë¡œ ê°„ê²°í•˜ê²Œ
                    items.append(f"{header}: {value}")
        
        if items:
            return ", ".join(items)
        else:
            return ""

# ==========================================
# Markdown Chunking
# ==========================================
class MarkdownChunker:
    """ë§ˆí¬ë‹¤ìš´ì„ ì²­í¬ë¡œ ë¶„í• """
    
    def __init__(self, markdown_content: str, doc_metadata: Dict[str, Any]):
        self.content = markdown_content
        self.doc_metadata = doc_metadata
        self.chunks = []
        self.current_section_path = []
    
    def process(self) -> List[Chunk]:
        """ë§ˆí¬ë‹¤ìš´ ì²˜ë¦¬"""
        
        lines = self.content.split('\n')
        
        i = 0
        while i < len(lines):
            line = lines[i]
            
            # YAML í—¤ë” ê±´ë„ˆë›°ê¸°
            if i == 0 and line.startswith('---'):
                while i < len(lines) and (i == 0 or not lines[i].startswith('---')):
                    i += 1
                i += 1
                continue
            
            # ì„¹ì…˜ í—¤ë”
            if line.startswith('#'):
                self._update_section(line)
                i += 1
                continue
            
            # í…Œì´ë¸”
            if line.startswith('|'):
                i = self._process_table(lines, i)
                continue
            
            # í…ìŠ¤íŠ¸
            if line.strip() and not line.startswith('#'):
                i = self._process_text(lines, i)
                continue
            
            i += 1
        
        return self.chunks
    
    def _update_section(self, header_line: str):
        """ì„¹ì…˜ ì—…ë°ì´íŠ¸"""
        level = header_line.count('#')
        title = header_line.lstrip('#').strip()
        
        # ë ˆë²¨ì— ë§ê²Œ ì¡°ì •
        while len(self.current_section_path) >= level:
            self.current_section_path.pop()
        
        self.current_section_path.append(title)
    
    def _process_table(self, lines: List[str], start_idx: int) -> int:
        """í…Œì´ë¸” ì²˜ë¦¬ - ë³µì¡í•œ êµ¬ì¡° ê°ì§€ ë° ì²˜ë¦¬"""
        
        # í…Œì´ë¸” ìœ„ì˜ ë‹¨ìœ„ ì •ë³´ ìˆ˜ì§‘ (í…Œì´ë¸” ë°”ë¡œ ìœ„ 3ì¤„ í™•ì¸)
        table_unit = None
        for check_idx in range(max(0, start_idx - 3), start_idx):
            if check_idx < len(lines):
                line = lines[check_idx].strip()
                # "ë‹¨ìœ„: ì²œì›", "(ë‹¨ìœ„: ë°±ë§Œì›)", "-(ë‹¨ìœ„: ì²œì›)", "(ë‹¨ìœ„ : ì²œì›/Ton)" ë“±
                # ê³µë°± í¬í•¨ íŒ¨í„´ ì§€ì›: (ë‹¨ìœ„ : ë°±ë§Œì›)
                unit_match = re.search(r'[\(\-]?\s*ë‹¨ìœ„\s*[:ï¼š\s]\s*([^\)/\)]+)', line, re.IGNORECASE)
                if unit_match:
                    table_unit = unit_match.group(1).strip()
                    # ë¹„í™”í ë‹¨ìœ„ëŠ” ì œì™¸ (Ton, %, ë¦¬í„°, ê°œ ë“±)
                    if any(non_monetary in table_unit for non_monetary in ['Ton', '%', 'ë¦¬í„°', 'ê°œ', 'ì£¼', 'ê±´', 'ëª…', 'íšŒ']):
                        table_unit = None
                        continue
                    # í™”í ë‹¨ìœ„ë§Œ ì¶”ì¶œ (ì›, ì²œì›, ë°±ë§Œì›, ì–µì›)
                    if 'ì›' in table_unit:
                        break
                    else:
                        table_unit = None
        
        # í…Œì´ë¸” ì¶”ì¶œ
        table_lines = []
        i = start_idx
        while i < len(lines) and (lines[i].startswith('|') or lines[i].strip() == ''):
            if lines[i].startswith('|'):
                table_lines.append(lines[i])
            i += 1
        
        if len(table_lines) < 3:
            return i
        
        # í…Œì´ë¸” êµ¬ì¡° ë¶„ì„
        table_type = self._analyze_table_structure(table_lines)
        
        if table_type == 'vertical':
            # ìˆ˜ì§ êµ¬ì¡° í…Œì´ë¸” (í•­ëª©ëª…ì´ ì²« ë²ˆì§¸ ì—´ì— ìˆìŒ)
            self._parse_vertical_table(table_lines, table_unit)
        else:
            # ì¼ë°˜ í…Œì´ë¸” (0í–‰ì´ í—¤ë”)
            self._parse_normal_table(table_lines, table_unit)
        
        return i
    
    def _analyze_table_structure(self, table_lines: List[str]) -> str:
        """í…Œì´ë¸” êµ¬ì¡° íŒë‹¨"""
        
        if len(table_lines) < 3:
            return 'normal'
        
        headers = [h.strip() for h in table_lines[0].split('|')[1:-1]]
        first_row = [c.strip() for c in table_lines[2].split('|')[1:-1]]
        
        # íŒ¨í„´ 1: í—¤ë” 0í–‰ì´ "1.", "2." ê°™ì€ ë²ˆí˜¸ë¡œ ì‹œì‘
        if headers and re.match(r'^\d+\.', headers[0]):
            return 'vertical'
        
        # íŒ¨í„´ 2: í—¤ë” 0í–‰ì´ "ì œ XX ê¸°" íŒ¨í„´
        if headers and re.match(r'^ì œ\s*\d+', headers[0]):
            return 'vertical'
        
        # íŒ¨í„´ 3: ì²« ë²ˆì§¸ ë°ì´í„° í–‰ì´ "1.", "2." ê°™ì€ ë²ˆí˜¸ë¡œ ì‹œì‘
        if first_row and re.match(r'^\d+\.', first_row[0]):
            return 'vertical'
        
        # íŒ¨í„´ 4: ì²« í–‰ì˜ ê°’ë“¤ì´ "ë§", "ì´ˆ", "ë¶€í„°", "ê¹Œì§€" ê°™ì€ í‚¤ì›Œë“œ
        if len(first_row) > 1:
            keyword_count = sum(1 for v in first_row if v in ['ë§', 'ì´ˆ', 'ë¶€í„°', 'ê¹Œì§€', 'ì‹œì‘ì¼', 'ì¢…ë£Œì¼'])
            if keyword_count >= 2:
                return 'vertical'
        
        # íŒ¨í„´ 5: í—¤ë”ì— ë™ì¼í•œ ê°’ì´ ë°˜ë³µë˜ë©´ ìˆ˜ì§ í…Œì´ë¸”
        # ì˜ˆ: "í•­ ëª©", "í•­ ëª©", "ê¸ˆ ì•¡"
        if len(headers) >= 2:
            header_counts = {}
            for h in headers:
                if h:
                    header_counts[h] = header_counts.get(h, 0) + 1
            # ë™ì¼í•œ í—¤ë”ê°€ 2ë²ˆ ì´ìƒ ë‚˜ì˜¤ë©´
            if any(count >= 2 for count in header_counts.values()):
                return 'vertical'
        
        # íŒ¨í„´ 6: ì—´ ê°œìˆ˜ê°€ 2-3ê°œì´ê³  ì²« ë²ˆì§¸ ì—´ì´ í•­ëª©ëª…ì²˜ëŸ¼ ë³´ì„
        if 2 <= len(headers) <= 3:
            # ë‘ ë²ˆì§¸ í–‰ë¶€í„° ì²« ë²ˆì§¸ ì—´ ê°’ë“¤ í™•ì¸
            first_col_values = []
            for row_idx in range(2, min(len(table_lines), 7)):  # ìµœëŒ€ 5ê°œ í–‰ í™•ì¸
                row = [c.strip() for c in table_lines[row_idx].split('|')[1:-1]]
                if row:
                    first_col_values.append(row[0])
            
            # ëŒ€ë¶€ë¶„ì´ í•­ëª©ëª…ì²˜ëŸ¼ ë³´ì´ë©´ ìˆ˜ì§ í…Œì´ë¸”
            item_like = sum(1 for v in first_col_values if 
                          ('ì¼' in v or 'ì' in v or 'ì•¡' in v or 'ëª…' in v or 'ìœ¨' in v or 'ëª©' in v or v.endswith('ì—¬ë¶€')))
            if item_like >= len(first_col_values) * 0.5:
                return 'vertical'
        
        return 'normal'
    
    def _parse_vertical_table(self, table_lines: List[str], table_unit: str = None):
        """ìˆ˜ì§ êµ¬ì¡° í…Œì´ë¸” íŒŒì‹± (í•­ëª©ëª…ì´ ì²« ë²ˆì§¸ ì—´)
        
        ì˜ˆì‹œ êµ¬ì¡° 1 (6ì—´):
        | 1. ê³„ì•½ê¸ˆì•¡ | 1. ê³„ì•½ê¸ˆì•¡ | 1. ê³„ì•½ê¸ˆì•¡ | 1,000,000,000 | 1,000,000,000 | 1,000,000,000 |
        | 2. ê³„ì•½ê¸°ê°„ | 2. ê³„ì•½ê¸°ê°„ | ì‹œì‘ì¼      | 2025-01-10    | 2025-01-10    | 2025-01-10    |
        
        ì˜ˆì‹œ êµ¬ì¡° 2 (3ì—´):
        | í•­ ëª©(1) | í•­ ëª©(2) | ê¸ˆ ì•¡ |
        | 1. ë°°ë‹¹ê°€ëŠ¥ì´ìµ | ê°€. ìˆœìì‚°ì•¡ | 193,082,198,582 |
        
        ì•ìª½ ì—´ë“¤ì€ í•­ëª©ëª… (ë³´í†µ ì¤‘ë³µ), ë’¤ìª½ ì—´ë“¤ì€ ë°ì´í„°
        """
        
        section_path = ' > '.join(self.current_section_path)
        
        # 0í–‰ íŒŒì‹±
        headers = [h.strip() for h in table_lines[0].split('|')[1:-1]]
        num_cols = len(headers)
        
        # 2í–‰ë¶€í„° ê° í–‰ ì²˜ë¦¬
        for row_idx in range(2, len(table_lines)):
            row = [c.strip() for c in table_lines[row_idx].split('|')[1:-1]]
            
            if not row:
                continue
            
            # ë°ì´í„° ì—´ ê°ì§€ (ë§ˆì§€ë§‰ì—ì„œë¶€í„° ì—­ìˆœ íƒìƒ‰)
            data_col_idx = -1
            for i in range(len(row) - 1, -1, -1):
                cell = row[i]
                
                # ë§ˆí¬ë‹¤ìš´ ë³¼ë“œ, ìˆ«ì, ë‚ ì§œ íŒ¨í„´ = ë°ì´í„° ì—´
                if '**' in cell or re.match(r'^\d{4}ë…„', cell) or re.match(r'^\d+,\d+', cell) or re.match(r'^\d+\.\d+', cell) or (cell and cell.replace(',', '').replace('-', '').isdigit()):
                    data_col_idx = i
                    break
            
            # ë°ì´í„° ì—´ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
            if data_col_idx == -1:
                continue
            
            # í•­ëª©ëª… ìˆ˜ì§‘ (ë°ì´í„° ì—´ ì´ì „ì˜ ëª¨ë“  ì—´, ì¤‘ë³µ ì œê±°)
            item_parts = []
            seen_items = set()
            
            for i in range(data_col_idx):
                cell = row[i]
                if cell and cell != '-':
                    # ì¤‘ë³µ ì œê±° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„)
                    if cell not in seen_items:
                        item_parts.append(cell)
                        seen_items.add(cell)
            
            # í•­ëª©ëª…ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
            if not item_parts:
                continue
            
            # í•­ëª©ëª… ì¡°í•©
            item_name = ' > '.join(item_parts)
            
            # ë°ì´í„° ì¶”ì¶œ
            value = row[data_col_idx].replace('**', '').strip()
            
            # ë¹ˆ ê°’ì´ë©´ ìŠ¤í‚µ (í‚¤ì™€ ê°’ì´ ê°™ì€ ê²½ìš°ëŠ” ìœ ì§€)
            if not value or value == '-':
                continue
            
            # êµ¬ì¡°í™” ë°ì´í„° ìƒì„±
            structured_data = {item_name: value}
            
            # ë‹¨ìœ„ ì ìš©í•˜ì—¬ natural_text ìƒì„±
            formatted_value = self._apply_unit_to_value(value, table_unit)
            
            # í‚¤ì— í¬í•¨ëœ ìˆ«ìë„ ë‹¨ìœ„ ë³€í™˜
            formatted_item_name = self._apply_unit_to_text(item_name, table_unit)
            natural_text = f"{formatted_item_name}: {formatted_value}"
            
            # ì²­í¬ ìƒì„±
            chunk = Chunk(
                chunk_id=self._generate_id(natural_text, 'table_row', section_path),
                doc_id=f"{self.doc_metadata.get('rcept_dt', '')}_{self.doc_metadata.get('corp_code', '')}",
                chunk_type='table_row',
                section_path=section_path,
                structured_data=structured_data,
                natural_text=natural_text,
                metadata={
                    'corp_name': self.doc_metadata.get('corp_name', ''),
                    'document_name': self.doc_metadata.get('document_name', ''),
                    'rcept_dt': self.doc_metadata.get('rcept_dt', ''),
                }
            )
            
            self.chunks.append(chunk)
    
    def _parse_normal_table(self, table_lines: List[str], table_unit: str = None):
        """ì¼ë°˜ í…Œì´ë¸” íŒŒì‹± (0í–‰ì´ í—¤ë”)"""
        
        section_path = ' > '.join(self.current_section_path)
        headers = [h.strip() for h in table_lines[0].split('|')[1:-1]]
        
        # ë°ì´í„° í–‰ ì²˜ë¦¬
        for row_idx in range(2, len(table_lines)):
            row = [c.strip() for c in table_lines[row_idx].split('|')[1:-1]]
            
            if len(row) != len(headers):
                continue
            
            # êµ¬ì¡°í™”
            structured_data = {}
            for j, header in enumerate(headers):
                if j < len(row):
                    structured_data[header] = row[j]
            
            # ìì—°ì–´ ë³€í™˜ (ë‹¨ìœ„ ì ìš©)
            natural_text = TableRowConverter.convert(headers, row, section_path)
            
            if not natural_text:
                continue
            
            # ë‹¨ìœ„ ì ìš©
            if table_unit:
                natural_text = self._apply_unit_to_text(natural_text, table_unit)
            
            # ì²­í¬ ìƒì„±
            chunk = Chunk(               
                chunk_id=self._generate_id(natural_text, 'table_row', section_path),
                doc_id=f"{self.doc_metadata.get('rcept_dt', '')}_{self.doc_metadata.get('corp_code', '')}",
                chunk_type='table_row',                
                section_path=section_path,
                structured_data=structured_data,
                natural_text=natural_text,
                metadata={
                    'corp_name': self.doc_metadata.get('corp_name', ''),
                    'document_name': self.doc_metadata.get('document_name', ''),
                    'rcept_dt': self.doc_metadata.get('rcept_dt', ''),
                }
            )
            
            self.chunks.append(chunk)
        
    def _apply_unit_to_value(self, value: str, table_unit: str) -> str:
        """ê°’ì— ë‹¨ìœ„ ì ìš©í•˜ì—¬ ìˆ«ìë¡œ ë³€í™˜ (ì½¤ë§ˆ ì—†ëŠ” 0 ì¶”ê°€)
        
        Args:
            value: ì›ë³¸ ê°’ (ì˜ˆ: "838,319" ë˜ëŠ” "32,711,600ì²œì›")
            table_unit: ë‹¨ìœ„ ì •ë³´ (ì˜ˆ: "ì²œì›", "ë°±ë§Œì›")
        
        Returns:
            ë‹¨ìœ„ê°€ ì ìš©ëœ ìˆ«ì (ì˜ˆ: "838319000" ë˜ëŠ” "32711600000")
        """
        # 1. ë¨¼ì € ê°’ ìì²´ì— ë‹¨ìœ„ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ë³€í™˜
        value = self._convert_inline_units(value)
        
        # 2. í…Œì´ë¸” ë‹¨ìœ„ ì ìš©
        if not table_unit:
            return value
        
        # ë‹¨ìœ„ ìŠ¹ìˆ˜ ê²°ì •
        unit_multiplier = 1
        if table_unit:
            if 'ì²œì›' in table_unit or 'ì²œ ì›' in table_unit:
                unit_multiplier = 1000
            elif 'ë°±ë§Œì›' in table_unit or 'ë°±ë§Œ ì›' in table_unit:
                unit_multiplier = 1000000
            elif 'ì–µì›' in table_unit or 'ì–µ ì›' in table_unit:
                unit_multiplier = 100000000
        
        if unit_multiplier == 1:
            return value
        
        # ìˆ«ì ì¶”ì¶œ ë° ë³€í™˜
        try:
            num_str = value.replace(',', '').replace(' ', '').strip()
            if not num_str.replace('.', '').replace('-', '').isdigit():
                return value
            
            num = int(float(num_str) * unit_multiplier)
            
            # ë‹¨ìœ„ ì ìš©ëœ ìˆ«ì ë°˜í™˜ (ì½¤ë§ˆ ì—†ëŠ” 0 ì¶”ê°€)
            return str(num)
        except:
            return value
    
    def _convert_inline_units(self, text: str) -> str:
        """ì…€ ë‚´ ë‹¨ìœ„(ì²œì›, ë°±ë§Œì›, ì–µì›)ë¥¼ raw ìˆ«ìë¡œ ë³€í™˜
        
        ì˜ˆ: "32,711,600ì²œì›" â†’ "32711600000"
        ì˜ˆ: "1,234ë°±ë§Œì›" â†’ "1234000000"
        ì˜ˆ: "500ì–µì›" â†’ "50000000000"
        """
        
        def convert_with_unit(match):
            num_str = match.group(1)  # ìˆ«ì ë¶€ë¶„
            unit = match.group(2)      # ë‹¨ìœ„ ë¶€ë¶„
            
            try:
                # ì½¤ë§ˆ ì œê±°í•˜ê³  ìˆ«ìë¡œ ë³€í™˜
                num = float(num_str.replace(',', ''))
                
                # ë‹¨ìœ„ë³„ ìŠ¹ìˆ˜
                multiplier = 1
                if 'ì²œì›' in unit or 'ì²œ ì›' in unit:
                    multiplier = 1000
                elif 'ë°±ë§Œì›' in unit or 'ë°±ë§Œ ì›' in unit:
                    multiplier = 1_000_000
                elif 'ì–µì›' in unit or 'ì–µ ì›' in unit:
                    multiplier = 100_000_000
                elif 'ì¡°ì›' in unit or 'ì¡° ì›' in unit:
                    multiplier = 1_000_000_000_000
                
                # ë³€í™˜ëœ ìˆ«ì (ì½¤ë§ˆ ì—†ëŠ” raw ìˆ«ì)
                result = int(num * multiplier)
                return str(result)
                
            except (ValueError, AttributeError):
                # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
                return match.group(0)
        
        # íŒ¨í„´: ìˆ«ì + ë‹¨ìœ„
        # ì˜ˆ: 32,711,600ì²œì›, 1234ë°±ë§Œì›, 500ì–µì›
        pattern = r'([\d,]+)(ì²œì›|ì²œ ì›|ë°±ë§Œì›|ë°±ë§Œ ì›|ì–µì›|ì–µ ì›|ì¡°ì›|ì¡° ì›)'
        text = re.sub(pattern, convert_with_unit, text)
        
        return text
    
    def _apply_unit_to_text(self, text: str, table_unit: str) -> str:
        """í…ìŠ¤íŠ¸ ë‚´ ëª¨ë“  ìˆ«ìì— ë‹¨ìœ„ ì ìš© (ì½¤ë§ˆ ì—†ëŠ” 0 ì¶”ê°€)
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸ (ì˜ˆ: "ë§¤ì¶œ: 838,319, ìì‚°: 1,200,000")
            table_unit: ë‹¨ìœ„ ì •ë³´
        
        Returns:
            ë‹¨ìœ„ê°€ ì ìš©ëœ í…ìŠ¤íŠ¸ (ì˜ˆ: "ë§¤ì¶œ: 838319000, ìì‚°: 1200000000")
        """
        
        # 1. ë¨¼ì € ì…€ ë‚´ ë‹¨ìœ„(ì²œì›, ë°±ë§Œì›, ì–µì›) ì²˜ë¦¬
        text = self._convert_inline_units(text)
        
        # 2. í…Œì´ë¸” ë‹¨ìœ„ê°€ ìˆìœ¼ë©´ ì ìš©
        if not table_unit:
            return text
        
        # ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ìˆ«ì íŒ¨í„´ ì°¾ê¸°
        def replace_number(match):
            return self._apply_unit_to_value(match.group(0), table_unit)
        
        # ìˆ«ì íŒ¨í„´ (ì½¤ë§ˆ í¬í•¨, ë‹¨ìœ„ ì—†ìŒ)
        text = re.sub(
            r'\b\d{1,3}(?:,\d{3})+\b(?!\s*[ì›ì£¼%ì–µë§Œì²œ])',
            replace_number,
            text
        )
        
        return text
    
    def _clean_header(self, header: str) -> str:
        """í—¤ë” ì •ë¦¬ (ë²ˆí˜¸, ê¸°í˜¸ ì œê±°)"""
        # "1. ê³„ì•½ê¸ˆì•¡(ì›)" â†’ "ê³„ì•½ê¸ˆì•¡"
        # "ì œ 78 ê¸°" â†’ "ì œ78ê¸°"
        
        cleaned = header.strip()
        
        # ì•ì˜ ë²ˆí˜¸ ì œê±° ("1.", "2." ë“±)
        cleaned = re.sub(r'^\d+\.\s*', '', cleaned)
        
        # ê´„í˜¸ ì•ˆ ë‚´ìš© ì œê±° (ë‹¨ìœ„ ë“±)
        cleaned = re.sub(r'\([^)]*\)', '', cleaned)
        
        # ê³µë°± ì •ë¦¬
        cleaned = re.sub(r'\s+', '', cleaned)
        
        return cleaned
    
    def _process_text(self, lines: List[str], start_idx: int) -> int:
        """í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
        
        text_lines = []
        i = start_idx
        
        while i < len(lines):
            line = lines[i]
            
            if line.startswith('|') or line.startswith('#'):
                break
            
            if not line.strip():
                if i + 1 < len(lines) and not lines[i + 1].strip():
                    break
            
            text_lines.append(line)
            i += 1
        
        text = '\n'.join(text_lines).strip()
        
        if len(text) > 10:
            section_path = ' > '.join(self.current_section_path)
            chunk = Chunk(
                chunk_id=self._generate_id(text, 'text', section_path),
                doc_id=f"{self.doc_metadata.get('rcept_dt', '')}_{self.doc_metadata.get('corp_code', '')}",
                chunk_type='text',
                section_path=section_path,
                structured_data={},
                natural_text=text,
                metadata={
                    'corp_name': self.doc_metadata.get('corp_name', ''),
                    'document_name': self.doc_metadata.get('document_name', ''),
                    'rcept_dt': self.doc_metadata.get('rcept_dt', ''),
                }
            )
            self.chunks.append(chunk)
        
        return i
    
    def _generate_id(self, content: str, chunk_type: str, section_path: str = "") -> str:
        """ID ìƒì„± - ì˜ë¯¸ìˆëŠ” IDë¡œ ê°œì„ """
        # ë¬¸ì„œ ID ê¸°ë°˜ + íƒ€ì… + ìˆœë²ˆìœ¼ë¡œ ìƒì„±
        doc_id = f"{self.doc_metadata.get('rcept_dt', '')}_{self.doc_metadata.get('corp_code', '')}"
        
        # ì„¹ì…˜ ê²½ë¡œì—ì„œ ë§ˆì§€ë§‰ ë¶€ë¶„ ì¶”ì¶œ
        section_name = section_path.split(' > ')[-1] if section_path else "unknown"
        section_clean = re.sub(r'[^\wê°€-í£]', '', section_name)[:10]  # íŠ¹ìˆ˜ë¬¸ì ì œê±°, 10ì ì œí•œ
        
        # íƒ€ì…ë³„ ìˆœë²ˆ ê³„ì‚°
        type_count = sum(1 for chunk in self.chunks if chunk.chunk_type == chunk_type)
        
        # ì˜ë¯¸ìˆëŠ” ID ìƒì„±: doc_id + type + section + ìˆœë²ˆ
        return f"{doc_id}_{chunk_type}_{section_clean}_{type_count:03d}"


def main(process_all=False):
    """
    Step 1: ë§ˆí¬ë‹¤ìš´ â†’ 1ì°¨ ì²­í¬ ë³€í™˜
    
    ì…ë ¥: data/markdown/*.md
    ì¶œë ¥: data/transform/parser/*_chunks.jsonl
    """
    
    # ì²˜ë¦¬ ëª¨ë“œ ì„¤ì •
    if process_all:
        max_files = None  # ì „ì²´ ì²˜ë¦¬
        print("ğŸ”§ ì „ì²´ íŒŒì¼ ì²˜ë¦¬ ëª¨ë“œ")
    else:
        max_files = 20  # í…ŒìŠ¤íŠ¸ìš© 20ê°œë§Œ
        print("ğŸ”§ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (20ê°œ íŒŒì¼ë§Œ ì²˜ë¦¬)")

    # ê²½ë¡œ ì„¤ì • (transform í´ë” ê¸°ì¤€)
    paths = get_transform_paths(__file__)
    markdown_dir = paths['markdown_dir']
    output_dir = paths['parser_dir']

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir.mkdir(parents=True, exist_ok=True)

    # ëª¨ë“  ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì°¾ê¸°
    all_markdown_files = list(markdown_dir.glob("*.md"))
    
    if not all_markdown_files:
        print("âŒ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # íŒŒì¼ ìˆ˜ ì œí•œ ì ìš©
    if max_files:
        markdown_files = all_markdown_files[:max_files]
        print(f"ğŸ“„ ì „ì²´ {len(all_markdown_files)}ê°œ ì¤‘ {len(markdown_files)}ê°œ íŒŒì¼ ì²˜ë¦¬")
    else:
        markdown_files = all_markdown_files
        print(f"ğŸ“„ ì „ì²´ {len(markdown_files)}ê°œ íŒŒì¼ ì²˜ë¦¬")

    print("=" * 80)
    print("Transform Pipeline - Step 1: êµ¬ì¡°í™” ë° 1ì°¨ ì²­í‚¹")
    print("=" * 80)
    print(f"ğŸ“ ì…ë ¥: {markdown_dir}")
    print(f"ğŸ“ ì¶œë ¥: {output_dir}")
    print(f"ğŸ“„ ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {len(markdown_files)}ê°œ")
    if max_files:
        print(f"ğŸ’¡ ì „ì²´ ì²˜ë¦¬í•˜ë ¤ë©´: python {Path(__file__).name} --all")
    print(f"\në‹¤ìŒ ë‹¨ê³„: normalizer.pyë¡œ ì •ê·œí™” ìˆ˜í–‰")
    print("=" * 80)
    print()

    # ì „ì²´ í†µê³„
    total_chunks = 0
    processed_files = 0
    failed_files = 0
    all_chunks = []  # ëª¨ë“  ì²­í¬ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

    # ê° ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì²˜ë¦¬
    for i, md_file in enumerate(markdown_files, 1):
        print(f"[{i}/{len(markdown_files)}] ì²˜ë¦¬ ì¤‘: {md_file.name}")
        
        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                markdown_content = f.read()

            # YAML í—¤ë”ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            doc_metadata = {}
            if markdown_content.startswith('---'):
                yaml_end = markdown_content.find('---', 3)
                if yaml_end != -1:
                    yaml_header = markdown_content[3:yaml_end]
                    for line in yaml_header.split('\n'):
                        if ':' in line:
                            key, value = line.split(':', 1)
                            doc_metadata[key.strip()] = value.strip()

            # ê¸°ë³¸ê°’ ì„¤ì •
            if not doc_metadata.get('corp_code'):
                doc_metadata = {
                    'corp_name': 'Unknown',
                    'document_name': 'Unknown'
                }

            print(f"  ğŸ“Š ë¬¸ì„œ ì •ë³´: {doc_metadata.get('corp_name')} ({doc_metadata.get('stock_code')})")

            # ì²­í¬ ìƒì„±
            chunker = MarkdownChunker(markdown_content, doc_metadata)
            chunks = chunker.process()

            print(f"  âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

            # JSONL ì €ì¥ (íŒŒì¼ëª…: ë§ˆí¬ë‹¤ìš´ íŒŒì¼ëª… ê·¸ëŒ€ë¡œ ì‚¬ìš©)
            output_filename = f"{md_file.stem}_chunks.jsonl"
            output_file = output_dir / output_filename

            # ê³µí†µ write_jsonl ì‚¬ìš©
            write_jsonl(output_file, [chunk.to_dict() for chunk in chunks])

            print(f"  ğŸ’¾ ì €ì¥: {output_file.name}")

            # í†µê³„ ì—…ë°ì´íŠ¸
            total_chunks += len(chunks)
            processed_files += 1
            all_chunks.extend(chunks)  # ëª¨ë“  ì²­í¬ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

            # ì²­í¬ íƒ€ì…ë³„ í†µê³„
            chunk_types = {}
            for chunk in chunks:
                chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1

            print(f"  ğŸ“ˆ ì²­í¬ íƒ€ì…: {', '.join([f'{k}({v})' for k, v in chunk_types.items()])}")
            print()

        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜: {e}")
            failed_files += 1
            print()

    # ì „ì²´ ê²°ê³¼ ì¶œë ¥
    print("=" * 60)
    print("     ì²˜ë¦¬ ì™„ë£Œ")
    print("=" * 60)

    # í…Œì´ë¸” ì²­í¬ ìƒ˜í”Œ
    table_chunks = [c for c in all_chunks if c.chunk_type == 'table_row'][:3]

    for i, chunk in enumerate(table_chunks):
        print(f"\n[{i+1}] {chunk.section_path}")
        print(f"íƒ€ì…: {chunk.chunk_type}")
        print(f"ìì—°ì–´: {chunk.natural_text}")
        print(f"êµ¬ì¡°í™”: {json.dumps(chunk.structured_data, ensure_ascii=False, indent=2)}")

    # í…ìŠ¤íŠ¸ ì²­í¬ ìƒ˜í”Œ
    text_chunks = [c for c in all_chunks if c.chunk_type == 'text'][:2]

    if text_chunks:
        print("\n" + "=" * 60)
        print("í…ìŠ¤íŠ¸ ì²­í¬ ìƒ˜í”Œ:")
        print("=" * 60)

        for i, chunk in enumerate(text_chunks):
            print(f"\n[{i+1}] {chunk.section_path}")
            print(f"ë‚´ìš©: {chunk.natural_text[:200]}...")

    # í†µê³„
    print("\n" + "=" * 60)
    print("ì²­í¬ í†µê³„:")
    print("=" * 60)

    chunk_types = {}
    for chunk in all_chunks:
        chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1

    for chunk_type, count in chunk_types.items():
        print(f"  {chunk_type}: {count}ê°œ")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] in ["--help", "-h"]:
        print("ì‚¬ìš©ë²•:")
        print(f"  python {Path(__file__).name}        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ (10ê°œ íŒŒì¼ë§Œ)")
        print(f"  python {Path(__file__).name} --all  # ì „ì²´ íŒŒì¼ ì²˜ë¦¬")
        print(f"  python {Path(__file__).name} --help # ë„ì›€ë§")
        sys.exit(0)
    
    # --all ì˜µì…˜ í™•ì¸
    process_all = len(sys.argv) > 1 and sys.argv[1] == "--all"
    main(process_all=process_all)