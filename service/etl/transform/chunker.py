#!/usr/bin/env python3
"""
====================================================================================
Transform Pipeline - Step 3: ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ë° ë©”íƒ€ë°ì´í„° ê°•í™”
====================================================================================

[íŒŒì´í”„ë¼ì¸ ìˆœì„œ]
1. parser.py      â†’ ë§ˆí¬ë‹¤ìš´ì„ êµ¬ì¡°í™”ëœ ì²­í¬ë¡œ ë³€í™˜
2. normalizer.py  â†’ ë°ì´í„° ì •ê·œí™” ë° ìì—°ì–´ í’ˆì§ˆ ê°œì„ 
3. chunker.py     â†’ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ë° ë©”íƒ€ë°ì´í„° ê°•í™” (í˜„ì¬ íŒŒì¼)

[ì´ íŒŒì¼ì˜ ì—­í• ]
- ì‘ì€ ì²­í¬ë“¤ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë³‘í•©
- í† í° ìˆ˜ ì œí•œ ë‚´ì—ì„œ ìµœì  í¬ê¸° ì¡°ì •
- ì•ë’¤ ë¬¸ë§¥ ìœˆë„ìš° ì¶”ê°€
- ë©”íƒ€ë°ì´í„° ê°•í™” (doc_type, data_category, keywords ë“±)
- ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ í† í° ìˆ˜ ê³„ì‚°

[ì…ë ¥]
- data/transform/normalized/*_chunks.jsonl (normalizer.py ì¶œë ¥)

[ì¶œë ¥]
- data/transform/final/*_chunks.jsonl (ìµœì¢… ì²­í¬, ë²¡í„° DB ì €ì¥ ì¤€ë¹„ ì™„ë£Œ)

[ë‹¤ìŒ ë‹¨ê³„]
- ë²¡í„° DBì— ì„ë² ë”© ë° ì €ì¥
====================================================================================
"""

import re
import math
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
from pathlib import Path
import tiktoken

# ê³µí†µ ëª¨ë“ˆ
from utils import read_jsonl, write_jsonl, get_file_list, ensure_output_dir, get_transform_paths

# LangChain text splitter (optional)
try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    LANGCHAIN_AVAILABLE = True
except ImportError:
    try:
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        LANGCHAIN_AVAILABLE = True
    except ImportError:
        LANGCHAIN_AVAILABLE = False
        RecursiveCharacterTextSplitter = None
        print("âš ï¸  LangChain not available. Text chunks will not be split further.")


@dataclass
class ChunkConfig:
    """ì²­í‚¹ ì„¤ì •"""
    # ìµœëŒ€ í† í° ìˆ˜ (OpenAI embedding ê¸°ì¤€)
    max_tokens: int = 7000  # ì•ˆì „í•œ ì œí•œ (8192 - ì—¬ìœ ë¶„)
    
    # ì²­í¬ ì˜¤ë²„ë© (ë¬¸ë§¥ ë³´ì¡´)
    overlap_tokens: int = 200
    
    # ìµœì†Œ ì²­í¬ í¬ê¸°
    min_tokens: int = 50
    
    # ì„ë² ë”© ëª¨ë¸
    embedding_model: str = "text-embedding-3-small"


class SmartChunker:
    """ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì²˜ë¦¬"""
    
    def __init__(self, config: ChunkConfig = None):
        self.config = config or ChunkConfig()
        
        # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        try:
            self.tokenizer = tiktoken.encoding_for_model(self.config.embedding_model)
        except:
            # fallback
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
    
    def split_text_chunk(self, chunk: Dict[str, Any]) -> List[Dict[str, Any]]:
        """text íƒ€ì… ì²­í¬ë¥¼ LangChain splitterë¡œ ë¶„í• 
        
        normalizer.pyì—ì„œ ì´ë™: í…ìŠ¤íŠ¸ ë¶„í• ì€ chunker.pyì˜ ì±…ì„
        
        ì ì‘í˜• chunk_size ì‚¬ìš©: max(300, min(1000, ceil(total_length // 30)))
        """
        if not LANGCHAIN_AVAILABLE:
            return [chunk]
        
        text = chunk.get('natural_text', '')
        if not text or len(text) < 200:
            # ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ë¶„í• í•˜ì§€ ì•ŠìŒ
            return [chunk]

        # ì ì‘í˜• chunk_size ê³„ì‚°
        total_length = len(text)
        chunk_size = max(300, min(1000, math.ceil(total_length / 30)))
        chunk_overlap = min(50, chunk_size // 5)

        # LangChain splitter ìƒì„±
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            is_separator_regex=False,
        )

        # í…ìŠ¤íŠ¸ ë¶„í• 
        split_texts = text_splitter.split_text(text)

        # ë¶„í• ëœ í…ìŠ¤íŠ¸ë¡œ chunk ìƒì„±
        result_chunks = []
        base_chunk_id = chunk.get('chunk_id', '')

        for idx, split_text in enumerate(split_texts):
            new_chunk = chunk.copy()
            new_chunk['natural_text'] = split_text
            # chunk_idì— ë¶„í•  ì¸ë±ìŠ¤ ì¶”ê°€
            if '_split_' in base_chunk_id:
                # ì´ë¯¸ ë¶„í• ëœ ê²½ìš° ìƒˆë¡œìš´ ì¸ë±ìŠ¤ë¡œ êµì²´
                new_chunk['chunk_id'] = re.sub(r'_split_\d+$', f'_split_{idx}', base_chunk_id)
            else:
                new_chunk['chunk_id'] = f"{base_chunk_id}_split_{idx}"

            # metadataì— ë¶„í•  ì •ë³´ ì¶”ê°€
            if 'metadata' not in new_chunk:
                new_chunk['metadata'] = {}
            new_chunk['metadata']['split_index'] = idx
            new_chunk['metadata']['total_splits'] = len(split_texts)
            new_chunk['metadata']['chunk_size'] = chunk_size

            result_chunks.append(new_chunk)

        return result_chunks
    
    def split_table_row_chunk(self, chunk: Dict[str, Any]) -> List[Dict[str, Any]]:
        """table_row íƒ€ì… ì²­í¬ë¥¼ ì ì‘í˜• í¬ê¸°ë¡œ ë¶„í• 
        
        ì ì‘í˜• chunk_size ì‚¬ìš©: max(300, min(1000, ceil(total_length // 30)))
        """
        text = chunk.get('natural_text', '')
        if not text or len(text) < 300:
            # ì§§ì€ í…Œì´ë¸”ì€ ë¶„í• í•˜ì§€ ì•ŠìŒ
            return [chunk]

        # ì ì‘í˜• chunk_size ê³„ì‚°
        total_length = len(text)
        chunk_size = max(300, min(1000, math.ceil(total_length / 30)))
        
        # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ í¬ê¸°ë¡œ ë¶„í• 
        split_texts = []
        start = 0
        
        while start < total_length:
            end = start + chunk_size
            
            # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° (ì½¤ë§ˆ, ì„¸ë¯¸ì½œë¡ , ë§ˆì¹¨í‘œ)
            if end < total_length:
                # ë’¤ì—ì„œë¶€í„° ë¬¸ì¥ êµ¬ë¶„ì ì°¾ê¸°
                for i in range(min(100, chunk_size // 2), 0, -1):
                    if start + i < total_length and text[start + i] in [',', ';', '.', ' ']:
                        end = start + i + 1
                        break
            
            split_text = text[start:end].strip()
            if split_text:
                split_texts.append(split_text)
            
            start = end

        # ë¶„í• ëœ í…ìŠ¤íŠ¸ë¡œ chunk ìƒì„±
        result_chunks = []
        base_chunk_id = chunk.get('chunk_id', '')

        for idx, split_text in enumerate(split_texts):
            new_chunk = chunk.copy()
            new_chunk['natural_text'] = split_text
            
            # chunk_idì— ë¶„í•  ì¸ë±ìŠ¤ ì¶”ê°€
            if '_split_' in base_chunk_id:
                new_chunk['chunk_id'] = re.sub(r'_split_\d+$', f'_split_{idx}', base_chunk_id)
            else:
                new_chunk['chunk_id'] = f"{base_chunk_id}_split_{idx}"

            # metadataì— ë¶„í•  ì •ë³´ ì¶”ê°€
            if 'metadata' not in new_chunk:
                new_chunk['metadata'] = {}
            new_chunk['metadata']['split_index'] = idx
            new_chunk['metadata']['total_splits'] = len(split_texts)
            new_chunk['metadata']['chunk_size'] = chunk_size

            result_chunks.append(new_chunk)

        return result_chunks
    
    def should_merge_chunks(self, chunks: List[Dict]) -> List[Dict]:
        """
        ì‘ì€ ì²­í¬ë“¤ì„ ë³‘í•©
        - í…Œì´ë¸” í–‰ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ê´€ë ¨ í–‰ë“¤ê³¼ ë³‘í•©
        - í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ì•ë’¤ì™€ ë³‘í•©
        """
        
        merged = []
        buffer = []
        buffer_tokens = 0
        
        for chunk in chunks:
            chunk_tokens = self._count_tokens(chunk['natural_text'])
            
            # ë²„í¼ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì¶”ê°€
            if not buffer:
                buffer.append(chunk)
                buffer_tokens = chunk_tokens
                continue
            
            # ê°™ì€ ì„¹ì…˜ì´ê³  í† í° í•©ì´ max ì´í•˜ë©´ ë³‘í•©
            if (self._same_section(buffer[-1], chunk) and 
                buffer_tokens + chunk_tokens <= self.config.max_tokens):
                buffer.append(chunk)
                buffer_tokens += chunk_tokens
            else:
                # ë²„í¼ë¥¼ ë³‘í•©í•´ì„œ ì €ì¥
                merged.append(self._merge_buffer(buffer))
                buffer = [chunk]
                buffer_tokens = chunk_tokens
        
        # ë‚¨ì€ ë²„í¼ ì²˜ë¦¬
        if buffer:
            merged.append(self._merge_buffer(buffer))
        
        return merged
    
    def _same_section(self, chunk1: Dict, chunk2: Dict) -> bool:
        """ê°™ì€ ì„¹ì…˜ì¸ì§€ í™•ì¸"""
        return (chunk1['section_path'] == chunk2['section_path'] and
                chunk1['chunk_type'] == chunk2['chunk_type'])
    
    def _merge_buffer(self, buffer: List[Dict]) -> Dict:
        """ë²„í¼ì˜ ì²­í¬ë“¤ì„ í•˜ë‚˜ë¡œ ë³‘í•©"""
        
        if len(buffer) == 1:
            return buffer[0]
        
        # ìì—°ì–´ í…ìŠ¤íŠ¸ ë³‘í•©
        natural_texts = [c['natural_text'] for c in buffer]
        merged_text = ' '.join(natural_texts)
        
        # êµ¬ì¡°í™” ë°ì´í„° ë³‘í•© (í…Œì´ë¸” í–‰ì¸ ê²½ìš°)
        merged_structured = {}
        if buffer[0]['chunk_type'] == 'table_row':
            for chunk in buffer:
                merged_structured.update(chunk.get('structured_data', {}))
        
        # ì²« ë²ˆì§¸ ì²­í¬ì˜ ë©”íƒ€ë°ì´í„° ì‚¬ìš© (chunk_idëŠ” ìƒˆë¡œ ìƒì„±)
        merged_metadata = buffer[0]['metadata'].copy()
        merged_metadata['merged_count'] = len(buffer)
        
        return {
            'chunk_id': f"{buffer[0]['chunk_id']}_merged",
            'doc_id': buffer[0]['doc_id'],
            'chunk_type': buffer[0]['chunk_type'],
            'section_path': buffer[0]['section_path'],
            'structured_data': merged_structured,
            'natural_text': merged_text,
            'metadata': merged_metadata
        }
    
    def add_context_window(self, chunks: List[Dict]) -> List[Dict]:
        """
        ê° ì²­í¬ì— ì•ë’¤ ë¬¸ë§¥ ì¶”ê°€
        - ì´ì „/ë‹¤ìŒ ì²­í¬ì˜ ì¼ë¶€ë¥¼ ë©”íƒ€ë°ì´í„°ì— í¬í•¨
        """
        
        for i, chunk in enumerate(chunks):
            # ì´ì „ ì²­í¬ ë¬¸ë§¥
            if i > 0:
                prev_text = chunks[i-1]['natural_text']
                chunk['metadata']['prev_context'] = self._truncate_text(
                    prev_text, 
                    max_tokens=100
                )
            
            # ë‹¤ìŒ ì²­í¬ ë¬¸ë§¥
            if i < len(chunks) - 1:
                next_text = chunks[i+1]['natural_text']
                chunk['metadata']['next_context'] = self._truncate_text(
                    next_text,
                    max_tokens=100
                )
        
        return chunks
    
    def enhance_metadata(self, chunk: Dict) -> Dict:
        """ë©”íƒ€ë°ì´í„° ê°•í™”"""
        
        metadata = chunk['metadata']
        
        # 1. ë¬¸ì„œ íƒ€ì… ì¶”ë¡ 
        metadata['doc_type'] = self._infer_doc_type(metadata.get('document_name', ''))
        
        # 2. ë°ì´í„° íƒ€ì… ë¶„ë¥˜
        metadata['data_category'] = self._classify_data_category(
            chunk['natural_text'],
            chunk.get('section_path', '')
        )
        
        # 3. íšŒê³„ ì—°ë„ ì¶”ì¶œ
        metadata['fiscal_year'] = self._extract_fiscal_year(
            chunk['natural_text'],
            chunk.get('structured_data', {})
        )
        
        # 4. í‚¤ì›Œë“œ ì¶”ì¶œ (ê²€ìƒ‰ ê°•í™”)
        metadata['keywords'] = self._extract_keywords(
            chunk['natural_text'],
            chunk.get('structured_data', {})
        )
        
        # 5. í† í° ìˆ˜ ê³„ì‚°
        metadata['token_count'] = self._count_tokens(chunk['natural_text'])
        
        return chunk
    
    def _infer_doc_type(self, doc_name: str) -> str:
        """ë¬¸ì„œ íƒ€ì… ì¶”ë¡ """
        if 'ê°ì‚¬ë³´ê³ ì„œ' in doc_name:
            return 'audit_report'
        elif 'ì‚¬ì—…ë³´ê³ ì„œ' in doc_name:
            return 'business_report'
        elif 'ë¶„ê¸°ë³´ê³ ì„œ' in doc_name:
            return 'quarterly_report'
        else:
            return 'other'
    
    def _classify_data_category(self, text: str, section: str) -> str:
        """ë°ì´í„° ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        
        # ì¬ë¬´ì œí‘œ
        if any(keyword in section for keyword in ['ì¬ë¬´ìƒíƒœí‘œ', 'ì†ìµê³„ì‚°ì„œ', 'í˜„ê¸ˆíë¦„']):
            return 'financial_statement'
        
        # ì£¼ì‹ ì •ë³´
        elif 'ì£¼ì‹' in section:
            return 'stock_info'
        
        # ê°ì‚¬ ì˜ê²¬
        elif 'ê°ì‚¬' in section and 'ì˜ê²¬' in text:
            return 'audit_opinion'
        
        # ì£¼ì„
        elif 'ì£¼ì„' in section:
            return 'footnote'
        
        # ê¸°íƒ€
        else:
            return 'general'
    
    def _extract_fiscal_year(self, text: str, structured_data: Dict) -> Optional[int]:
        """íšŒê³„ ì—°ë„ ì¶”ì¶œ"""
        
        # êµ¬ì¡°í™” ë°ì´í„°ì—ì„œ ì¶”ì¶œ
        for key, value in structured_data.items():
            if 'ê¸°' in key or 'ë…„ë„' in key:
                match = re.search(r'20\d{2}', str(value))
                if match:
                    return int(match.group())
        
        # í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
        years = re.findall(r'20\d{2}', text)
        if years:
            return int(years[0])
        
        return None
    
    def _extract_keywords(self, text: str, structured_data: Dict) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)"""
        
        keywords = set()
        
        # êµ¬ì¡°í™” ë°ì´í„°ì—ì„œ í‚¤ ì¶”ì¶œ
        keywords.update(structured_data.keys())
        
        # í…ìŠ¤íŠ¸ì—ì„œ ì¬ë¬´ ìš©ì–´ ì¶”ì¶œ
        financial_terms = [
            'ìì‚°', 'ë¶€ì±„', 'ìë³¸', 'ë§¤ì¶œ', 'ì˜ì—…ì´ìµ', 'ë‹¹ê¸°ìˆœì´ìµ',
            'í˜„ê¸ˆ', 'ìœ ë™', 'ë¹„ìœ ë™', 'ê°ê°€ìƒê°', 'ì´ì', 'ë°°ë‹¹',
            'ì£¼ì‹', 'ë³´í†µì£¼', 'ìš°ì„ ì£¼'
        ]
        
        for term in financial_terms:
            if term in text:
                keywords.add(term)
        
        return list(keywords)[:10]  # ìµœëŒ€ 10ê°œ
    
    def _count_tokens(self, text: str) -> int:
        """í† í° ìˆ˜ ê³„ì‚°"""
        try:
            return len(self.tokenizer.encode(text))
        except:
            # fallback: ëŒ€ëµ 4ìë‹¹ 1í† í°
            return len(text) // 4
    
    def _truncate_text(self, text: str, max_tokens: int) -> str:
        """í…ìŠ¤íŠ¸ë¥¼ ìµœëŒ€ í† í° ìˆ˜ë¡œ ìë¥´ê¸°"""
        tokens = self.tokenizer.encode(text)
        if len(tokens) <= max_tokens:
            return text
        
        truncated_tokens = tokens[:max_tokens]
        return self.tokenizer.decode(truncated_tokens) + "..."


def process_chunks_with_enhancement(chunks: List[Dict]) -> List[Dict]:
    """ì²­í¬ ê°•í™” ì²˜ë¦¬"""
    
    chunker = SmartChunker()
    
    # 0. ì²­í¬ ë¶„í•  (text: LangChain splitter, table_row: ì ì‘í˜• ë¶„í• )
    split_chunks = []
    for chunk in chunks:
        if chunk.get('chunk_type') == 'text':
            split_chunks.extend(chunker.split_text_chunk(chunk))
        elif chunk.get('chunk_type') == 'table_row':
            split_chunks.extend(chunker.split_table_row_chunk(chunk))
        else:
            split_chunks.append(chunk)
    
    print(f"âœ… ì²­í¬ ë¶„í•  (text/table_row): {len(chunks)} â†’ {len(split_chunks)}")
    
    # 1. ì‘ì€ ì²­í¬ ë³‘í•©
    merged = chunker.should_merge_chunks(split_chunks)
    print(f"âœ… ì²­í¬ ë³‘í•©: {len(split_chunks)} â†’ {len(merged)}")
    
    # 2. ë¬¸ë§¥ ìœˆë„ìš° ì¶”ê°€
    with_context = chunker.add_context_window(merged)
    
    # 3. ë©”íƒ€ë°ì´í„° ê°•í™”
    enhanced = [chunker.enhance_metadata(chunk) for chunk in with_context]
    
    # 4. í° ì²­í¬ ì¶”ê°€ ë¶„í•  (7000 í† í° ì´ìƒ)
    final_chunks = []
    oversized_count = 0
    split_count = 0
    
    for chunk in enhanced:
        token_count = chunk['metadata'].get('token_count', 0)
        if token_count > 7000:
            oversized_count += 1
            text = chunk['natural_text']
            
            # ëª©í‘œ: 3500 í† í°ì”© ë¶„í•  (ì•ˆì „ ë§ˆì§„ í¬í•¨)
            # í•œê¸€ ê¸°ì¤€: 1í† í° â‰ˆ 1.1ì â†’ 3500í† í° â‰ˆ 3850ì
            target_chars = 3850
            num_parts = math.ceil(len(text) / target_chars)
            part_size = len(text) // num_parts
            
            if num_parts > 1:
                split_count += 1
                for idx in range(num_parts):
                    start = idx * part_size
                    end = start + part_size if idx < num_parts - 1 else len(text)
                    
                    # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° (ì½¤ë§ˆ, ê³µë°±, ì¤„ë°”ê¿ˆ)
                    if end < len(text):
                        for i in range(min(300, part_size // 3), 0, -1):
                            pos = start + i
                            if pos < len(text) and text[pos] in [',', ' ', '\n', '.', ':']:
                                end = pos + 1
                                break
                    
                    part_text = text[start:end].strip()
                    if not part_text:
                        continue
                    
                    new_chunk = chunk.copy()
                    new_chunk['natural_text'] = part_text
                    new_chunk['chunk_id'] = f"{chunk['chunk_id']}_oversized_{idx}"
                    new_chunk['metadata'] = chunk['metadata'].copy()
                    new_chunk['metadata']['oversized_split'] = True
                    new_chunk['metadata']['oversized_index'] = idx
                    new_chunk['metadata']['oversized_total'] = num_parts
                    new_chunk['metadata']['token_count'] = chunker._count_tokens(part_text)
                    
                    final_chunks.append(new_chunk)
            else:
                # ë¶„í• ì´ í•„ìš”í•˜ì§€ ì•Šì€ ê²½ìš° (í…ìŠ¤íŠ¸ê°€ ì§§ìŒ)
                final_chunks.append(chunk)
        else:
            final_chunks.append(chunk)
    
    if oversized_count > 0:
        print(f"âœ… í° ì²­í¬ ë°œê²¬: {oversized_count}ê°œ (7000+ í† í°)")
        if split_count > 0:
            print(f"âœ… í° ì²­í¬ ë¶„í• : {split_count}ê°œ ì²­í¬ â†’ í‰ê·  3500 í† í°ìœ¼ë¡œ ë¶„í• ")
    
    return final_chunks


def process_jsonl_file(input_file: Path, output_file: Path):
    """
    Step 3: JSONL íŒŒì¼ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹
    
    ì…ë ¥: step2_normalizedì˜ JSONL íŒŒì¼
    ì¶œë ¥: finalì˜ JSONL íŒŒì¼
    """
    
    # ëª¨ë“  ì²­í¬ ì½ê¸°
    chunks = list(read_jsonl(input_file))
    
    # ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì ìš©
    enhanced_chunks = process_chunks_with_enhancement(chunks)
    
    # ì €ì¥
    write_jsonl(output_file, enhanced_chunks)
    
    print(f"âœ… {len(enhanced_chunks)}ê°œ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ")
    
    # í†µê³„ ì¶œë ¥
    token_counts = [c['metadata']['token_count'] for c in enhanced_chunks]
    print(f"  ğŸ“Š í‰ê·  í† í° ìˆ˜: {sum(token_counts) / len(token_counts):.0f}")
    print(f"  ğŸ“Š ìµœì†Œ í† í° ìˆ˜: {min(token_counts)}")
    print(f"  ğŸ“Š ìµœëŒ€ í† í° ìˆ˜: {max(token_counts)}")


def process_directory(input_dir: Path, output_dir: Path):
    """
    Step 3: ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  JSONL íŒŒì¼ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹
    
    ì…ë ¥: data/transform/normalized/
    ì¶œë ¥: data/transform/final/
    """
    
    ensure_output_dir(output_dir)
    
    jsonl_files = get_file_list(input_dir)
    
    if not jsonl_files:
        print("âŒ JSONL íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("=" * 80)
    print("Transform Pipeline - Step 3: ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ë° ë©”íƒ€ë°ì´í„° ê°•í™”")
    print("=" * 80)
    print(f"ğŸ“ ì…ë ¥: {input_dir}")
    print(f"ğŸ“ ì¶œë ¥: {output_dir}")
    print(f"ğŸ“„ ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {len(jsonl_files)}ê°œ")
    print(f"\nì²˜ë¦¬ ë‚´ìš©: í…ìŠ¤íŠ¸ ë¶„í• , ì²­í¬ ë³‘í•©, ë¬¸ë§¥ ìœˆë„ìš° ì¶”ê°€, ë©”íƒ€ë°ì´í„° ê°•í™”")
    print(f"ë‹¤ìŒ ë‹¨ê³„: ë²¡í„° DBì— ì„ë² ë”© ë° ì €ì¥")
    print("=" * 80)
    print()
    
    total_input = 0
    total_output = 0
    
    for i, input_file in enumerate(jsonl_files, 1):
        print(f"[{i}/{len(jsonl_files)}] ì²˜ë¦¬ ì¤‘: {input_file.name}")
        
        # ì…ë ¥ ì²­í¬ ìˆ˜ ì¹´ìš´íŠ¸
        input_count = sum(1 for _ in read_jsonl(input_file))
        total_input += input_count
        
        output_file = output_dir / input_file.name
        process_jsonl_file(input_file, output_file)
        
        # ì¶œë ¥ ì²­í¬ ìˆ˜ ì¹´ìš´íŠ¸
        output_count = sum(1 for _ in read_jsonl(output_file))
        total_output += output_count
        
        print(f"  ğŸ’¾ ì €ì¥: {output_file.name}")
        print()
    
    print("=" * 80)
    print("Step 3 ì™„ë£Œ!")
    print("=" * 80)
    print(f"ì´ ì²˜ë¦¬: {total_input}ê°œ â†’ {total_output}ê°œ ì²­í¬")
    if total_input != total_output:
        change_ratio = ((total_output - total_input) / total_input) * 100
        if change_ratio > 0:
            print(f"ë¶„í• /ë³‘í•© íš¨ê³¼: {change_ratio:+.1f}% ë³€í™”")
        else:
            print(f"ë³‘í•© íš¨ê³¼: {abs(change_ratio):.1f}% ê°ì†Œ")


def main():
    """Chunker ë©”ì¸ í•¨ìˆ˜"""
    import sys
    
    # ë””ë ‰í† ë¦¬ ëª¨ë“œ (ê¶Œì¥)
    if len(sys.argv) == 1:
        # ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
        paths = get_transform_paths(__file__)
        input_dir = paths['normalized_dir']
        output_dir = paths['final_dir']
        
        process_directory(input_dir, output_dir)
    
    # ë‹¨ì¼ íŒŒì¼ ëª¨ë“œ
    elif len(sys.argv) == 3:
        input_file = Path(sys.argv[1])
        output_file = Path(sys.argv[2])
        process_jsonl_file(input_file, output_file)
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    elif len(sys.argv) == 2 and sys.argv[1] == "--test":
        print("=" * 80)
        print("í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
        print("=" * 80)
        
        sample_chunks = [
            {
                'chunk_id': 'test_001',
                'doc_id': 'doc_001',
                'chunk_type': 'table_row',
                'section_path': 'ì¬ë¬´ì œí‘œ > ì¬ë¬´ìƒíƒœí‘œ',
                'natural_text': 'ìœ ë™ìì‚°ì€ 14,220ì–µì›',
                'structured_data': {'ê³¼ëª©': 'ìœ ë™ìì‚°', 'ê¸ˆì•¡': '1422091558149'},
                'metadata': {
                    'document_name': 'ê°ì‚¬ë³´ê³ ì„œ'
                }
            }
        ]
        
        enhanced = process_chunks_with_enhancement(sample_chunks)
        
        import json
        print(json.dumps(enhanced[0], ensure_ascii=False, indent=2))
    
    else:
        print("ì‚¬ìš©ë²•:")
        print("  1. ë””ë ‰í† ë¦¬ ëª¨ë“œ (ê¶Œì¥): python chunker.py")
        print("  2. ë‹¨ì¼ íŒŒì¼ ëª¨ë“œ:      python chunker.py <input.jsonl> <output.jsonl>")
        print("  3. í…ŒìŠ¤íŠ¸ ëª¨ë“œ:         python chunker.py --test")
        sys.exit(1)


if __name__ == "__main__":
    main()