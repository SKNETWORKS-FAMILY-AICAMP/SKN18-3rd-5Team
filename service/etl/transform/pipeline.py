#!/usr/bin/env python3
"""
RAG ë°ì´í„° ë³€í™˜ íŒŒì´í”„ë¼ì¸

ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰:
1. Parser: ë§ˆí¬ë‹¤ìš´ â†’ êµ¬ì¡°í™”ëœ ì²­í¬
2. Normalizer: í…ìŠ¤íŠ¸ ì •ê·œí™” ë° ë‹¨ìœ„ ë³€í™˜
3. Chunker: ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€

ì‚¬ìš©ë²•:
    python pipeline.py           # í…ŒìŠ¤íŠ¸ ëª¨ë“œ (20ê°œ íŒŒì¼)
    python pipeline.py --all     # ì „ì²´ íŒŒì¼ ì²˜ë¦¬
    python pipeline.py --help    # ë„ì›€ë§
"""

import sys
import time
import shutil
from pathlib import Path
from typing import Optional

# ê³µí†µ ìœ í‹¸ë¦¬í‹° import
from utils import get_project_paths, get_transform_paths

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ë””ë ‰í† ë¦¬
SCRIPT_DIR = Path(__file__).parent  # service/etl/transform

def cleanup_parser_files():
    """Parser íŒŒì¼ ì •ë¦¬ (Normalizer ì™„ë£Œ í›„)"""
    print("\n" + "=" * 80)
    print("ğŸ§¹ Parser íŒŒì¼ ì •ë¦¬")
    print("=" * 80)
    
    paths = get_transform_paths(__file__)
    parser_dir = paths['parser_dir']
    
    if parser_dir.exists():
        # íŒŒì¼ ê°œìˆ˜ í™•ì¸
        parser_files = list(parser_dir.glob("*.jsonl"))
        file_count = len(parser_files)
        
        if file_count > 0:
            # ë””ë ‰í† ë¦¬ ì „ì²´ ì‚­ì œ
            shutil.rmtree(parser_dir)
            print(f"âœ… Parser íŒŒì¼ {file_count}ê°œ ì‚­ì œ ì™„ë£Œ")
            print(f"   ì‚­ì œëœ ë””ë ‰í† ë¦¬: {parser_dir}")
        else:
            print("â„¹ï¸  ì‚­ì œí•  Parser íŒŒì¼ ì—†ìŒ")
    else:
        print("â„¹ï¸  Parser ë””ë ‰í† ë¦¬ ì—†ìŒ")

def cleanup_normalized_files():
    """Normalized íŒŒì¼ ì •ë¦¬ (Chunker ì™„ë£Œ í›„)"""
    print("\n" + "=" * 80)
    print("ğŸ§¹ Normalized íŒŒì¼ ì •ë¦¬")
    print("=" * 80)
    
    paths = get_transform_paths(__file__)
    normalized_dir = paths['normalized_dir']
    
    if normalized_dir.exists():
        # íŒŒì¼ ê°œìˆ˜ í™•ì¸
        normalized_files = list(normalized_dir.glob("*.jsonl"))
        file_count = len(normalized_files)
        
        if file_count > 0:
            # ë””ë ‰í† ë¦¬ ì „ì²´ ì‚­ì œ
            shutil.rmtree(normalized_dir)
            print(f"âœ… Normalized íŒŒì¼ {file_count}ê°œ ì‚­ì œ ì™„ë£Œ")
            print(f"   ì‚­ì œëœ ë””ë ‰í† ë¦¬: {normalized_dir}")
        else:
            print("â„¹ï¸  ì‚­ì œí•  Normalized íŒŒì¼ ì—†ìŒ")
    else:
        print("â„¹ï¸  Normalized ë””ë ‰í† ë¦¬ ì—†ìŒ")

def run_parser(process_all: bool = False) -> bool:
    """Step 1: Parser ì‹¤í–‰"""
    print("=" * 80)
    print("ğŸ”§ Step 1: Parser ì‹¤í–‰")
    print("=" * 80)
    
    try:
        # parser.py ëª¨ë“ˆ import ë° ì‹¤í–‰
        import importlib.util
        parser_path = SCRIPT_DIR / "parser.py"
        spec = importlib.util.spec_from_file_location("parser", parser_path)
        parser_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(parser_module)
        parser_main = parser_module.main
        
        parser_main(process_all=process_all)
        print("âœ… Parser ì™„ë£Œ")
        return True
        
    except Exception as e:
        print(f"âŒ Parser ì‹¤íŒ¨: {e}")
        return False

def run_normalizer() -> bool:
    """Step 2: Normalizer ì‹¤í–‰"""
    print("\n" + "=" * 80)
    print("ğŸ”§ Step 2: Normalizer ì‹¤í–‰")
    print("=" * 80)
    
    try:
        # normalizer.py ëª¨ë“ˆ import ë° ì‹¤í–‰
        import importlib.util
        normalizer_path = SCRIPT_DIR / "normalizer.py"
        spec = importlib.util.spec_from_file_location("normalizer", normalizer_path)
        normalizer_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(normalizer_module)
        normalizer_main = normalizer_module.main
        
        # sys.argvë¥¼ ì„ì‹œë¡œ ìˆ˜ì •í•˜ì—¬ ë””ë ‰í† ë¦¬ ëª¨ë“œë¡œ ì‹¤í–‰
        original_argv = sys.argv.copy()
        sys.argv = [sys.argv[0]]  # ìŠ¤í¬ë¦½íŠ¸ ì´ë¦„ë§Œ ë‚¨ê¹€
        
        try:
            normalizer_main()
            print("âœ… Normalizer ì™„ë£Œ")
        finally:
            # sys.argv ë³µì›
            sys.argv = original_argv
        
        # Parser íŒŒì¼ ì •ë¦¬
        cleanup_parser_files()
        
        return True
        
    except Exception as e:
        print(f"âŒ Normalizer ì‹¤íŒ¨: {e}")
        return False

def run_chunker() -> bool:
    """Step 3: Chunker ì‹¤í–‰"""
    print("\n" + "=" * 80)
    print("ğŸ”§ Step 3: Chunker ì‹¤í–‰")
    print("=" * 80)
    
    try:
        # chunker.py ëª¨ë“ˆ import ë° ì‹¤í–‰
        import importlib.util
        chunker_path = SCRIPT_DIR / "chunker.py"
        spec = importlib.util.spec_from_file_location("chunker", chunker_path)
        chunker_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(chunker_module)
        chunker_main = chunker_module.main
        
        # sys.argvë¥¼ ì„ì‹œë¡œ ìˆ˜ì •í•˜ì—¬ ë””ë ‰í† ë¦¬ ëª¨ë“œë¡œ ì‹¤í–‰
        original_argv = sys.argv.copy()
        sys.argv = [sys.argv[0]]  # ìŠ¤í¬ë¦½íŠ¸ ì´ë¦„ë§Œ ë‚¨ê¹€
        
        try:
            chunker_main()
            print("âœ… Chunker ì™„ë£Œ")
        finally:
            # sys.argv ë³µì›
            sys.argv = original_argv
        
        # Normalized íŒŒì¼ ì •ë¦¬
        cleanup_normalized_files()
        
        return True
        
    except Exception as e:
        print(f"âŒ Chunker ì‹¤íŒ¨: {e}")
        return False

def check_input_files(process_all: bool = False) -> bool:
    """ì…ë ¥ íŒŒì¼ í™•ì¸"""
    print("=" * 80)
    print("ğŸ“ ì…ë ¥ íŒŒì¼ í™•ì¸")
    print("=" * 80)
    
    # ê²½ë¡œ ì„¤ì •
    paths = get_transform_paths(__file__)
    markdown_dir = paths['markdown_dir']
    
    if not markdown_dir.exists():
        print(f"âŒ ë§ˆí¬ë‹¤ìš´ ë””ë ‰í† ë¦¬ ì—†ìŒ: {markdown_dir}")
        return False
    
    # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ëª©ë¡
    md_files = list(markdown_dir.glob("*.md"))
    
    if not md_files:
        print(f"âŒ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì—†ìŒ: {markdown_dir}")
        return False
    
    print(f"âœ… ë§ˆí¬ë‹¤ìš´ íŒŒì¼: {len(md_files)}ê°œ")
    
    if process_all:
        print(f"   â†’ ì „ì²´ íŒŒì¼ ì²˜ë¦¬ ëª¨ë“œ")
    else:
        print(f"   â†’ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì²˜ìŒ 20ê°œë§Œ)")
        md_files = md_files[:20]
    
    print(f"   â†’ ì²˜ë¦¬í•  íŒŒì¼: {len(md_files)}ê°œ")
    
    # ìƒ˜í”Œ íŒŒì¼ëª… ì¶œë ¥
    if md_files:
        print(f"   â†’ ìƒ˜í”Œ: {md_files[0].name}")
        if len(md_files) > 1:
            print(f"   â†’ ìƒ˜í”Œ: {md_files[1].name}")
    
    return True

def check_output_directories():
    """ì¶œë ¥ ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±"""
    print("\n" + "=" * 80)
    print("ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬ í™•ì¸")
    print("=" * 80)
    
    paths = get_transform_paths(__file__)
    output_dirs = [
        paths['parser_dir'],
        paths['normalized_dir'], 
        paths['final_dir']
    ]
    
    for output_dir in output_dirs:
        output_dir.mkdir(parents=True, exist_ok=True)
        print(f"âœ… {output_dir.name}: {output_dir}")
    
    return True

def get_pipeline_stats():
    """íŒŒì´í”„ë¼ì¸ ê²°ê³¼ í†µê³„"""
    print("\n" + "=" * 80)
    print("ğŸ“Š íŒŒì´í”„ë¼ì¸ ê²°ê³¼ í†µê³„")
    print("=" * 80)
    
    paths = get_transform_paths(__file__)
    
    # ê° ë‹¨ê³„ë³„ íŒŒì¼ ìˆ˜ í™•ì¸
    stages = {
        "Parser": paths['parser_dir'],
        "Normalized": paths['normalized_dir'],
        "Final": paths['final_dir']
    }
    
    for stage_name, stage_dir in stages.items():
        if stage_dir.exists():
            files = list(stage_dir.glob("*.jsonl"))
            print(f"  {stage_name:12s}: {len(files):4,}ê°œ íŒŒì¼")
        else:
            if stage_name in ["Parser", "Normalized"]:
                print(f"  {stage_name:12s}: ì •ë¦¬ë¨ (ì‚­ì œ)")
            else:
                print(f"  {stage_name:12s}: ë””ë ‰í† ë¦¬ ì—†ìŒ")
    
    # Final íŒŒì¼ì˜ ì´ ì²­í¬ ìˆ˜
    final_dir = paths['final_dir']
    if final_dir.exists():
        total_chunks = 0
        for jsonl_file in final_dir.glob("*.jsonl"):
            with open(jsonl_file, 'r', encoding='utf-8') as f:
                total_chunks += sum(1 for _ in f)
        
        print(f"\n  ì´ ì²­í¬ ìˆ˜: {total_chunks:,}ê°œ")
        
        # ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
        total_size = sum(f.stat().st_size for f in final_dir.glob("*.jsonl"))
        size_mb = total_size / (1024 * 1024)
        print(f"  ì´ íŒŒì¼ í¬ê¸°: {size_mb:.1f} MB")

def main():
    """ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    
    # ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬
    if len(sys.argv) > 1 and sys.argv[1] in ["--help", "-h"]:
        print("RAG ë°ì´í„° ë³€í™˜ íŒŒì´í”„ë¼ì¸")
        print()
        print("ì‚¬ìš©ë²•:")
        print(f"  python {Path(__file__).name}        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ (20ê°œ íŒŒì¼)")
        print(f"  python {Path(__file__).name} --all  # ì „ì²´ íŒŒì¼ ì²˜ë¦¬")
        print(f"  python {Path(__file__).name} --help # ë„ì›€ë§")
        print()
        print("íŒŒì´í”„ë¼ì¸ ë‹¨ê³„:")
        print("  1. Parser: ë§ˆí¬ë‹¤ìš´ â†’ êµ¬ì¡°í™”ëœ ì²­í¬")
        print("  2. Normalizer: í…ìŠ¤íŠ¸ ì •ê·œí™” ë° ë‹¨ìœ„ ë³€í™˜")
        print("     â†’ Parser íŒŒì¼ ìë™ ì •ë¦¬")
        print("  3. Chunker: ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€")
        print("     â†’ Normalized íŒŒì¼ ìë™ ì •ë¦¬")
        print()
        print("ìë™ ì •ë¦¬:")
        print("  â€¢ Normalizer ì™„ë£Œ í›„ Parser íŒŒì¼ ì‚­ì œ")
        print("  â€¢ Chunker ì™„ë£Œ í›„ Normalized íŒŒì¼ ì‚­ì œ")
        print("  â€¢ ìµœì¢… ê²°ê³¼ë§Œ Final í´ë”ì— ë³´ì¡´")
        sys.exit(0)
    
    # --all ì˜µì…˜ í™•ì¸
    process_all = len(sys.argv) > 1 and sys.argv[1] == "--all"
    
    # ì‹œì‘ ì‹œê°„ ê¸°ë¡
    start_time = time.time()
    
    print("ğŸš€ RAG ë°ì´í„° ë³€í™˜ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print(f"   ëª¨ë“œ: {'ì „ì²´ íŒŒì¼ ì²˜ë¦¬' if process_all else 'í…ŒìŠ¤íŠ¸ ëª¨ë“œ (20ê°œ íŒŒì¼)'}")
    print(f"   ì‹œì‘ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # 1. ì…ë ¥ íŒŒì¼ í™•ì¸
    if not check_input_files(process_all):
        print("\nâŒ ì…ë ¥ íŒŒì¼ í™•ì¸ ì‹¤íŒ¨")
        sys.exit(1)
    
    # 2. ì¶œë ¥ ë””ë ‰í† ë¦¬ í™•ì¸
    if not check_output_directories():
        print("\nâŒ ì¶œë ¥ ë””ë ‰í† ë¦¬ í™•ì¸ ì‹¤íŒ¨")
        sys.exit(1)
    
    # 3. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    success_count = 0
    total_steps = 3
    
    # Step 1: Parser
    if run_parser(process_all):
        success_count += 1
    else:
        print("\nâŒ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨: Parser ì‹¤íŒ¨")
        sys.exit(1)
    
    # Step 2: Normalizer
    if run_normalizer():
        success_count += 1
    else:
        print("\nâŒ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨: Normalizer ì‹¤íŒ¨")
        sys.exit(1)
    
    # Step 3: Chunker
    if run_chunker():
        success_count += 1
    else:
        print("\nâŒ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨: Chunker ì‹¤íŒ¨")
        sys.exit(1)
    
    # 4. ê²°ê³¼ í†µê³„
    get_pipeline_stats()
    
    # 5. ì™„ë£Œ ë©”ì‹œì§€
    end_time = time.time()
    duration = end_time - start_time
    
    print("\n" + "=" * 80)
    print("ğŸ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print("=" * 80)
    print(f"   ì„±ê³µ ë‹¨ê³„: {success_count}/{total_steps}")
    print(f"   ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ")
    print(f"   ì™„ë£Œ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    if success_count == total_steps:
        print("   ìƒíƒœ: âœ… ëª¨ë“  ë‹¨ê³„ ì„±ê³µ")
    else:
        print("   ìƒíƒœ: âš ï¸  ì¼ë¶€ ë‹¨ê³„ ì‹¤íŒ¨")
        sys.exit(1)

if __name__ == "__main__":
    main()
