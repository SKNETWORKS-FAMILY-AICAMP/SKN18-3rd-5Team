#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í†µí•© RAG í‰ê°€ ë„êµ¬
- RAG ê²€ìƒ‰ ê²°ê³¼ ìƒì„± (rag_success í˜•ì‹)
- RAGAs ê¸°ë°˜ í‰ê°€ (Faithfulness, Answer Relevancy ë“±)
- ë‹¨ì¼ ëª¨ë“œ, í†µí•© ê²°ê³¼ë¬¼
"""

import sys
import json
import argparse
import time
import logging
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€ (ì§ì ‘ ì‹¤í–‰ ì‹œ ê²½ë¡œ ë¬¸ì œ í•´ê²°)
current_file = Path(__file__).resolve()
project_root = current_file.parents[3]  # service/rag/evaluation/ -> project_root (SKN18-3rd-5Team)
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from service.rag.rag_system import RAGSystem
from service.rag.models.config import EmbeddingModelType
from service.rag.evaluation.evaluator import RAGEvaluator, EvaluationConfig
from service.rag.evaluation.metrics import MetricsCalculator
from service.rag.cli.rag_cli import RAGJSONLSystem, get_db_config
from config.vector_database import get_vector_db_config
from service.rag.generation.generator import OllamaGenerator, OpenAIGenerator, GenerationConfig

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class RAGEvaluation:
    """RAG í‰ê°€ ê²°ê³¼"""
    query: str
    answer: str
    contexts: List[str]
    ground_truth: Optional[str] = None

    # ë©”íŠ¸ë¦­ ì ìˆ˜
    faithfulness: Optional[float] = None
    answer_relevancy: Optional[float] = None
    context_precision: Optional[float] = None
    context_recall: Optional[float] = None

    # í‰ê·  ì ìˆ˜
    average_score: Optional[float] = None

    def calculate_average(self):
        """í‰ê·  ì ìˆ˜ ê³„ì‚°"""
        scores = [
            s for s in [
                self.faithfulness,
                self.answer_relevancy,
                self.context_precision,
                self.context_recall
            ] if s is not None
        ]
        if scores:
            self.average_score = sum(scores) / len(scores)


class RAGASEvaluator:
    """
    RAGAs ê¸°ë°˜ í‰ê°€ê¸°

    Note: ì‹¤ì œ RAGAsëŠ” LLM(OpenAI GPT-4 ë“±)ì„ ì‚¬ìš©í•˜ì—¬ í‰ê°€í•©ë‹ˆë‹¤.
    ì—¬ê¸°ì„œëŠ” ê°„ì†Œí™”ëœ ë²„ì „ì„ êµ¬í˜„í•˜ê³ , ì‹¤ì œ RAGAs ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©ë²•ë„ ì œê³µí•©ë‹ˆë‹¤.
    """

    def __init__(self, use_ragas_library: bool = False):
        """
        Args:
            use_ragas_library: ragas ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ì—¬ë¶€
        """
        self.use_ragas_library = use_ragas_library

        if use_ragas_library:
            try:
                from ragas import evaluate
                from ragas.metrics import (
                    faithfulness,
                    answer_relevancy,
                    context_precision,
                    context_recall
                )
                self.ragas_evaluate = evaluate
                self.metrics = {
                    'faithfulness': faithfulness,
                    'answer_relevancy': answer_relevancy,
                    'context_precision': context_precision,
                    'context_recall': context_recall
                }
                logger.info("RAGAs library loaded successfully")
            except ImportError:
                logger.warning("RAGAs library not found. Install with: pip install ragas")
                self.use_ragas_library = False

    def evaluate_faithfulness(
        self,
        answer: str,
        contexts: List[str]
    ) -> float:
        """
        Faithfulness (ì¶©ì‹¤ì„±) í‰ê°€
        ë‹µë³€ì˜ ê° ë¬¸ì¥ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì¶”ë¡  ê°€ëŠ¥í•œì§€ í‰ê°€

        ê°„ì†Œí™”ëœ ë²„ì „: ë‹µë³€ì˜ í‚¤ì›Œë“œê°€ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸

        Args:
            answer: ìƒì„±ëœ ë‹µë³€
            contexts: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            Faithfulness ì ìˆ˜ (0.0 ~ 1.0)
        """
        if not answer or not contexts:
            return 0.0

        # ë‹µë³€ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ê°„ë‹¨íˆ ë§ˆì¹¨í‘œ ê¸°ì¤€)
        sentences = [s.strip() for s in answer.split('.') if s.strip()]
        if not sentences:
            return 0.0

        # ì „ì²´ ì»¨í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
        full_context = ' '.join(contexts)

        # ê° ë¬¸ì¥ì˜ í‚¤ì›Œë“œê°€ ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸
        supported_sentences = 0
        for sentence in sentences:
            # ë¬¸ì¥ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (3ê¸€ì ì´ìƒ)
            words = [w for w in sentence.split() if len(w) >= 3]
            if not words:
                continue

            # í‚¤ì›Œë“œ ì¤‘ 50% ì´ìƒì´ ì»¨í…ìŠ¤íŠ¸ì— ìˆìœ¼ë©´ ì§€ì›ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
            supported_words = sum(1 for w in words if w in full_context)
            if supported_words / len(words) >= 0.5:
                supported_sentences += 1

        return supported_sentences / len(sentences)

    def evaluate_answer_relevancy(
        self,
        query: str,
        answer: str
    ) -> float:
        """
        Answer Relevancy (ë‹µë³€ ê´€ë ¨ì„±) í‰ê°€
        ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ì§€ í‰ê°€

        ê°„ì†Œí™”ëœ ë²„ì „: ì§ˆë¬¸ì˜ í‚¤ì›Œë“œê°€ ë‹µë³€ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸

        Args:
            query: ì§ˆë¬¸
            answer: ìƒì„±ëœ ë‹µë³€

        Returns:
            Answer Relevancy ì ìˆ˜ (0.0 ~ 1.0)
        """
        if not query or not answer:
            return 0.0

        # ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
        query_words = set(w for w in query.split() if len(w) >= 2)
        if not query_words:
            return 0.0

        # ë‹µë³€ì— ìˆëŠ” ì§ˆë¬¸ í‚¤ì›Œë“œ ìˆ˜
        matching_words = sum(1 for w in query_words if w in answer)

        return matching_words / len(query_words)

    def evaluate_context_precision(
        self,
        query: str,
        contexts: List[str],
        ground_truth: Optional[str] = None
    ) -> float:
        """
        Context Precision (ì»¨í…ìŠ¤íŠ¸ ì •ë°€ë„) í‰ê°€
        ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë‹µ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ”ì§€ í‰ê°€

        ê°„ì†Œí™”ëœ ë²„ì „: ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸

        Args:
            query: ì§ˆë¬¸
            contexts: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            ground_truth: ì •ë‹µ (ìˆëŠ” ê²½ìš°)

        Returns:
            Context Precision ì ìˆ˜ (0.0 ~ 1.0)
        """
        if not contexts:
            return 0.0

        # ì§ˆë¬¸ í‚¤ì›Œë“œ
        query_words = set(w for w in query.split() if len(w) >= 2)
        if not query_words:
            return 0.0

        # ê° ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆëŠ”ì§€ í™•ì¸
        relevant_contexts = 0
        for context in contexts:
            matching_words = sum(1 for w in query_words if w in context)
            if matching_words / len(query_words) >= 0.3:  # 30% ì´ìƒ ë§¤ì¹­
                relevant_contexts += 1

        return relevant_contexts / len(contexts)

    def evaluate_context_recall(
        self,
        contexts: List[str],
        ground_truth: Optional[str] = None
    ) -> float:
        """
        Context Recall (ì»¨í…ìŠ¤íŠ¸ ì¬í˜„ìœ¨) í‰ê°€
        ì •ë‹µì— í•„ìš”í•œ ì •ë³´ë¥¼ ëª¨ë‘ ê²€ìƒ‰í–ˆëŠ”ì§€ í‰ê°€

        Note: ground_truthê°€ ì—†ìœ¼ë©´ í‰ê°€ ë¶ˆê°€

        Args:
            contexts: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            ground_truth: ì •ë‹µ

        Returns:
            Context Recall ì ìˆ˜ (0.0 ~ 1.0)
        """
        if not ground_truth or not contexts:
            return 0.0

        # ì •ë‹µì˜ í‚¤ì›Œë“œ
        truth_words = set(w for w in ground_truth.split() if len(w) >= 2)
        if not truth_words:
            return 0.0

        # ì „ì²´ ì»¨í…ìŠ¤íŠ¸
        full_context = ' '.join(contexts)

        # ì •ë‹µ í‚¤ì›Œë“œ ì¤‘ ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ë¹„ìœ¨
        covered_words = sum(1 for w in truth_words if w in full_context)

        return covered_words / len(truth_words)

    def evaluate(
        self,
        query: str,
        answer: str,
        contexts: List[str],
        ground_truth: Optional[str] = None
    ) -> RAGEvaluation:
        """
        ì „ì²´ RAG í‰ê°€ ìˆ˜í–‰

        Args:
            query: ì§ˆë¬¸
            answer: ìƒì„±ëœ ë‹µë³€
            contexts: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            ground_truth: ì •ë‹µ (ì„ íƒ)

        Returns:
            RAGEvaluation ê²°ê³¼
        """
        evaluation = RAGEvaluation(
            query=query,
            answer=answer,
            contexts=contexts,
            ground_truth=ground_truth
        )

        # ê° ë©”íŠ¸ë¦­ í‰ê°€
        evaluation.faithfulness = self.evaluate_faithfulness(answer, contexts)
        evaluation.answer_relevancy = self.evaluate_answer_relevancy(query, answer)
        evaluation.context_precision = self.evaluate_context_precision(query, contexts, ground_truth)

        if ground_truth:
            evaluation.context_recall = self.evaluate_context_recall(contexts, ground_truth)

        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        evaluation.calculate_average()

        return evaluation

    def evaluate_batch(
        self,
        test_cases: List[Dict[str, Any]]
    ) -> List[RAGEvaluation]:
        """
        ë°°ì¹˜ í‰ê°€

        Args:
            test_cases: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸
                ê° ì¼€ì´ìŠ¤ëŠ” {'query', 'answer', 'contexts', 'ground_truth'} í¬í•¨

        Returns:
            í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        for case in test_cases:
            evaluation = self.evaluate(
                query=case['query'],
                answer=case['answer'],
                contexts=case['contexts'],
                ground_truth=case.get('ground_truth')
            )
            results.append(evaluation)

        return results

    def get_summary_statistics(
        self,
        evaluations: List[RAGEvaluation]
    ) -> Dict[str, float]:
        """
        í‰ê°€ ê²°ê³¼ í†µê³„ ìš”ì•½

        Args:
            evaluations: í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

        Returns:
            í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        metrics = {
            'faithfulness': [],
            'answer_relevancy': [],
            'context_precision': [],
            'context_recall': [],
            'average': []
        }

        for eval_result in evaluations:
            if eval_result.faithfulness is not None:
                metrics['faithfulness'].append(eval_result.faithfulness)
            if eval_result.answer_relevancy is not None:
                metrics['answer_relevancy'].append(eval_result.answer_relevancy)
            if eval_result.context_precision is not None:
                metrics['context_precision'].append(eval_result.context_precision)
            if eval_result.context_recall is not None:
                metrics['context_recall'].append(eval_result.context_recall)
            if eval_result.average_score is not None:
                metrics['average'].append(eval_result.average_score)

        # í†µê³„ ê³„ì‚°
        summary = {}
        for metric_name, values in metrics.items():
            if values:
                summary[f'{metric_name}_mean'] = np.mean(values)
                summary[f'{metric_name}_std'] = np.std(values)
                summary[f'{metric_name}_min'] = np.min(values)
                summary[f'{metric_name}_max'] = np.max(values)

        return summary


class UnifiedRAGEvaluator:
    """í†µí•© RAG í‰ê°€ ë„êµ¬"""

    def __init__(self,
                 model: str = "gemma"):  # "gemma" ë˜ëŠ” "openai"
        """
        ì´ˆê¸°í™”

        Args:
            model: ì‚¬ìš©í•  ëª¨ë¸ ("gemma" ë˜ëŠ” "openai")
        """
        self.model = model

        # ì‹¬í”Œ RAG ì‹œìŠ¤í…œ ì‚¬ìš© (ì•ˆì •ì„± ìš°ì„ )
        self.rag_system = RAGJSONLSystem(
            db_config=get_db_config(),
            embedding_model="intfloat/multilingual-e5-small"
        )

        # LLM Generator ì´ˆê¸°í™”
        if model == "openai":
            logger.info("Using OpenAI Generator (gpt-5-nano)")
            self.generator = OpenAIGenerator(default_model="gpt-5-nano")
            model_name = "gpt-5-nano"
            timeout = 60  # OpenAIëŠ” ë” ë¹ ë¦„
        else:  # gemma
            logger.info("Using Ollama Generator (gemma3:4b)")
            self.generator = OllamaGenerator(
                base_url="http://localhost:11434",
                default_model="gemma3:4b"
            )
            model_name = "gemma3:4b"
            timeout = 120  # OllamaëŠ” ë” ëŠë¦¼

        # Generation Config
        self.generation_config = GenerationConfig(
            model=model_name,
            temperature=0.7,
            max_tokens=512,
            top_p=0.9,
            timeout=timeout
        )

        # ë©”íŠ¸ë¦­ ê³„ì‚°ê¸° ì´ˆê¸°í™”
        self.metrics_calculator = MetricsCalculator()

        # RAGAs í‰ê°€ê¸° ì´ˆê¸°í™”
        self.ragas_evaluator = RAGASEvaluator(use_ragas_library=False)

        logger.info(f"Unified RAG Evaluation Tool ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"  - Model: {model_name}")
    
    def _load_evaluation_queries(self) -> List[Dict[str, Any]]:
        """í‰ê°€ ì¿¼ë¦¬ ë¡œë“œ"""
        queries_path = project_root / "service" / "rag" / "evaluation" / "evaluation_queries.json"
        try:
            with open(queries_path, 'r', encoding='utf-8') as f:
                queries = json.load(f)
            logger.info(f"í‰ê°€ ì¿¼ë¦¬ ë¡œë“œ ì™„ë£Œ: {len(queries)}ê°œ")
            return queries
        except Exception as e:
            logger.error(f"í‰ê°€ ì¿¼ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def run_evaluation(self, 
                      queries: List[Dict[str, Any]] = None,
                      top_k: int = 5,
                      min_similarity: float = 0.0,
                      corp_filter: str = None) -> List[Dict[str, Any]]:
        """
        RAG í‰ê°€ ì‹¤í–‰ (í†µí•©ëœ ë‹¨ì¼ ë©”ì„œë“œ)
        
        Args:
            queries: ê²€ìƒ‰í•  ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: evaluation_queries.json)
            top_k: ìƒìœ„ Kê°œ ê²°ê³¼ ë°˜í™˜
            min_similarity: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
            corp_filter: ê¸°ì—… í•„í„° (ì˜ˆ: "ì‚¼ì„±ì „ì")
        
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if queries is None:
            queries = self._load_evaluation_queries()
        
        logger.info("ğŸš€ RAG í‰ê°€ ì‹œì‘")
        logger.info(f"ğŸ“Š ëª¨ë¸: {self.model}")
        logger.info(f"ğŸ” Top-K: {top_k}")
        if corp_filter:
            logger.info(f"ğŸ¢ ê¸°ì—… í•„í„°: {corp_filter}")
        logger.info("=" * 80)
        
        results = []
        
        for i, query_data in enumerate(queries, 1):
            query_id = query_data.get("query_id", f"Q{i:03d}")
            query_text = query_data.get("query", "")
            
            # ì¿¼ë¦¬ë³„ íšŒì‚¬ëª… í•„í„° ì¶”ì¶œ
            query_company_filter = None
            if "company_name" in query_data:
                company_name = query_data["company_name"]
                if isinstance(company_name, list):
                    # ì—¬ëŸ¬ íšŒì‚¬ ë¹„êµ ì¿¼ë¦¬ì˜ ê²½ìš° í•„í„° ì—†ì´ ê²€ìƒ‰
                    query_company_filter = None
                    logger.info(f"  ë‹¤ì¤‘ íšŒì‚¬ ë¹„êµ ì¿¼ë¦¬: {company_name}")
                else:
                    query_company_filter = company_name
                    logger.info(f"  íšŒì‚¬ í•„í„° ì ìš©: {company_name}")
            
            logger.info(f"[{i}/{len(queries)}] {query_id}: {query_text}")
            
            try:
                # ì‹¬í”Œ ê²€ìƒ‰ ì‹¤í–‰ (ì¿¼ë¦¬ë³„ íšŒì‚¬ í•„í„° ì ìš©)
                search_results = self.rag_system.search(
                    query=query_text,
                    top_k=top_k,
                    min_similarity=min_similarity,
                    corp_filter=query_company_filter
                )
                
                # ê²°ê³¼ ì²˜ë¦¬
                if search_results:
                    # ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ (natural_text ë˜ëŠ” chunk_text í•„ë“œ ì‚¬ìš©)
                    contexts = [
                        result.get("natural_text", result.get("chunk_text", "")) 
                        for result in search_results
                    ]
                    
                    # ë¹ˆ ì»¨í…ìŠ¤íŠ¸ í•„í„°ë§
                    contexts = [ctx for ctx in contexts if ctx.strip()]
                    
                    # ì»¨í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
                    context_text = "\n\n".join(contexts)
                    
                    # Ollama Generatorë¡œ ë‹µë³€ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
                    try:
                        generated_result = self.generator.generate(
                            query=query_text,
                            context=context_text,
                            config=self.generation_config
                        )
                        generated_answer = generated_result.answer
                        logger.info(f"  ë‹µë³€ ìƒì„± ì™„ë£Œ: {len(generated_answer)} chars")
                        if generated_result.generation_time_ms:
                            logger.info(f"  ìƒì„± ì‹œê°„: {generated_result.generation_time_ms:.0f}ms")
                    except Exception as e:
                        logger.error(f"  ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
                        generated_answer = "ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                    
                    # RAGAs í‰ê°€ ìˆ˜í–‰ (Reference-Free)
                    ragas_evaluation = self.ragas_evaluator.evaluate(
                        query=query_text,
                        answer=generated_answer,
                        contexts=contexts,
                        ground_truth=None  # Reference-Free í‰ê°€
                    )
                    
                    # ê²°ê³¼ êµ¬ì„±
                    result = {
                        "query_id": query_id,
                        "query": query_text,
                        "query_type": query_data.get("query_type", "unknown"),
                        "difficulty": query_data.get("difficulty", "medium"),
                        "search_results": search_results,
                        "contexts": contexts,
                        "answer": generated_answer,
                        "ragas_scores": {
                            "faithfulness": ragas_evaluation.faithfulness,
                            "answer_relevancy": ragas_evaluation.answer_relevancy,
                            "context_precision": ragas_evaluation.context_precision,
                            "context_recall": ragas_evaluation.context_recall,
                            "average_score": ragas_evaluation.average_score
                        },
                        "timestamp": datetime.now().isoformat()
                    }
                    
                    results.append(result)
                    logger.info(f"âœ… {query_id} ì™„ë£Œ - í‰ê·  ì ìˆ˜: {ragas_evaluation.average_score:.3f}")
                else:
                    logger.warning(f"âš ï¸ {query_id}: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                    
            except Exception as e:
                logger.error(f"âŒ {query_id} ì˜¤ë¥˜: {e}")
                continue
        
        logger.info("ğŸ‰ RAG í‰ê°€ ì™„ë£Œ!")
        logger.info(f"ğŸ“ˆ ì´ {len(results)}ê°œ ì¿¼ë¦¬ ì²˜ë¦¬ ì™„ë£Œ")
        
        return results
    
    def save_results(self, results: List[Dict[str, Any]], output_dir: str = "results") -> Dict[str, str]:
        """
        ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            results: í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ë”•ì…”ë„ˆë¦¬
        """
        # í˜„ì¬ íŒŒì¼ì˜ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ê²½ë¡œ ê³„ì‚°
        current_file = Path(__file__).resolve()
        # service/rag/evaluation -> service/rag (rag ë””ë ‰í† ë¦¬)
        rag_dir = current_file.parents[1]
        
        # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if Path(output_dir).is_absolute():
            output_path = Path(output_dir)
        else:
            # "results"ë§Œ ì…ë ¥ëœ ê²½ìš° rag/resultsë¡œ ì„¤ì •
            if output_dir == "results":
                output_path = rag_dir / "results"
            else:
                # rag ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ê³„ì‚°
                output_path = rag_dir / output_dir
            
        output_path.mkdir(parents=True, exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ìƒì„¸ ê²°ê³¼ ì €ì¥ (JSON)
        detailed_file = output_path / f"unified_evaluation_{timestamp}.json"
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ìš”ì•½ í†µê³„ ê³„ì‚°
        ragas_evaluations = []
        for result in results:
            ragas_eval = RAGEvaluation(
                query=result["query"],
                answer=result["answer"],
                contexts=result["contexts"],
                ground_truth=None  # Reference-Free í‰ê°€
            )
            ragas_eval.faithfulness = result["ragas_scores"]["faithfulness"]
            ragas_eval.answer_relevancy = result["ragas_scores"]["answer_relevancy"]
            ragas_eval.context_precision = result["ragas_scores"]["context_precision"]
            ragas_eval.context_recall = result["ragas_scores"]["context_recall"]
            ragas_eval.average_score = result["ragas_scores"]["average_score"]
            ragas_evaluations.append(ragas_eval)
        
        summary_stats = self.ragas_evaluator.get_summary_statistics(ragas_evaluations)

        # ìš”ì•½ ë³´ê³ ì„œ ì €ì¥ (TXT)
        summary_file = output_path / f"unified_summary_{timestamp}.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"Unified RAG Evaluation Summary Report\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Total Samples: {len(results)}\n")
            f.write(f"Model: {self.model}\n\n")
            
            f.write("=== RAGAs Metric Statistics ===\n")
            for metric_name, values in summary_stats.items():
                if 'mean' in metric_name:
                    base_name = metric_name.replace('_mean', '')
                    f.write(f"{base_name.upper()}:\n")
                    f.write(f"  Mean: {values:.3f}\n")
                    f.write(f"  Std:  {summary_stats.get(f'{base_name}_std', 0):.3f}\n")
                    f.write(f"  Min:  {summary_stats.get(f'{base_name}_min', 0):.3f}\n")
                    f.write(f"  Max:  {summary_stats.get(f'{base_name}_max', 0):.3f}\n\n")
            
            f.write("=== Individual Results ===\n")
            for i, result in enumerate(results):
                f.write(f"Sample {i+1} ({result['query_id']}):\n")
                f.write(f"  Query: {result['query'][:50]}...\n")
                f.write(f"  Type: {result['query_type']}\n")
                f.write(f"  Difficulty: {result['difficulty']}\n")
                f.write(f"  Faithfulness: {result['ragas_scores']['faithfulness']:.3f}\n")
                f.write(f"  Answer Relevancy: {result['ragas_scores']['answer_relevancy']:.3f}\n")
                f.write(f"  Context Precision: {result['ragas_scores']['context_precision']:.3f}\n")
                context_recall_str = f"{result['ragas_scores']['context_recall']:.3f}" if result['ragas_scores']['context_recall'] is not None else "N/A"
                f.write(f"  Context Recall: {context_recall_str}\n")
                f.write(f"  Average Score: {result['ragas_scores']['average_score']:.3f}\n\n")
        
        logger.info(f"ğŸ’¾ JSON ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {detailed_file}")
        logger.info(f"ğŸ“„ ìƒì„¸ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {summary_file}")
        
        return {
            "detailed_results": str(detailed_file),
            "summary_report": str(summary_file)
        }


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="í†µí•© RAG í‰ê°€ ë„êµ¬ - evaluation_queries.jsonì„ ì‚¬ìš©í•œ ìë™ í‰ê°€",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # Gemma ëª¨ë¸ë¡œ í‰ê°€ (ê¸°ë³¸ê°’)
  python unified_evaluator.py
  python unified_evaluator.py --model gemma

  # OpenAI ëª¨ë¸ë¡œ í‰ê°€
  python unified_evaluator.py --model openai
        """
    )
    parser.add_argument("--model", type=str, default="gemma", choices=["gemma", "openai"],
                       help="ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸ê°’: gemma)")

    args = parser.parse_args()

    # í‰ê°€ ë„êµ¬ ì´ˆê¸°í™”
    evaluator = UnifiedRAGEvaluator(model=args.model)
    
    # í‰ê°€ ì‹¤í–‰ (evaluation_queries.json ì‚¬ìš©)
    results = evaluator.run_evaluation()

    # ê²°ê³¼ ì €ì¥
    saved_files = evaluator.save_results(results, "results")
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½:")
    print(f"   - ì´ ì¿¼ë¦¬ ìˆ˜: {len(results)}")
    print(f"   - ëª¨ë¸: {evaluator.model}")
    print(f"   - ì„±ê³µí•œ ì¿¼ë¦¬: {len(results)}")
    
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜:")
    print(f"   - ìƒì„¸ ê²°ê³¼: {saved_files['detailed_results']}")
    print(f"   - ìš”ì•½ ë³´ê³ ì„œ: {saved_files['summary_report']}")


if __name__ == "__main__":
    main()
