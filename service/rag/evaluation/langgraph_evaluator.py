#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LangGraph íŒŒì´í”„ë¼ì¸ í‰ê°€ ë„êµ¬
ì „ì²´ LangGraph íŒŒì´í”„ë¼ì¸ (Router â†’ QueryRewrite â†’ Retrieve â†’ Rerank â†’ ContextTrim â†’ Generate â†’ GroundingCheck â†’ Guardrail â†’ Answer)ì„ í‰ê°€í•©ë‹ˆë‹¤.
"""

import sys
import json
import argparse
import logging
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
current_file = Path(__file__).resolve()
project_root = current_file.parents[3]  # service/rag/evaluation/ -> project_root
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from graph.app_graph import build_app
from graph.state import QAState
from graph.utils.level import defaults as get_level_config

# unified_evaluatorì—ì„œ RAGASEvaluator import
sys.path.insert(0, str(current_file.parent))
from unified_evaluator import RAGASEvaluator

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LangGraphEvaluator:
    """LangGraph íŒŒì´í”„ë¼ì¸ í‰ê°€ê¸°"""
    
    def __init__(self, model: str = "gemma", verify_metadata: bool = True):
        """
        ì´ˆê¸°í™”
        
        Args:
            model: ì‚¬ìš©í•  LLM ëª¨ë¸ ("gemma" ë˜ëŠ” "openai")
            verify_metadata: ë©”íƒ€ë°ì´í„° ê²€ì¦ ì‹¤í–‰ ì—¬ë¶€
        """
        self.model = model
        self.graph = build_app()
        self.pipeline_name = "LangGraph Full Pipeline"
        
        # RAGAs í‰ê°€ê¸° ì´ˆê¸°í™”
        self.ragas_evaluator = RAGASEvaluator(use_ragas_library=False)
        
        # ë©”íƒ€ë°ì´í„° ê²€ì¦ ì‹¤í–‰ (ê¸°ë³¸ê°’: True)
        if verify_metadata:
            self._verify_metadata_once()
        
        logger.info(f"LangGraph Evaluator ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"  - Pipeline: {self.pipeline_name}")
        logger.info(f"  - Model: {model}")
    
    def _verify_metadata_once(self):
        """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ê²€ì¦ (í•œ ë²ˆë§Œ ì‹¤í–‰)"""
        try:
            from service.rag.vectorstore.pgvector_store import PgVectorStore
            from service.rag.models.encoder import EmbeddingEncoder
            from service.rag.models.config import EmbeddingModelType
            
            logger.info("ğŸ” ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ê²€ì¦ ì¤‘...")
            
            # ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
            store = PgVectorStore()
            encoder = EmbeddingEncoder(EmbeddingModelType.MULTILINGUAL_E5_SMALL)
            
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
            test_query = "í…ŒìŠ¤íŠ¸"
            query_embedding = encoder.encode_query(test_query)
            
            # ê²€ìƒ‰ ì‹¤í–‰
            results = store.search_similar(
                query_embedding=query_embedding,
                model_type=EmbeddingModelType.MULTILINGUAL_E5_SMALL,
                top_k=1,
                min_similarity=0.0
            )
            
            # ë©”íƒ€ë°ì´í„° í™•ì¸
            if results and results[0].metadata:
                has_corp_name = bool(results[0].corp_name or results[0].metadata.get('corp_name'))
                if has_corp_name:
                    logger.info("âœ“ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì •ìƒ ì‘ë™")
                else:
                    logger.warning("âš ï¸ ë©”íƒ€ë°ì´í„°ì— corp_nameì´ ì—†ìŠµë‹ˆë‹¤")
            else:
                logger.warning("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                
        except Exception as e:
            logger.warning(f"âš ï¸ ë©”íƒ€ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
    
    def _load_evaluation_queries(self) -> List[Dict[str, Any]]:
        """í‰ê°€ ì¿¼ë¦¬ ë¡œë“œ"""
        queries_path = project_root / "service" / "rag" / "evaluation" / "evaluation_queries.json"
        try:
            with open(queries_path, 'r', encoding='utf-8') as f:
                queries = json.load(f)
            logger.info(f"í‰ê°€ ì¿¼ë¦¬ ë¡œë“œ ì™„ë£Œ: {len(queries)}ê°œ")
            return queries
        except Exception as e:
            logger.error(f"í‰ê°€ ì¿¼ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def _create_initial_state(self, query_data: Dict[str, Any], user_level: str = "intermediate") -> QAState:
        """
        ì´ˆê¸° State ìƒì„±
        
        Args:
            query_data: ì¿¼ë¦¬ ë°ì´í„°
            user_level: ì‚¬ìš©ì ìˆ˜ì¤€ ("beginner", "intermediate", "advanced")
        
        Returns:
            ì´ˆê¸° QAState
        """
        # ë ˆë²¨ë³„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        meta = get_level_config(user_level)
        
        initial_state = QAState(
            question=query_data.get("query", ""),
            user_level=user_level,
            rewritten_query="",  # QueryRewrite ë…¸ë“œì—ì„œ ì±„ì›Œì§
            retrieved=[],  # Retrieve ë…¸ë“œì—ì„œ ì±„ì›Œì§
            reranked=[],  # Rerank ë…¸ë“œì—ì„œ ì±„ì›Œì§
            context="",  # ContextTrim ë…¸ë“œì—ì„œ ì±„ì›Œì§
            draft_answer="",  # Generate ë…¸ë“œì—ì„œ ì±„ì›Œì§
            citations=[],  # ContextTrim ë…¸ë“œì—ì„œ ì±„ì›Œì§
            grounded=False,  # GroundingCheck ë…¸ë“œì—ì„œ ì±„ì›Œì§
            policy_flag=None,  # Guardrail ë…¸ë“œì—ì„œ ì±„ì›Œì§
            meta=meta
        )
        
        return initial_state
    
    def run_evaluation(
        self,
        queries: Optional[List[Dict[str, Any]]] = None,
        user_level: str = "intermediate"
    ) -> List[Dict[str, Any]]:
        """
        LangGraph íŒŒì´í”„ë¼ì¸ í‰ê°€ ì‹¤í–‰
        
        Args:
            queries: í‰ê°€ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: evaluation_queries.json)
            user_level: ì‚¬ìš©ì ìˆ˜ì¤€
        
        Returns:
            í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if queries is None:
            queries = self._load_evaluation_queries()
        
        logger.info("ğŸš€ LangGraph íŒŒì´í”„ë¼ì¸ í‰ê°€ ì‹œì‘")
        logger.info(f"ğŸ“Š Pipeline: {self.pipeline_name}")
        logger.info(f"ğŸ‘¤ User Level: {user_level}")
        logger.info(f"ğŸ” Total Queries: {len(queries)}")
        logger.info("=" * 80)
        
        results = []
        
        for i, query_data in enumerate(queries, 1):
            query_id = query_data.get("query_id", f"Q{i:03d}")
            query_text = query_data.get("query", "")
            
            logger.info(f"[{i}/{len(queries)}] {query_id}: {query_text[:50]}...")
            
            try:
                start_time = time.time()
                
                # ì´ˆê¸° State ìƒì„±
                initial_state = self._create_initial_state(query_data, user_level)
                
                # LangGraph ì‹¤í–‰
                final_state = self.graph.invoke(initial_state)
                
                execution_time = (time.time() - start_time) * 1000  # ms
                
                # RAGAs í‰ê°€ ìˆ˜í–‰
                answer = final_state.get("draft_answer", "")
                context_text = final_state.get("context", "")
                contexts = [ctx for ctx in context_text.split("\n\n---\n\n") if ctx.strip()]
                ground_truth = query_data.get("ground_truth_answer")
                
                # ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ (verify_metadata ìŠ¤íƒ€ì¼)
                retrieved_docs = []
                for doc in final_state.get("retrieved", []):
                    retrieved_docs.append({
                        "chunk_id": doc.get("chunk_id"),
                        "content_preview": doc.get("chunk_text", "")[:200] if isinstance(doc.get("chunk_text"), str) else "",
                        "corp_name": doc.get("metadata", {}).get("corp_name") if isinstance(doc.get("metadata"), dict) else None,
                        "document_name": doc.get("metadata", {}).get("document_name") if isinstance(doc.get("metadata"), dict) else None,
                        "doc_type": doc.get("metadata", {}).get("doc_type") if isinstance(doc.get("metadata"), dict) else None,
                        "rcept_dt": doc.get("metadata", {}).get("rcept_dt") if isinstance(doc.get("metadata"), dict) else None,
                        "fiscal_year": doc.get("metadata", {}).get("fiscal_year") if isinstance(doc.get("metadata"), dict) else None
                    })
                
                ragas_evaluation = self.ragas_evaluator.evaluate(
                    query=query_text,
                    answer=answer,
                    contexts=contexts,
                    ground_truth=ground_truth
                )
                
                # ê²°ê³¼ êµ¬ì„±
                result = {
                    "query_id": query_id,
                    "query": query_text,
                    "query_type": query_data.get("query_type", "unknown"),
                    "difficulty": query_data.get("difficulty", "medium"),
                    "user_level": user_level,
                    
                    # LangGraph íŒŒì´í”„ë¼ì¸ ê²°ê³¼
                    "retrieved_count": len(final_state.get("retrieved", [])),
                    "reranked_count": len(final_state.get("reranked", [])),
                    "citations_count": len(final_state.get("citations", [])),
                    "context_length": len(context_text),
                    "answer": answer,
                    "context": context_text,  # ì „ì²´ context ì¶”ê°€
                    "grounded": final_state.get("grounded", False),
                    "policy_flag": final_state.get("policy_flag"),
                    
                    # ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ (verify_metadata ìŠ¤íƒ€ì¼)
                    "retrieved_docs": retrieved_docs,
                    
                    # Citations ì •ë³´ (ë©”íƒ€ë°ì´í„° í¬í•¨)
                    "citations": self._extract_citations_info(final_state.get("citations", [])),
                    
                    # RAGAs í‰ê°€ ì ìˆ˜
                    "ragas_scores": {
                        "faithfulness": ragas_evaluation.faithfulness,
                        "answer_relevancy": ragas_evaluation.answer_relevancy,
                        "context_precision": ragas_evaluation.context_precision,
                        "context_recall": ragas_evaluation.context_recall,
                        "average_score": ragas_evaluation.average_score
                    },
                    
                    # Execution metrics
                    "execution_time_ms": execution_time,
                    "timestamp": datetime.now().isoformat()
                }
                
                results.append(result)
                
                # íŒŒì´í”„ë¼ì¸ ê²€ì¦
                pipeline_validation = self._validate_pipeline_output(result, final_state)
                result["pipeline_validation"] = pipeline_validation
                
                logger.info(f"âœ… {query_id} ì™„ë£Œ")
                logger.info(f"   - Retrieved: {result['retrieved_count']} docs")
                logger.info(f"   - Reranked: {result['reranked_count']} docs")
                logger.info(f"   - Citations: {result['citations_count']}")
                logger.info(f"   - Grounded: {result['grounded']}")
                logger.info(f"   - RAGAs Score: {ragas_evaluation.average_score:.3f}")
                logger.info(f"   - Pipeline Valid: {pipeline_validation['is_valid']}")
                logger.info(f"   - Time: {execution_time:.0f}ms")
                
            except Exception as e:
                logger.error(f"âŒ {query_id} ì˜¤ë¥˜: {e}")
                results.append({
                    "query_id": query_id,
                    "query": query_text,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                })
        
        logger.info("ğŸ‰ LangGraph íŒŒì´í”„ë¼ì¸ í‰ê°€ ì™„ë£Œ!")
        logger.info(f"ğŸ“ˆ ì´ {len(results)}ê°œ ì¿¼ë¦¬ ì²˜ë¦¬ ì™„ë£Œ")
        
        return results
    
    def _validate_pipeline_output(self, result: Dict[str, Any], final_state: QAState) -> Dict[str, Any]:
        """
        LangGraph íŒŒì´í”„ë¼ì¸ ì¶œë ¥ ê²€ì¦
        
        Args:
            result: í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            final_state: LangGraph ìµœì¢… ìƒíƒœ
        
        Returns:
            ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        validation = {
            "is_valid": True,
            "issues": [],
            "warnings": []
        }
        
        # 1. ê²€ìƒ‰ ë‹¨ê³„ ê²€ì¦
        if result['retrieved_count'] == 0:
            validation["issues"].append("No documents retrieved")
            validation["is_valid"] = False
        
        # 2. ë©”íƒ€ë°ì´í„° ê²€ì¦
        retrieved_docs = result.get('retrieved_docs', [])
        metadata_complete = 0
        for doc in retrieved_docs:
            if doc.get('corp_name') and doc.get('document_name'):
                metadata_complete += 1
        
        if retrieved_docs and metadata_complete == 0:
            validation["issues"].append("No metadata found in retrieved documents")
            validation["is_valid"] = False
        elif retrieved_docs and metadata_complete < len(retrieved_docs):
            validation["warnings"].append(f"Incomplete metadata: {metadata_complete}/{len(retrieved_docs)} docs")
        
        # 3. Context ê²€ì¦
        if not result.get('context') or len(result['context']) < 50:
            validation["issues"].append("Context too short or empty")
            validation["is_valid"] = False
        
        # 4. ë‹µë³€ ê²€ì¦
        answer = result.get('answer', '')
        if not answer or len(answer) < 10:
            validation["issues"].append("Answer too short or empty")
            validation["is_valid"] = False
        elif "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤" in answer:
            validation["issues"].append("Answer generation failed")
            validation["is_valid"] = False
        
        # 5. Grounding ê²€ì¦
        if not result.get('grounded'):
            validation["warnings"].append("Answer not grounded (no [ref:] citations)")
        
        # 6. Citations ê²€ì¦
        citations = result.get('citations', [])
        if not citations:
            validation["warnings"].append("No citations extracted")
        else:
            citation_with_metadata = sum(1 for c in citations if c.get('corp_name'))
            if citation_with_metadata == 0:
                validation["issues"].append("Citations missing metadata")
                validation["is_valid"] = False
        
        # 7. RAGAs ì ìˆ˜ ê²€ì¦
        ragas_scores = result.get('ragas_scores', {})
        avg_score = ragas_scores.get('average_score', 0)
        if avg_score < 0.3:
            validation["warnings"].append(f"Low RAGAs score: {avg_score:.3f}")
        
        return validation
    
    def _extract_citations_info(self, citations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Citations ì •ë³´ ì¶”ì¶œ (ë©”íƒ€ë°ì´í„° í¬í•¨)
        
        Args:
            citations: citations ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ì¶”ì¶œëœ citations ì •ë³´
        """
        extracted = []
        for citation in citations:
            citation_info = {
                "corp_name": citation.get("corp_name"),
                "document_name": citation.get("document_name"),
                "doc_type": citation.get("doc_type"),
                "chunk_id": citation.get("chunk_id")
            }
            extracted.append(citation_info)
        return extracted
    
    def save_results(self, results: List[Dict[str, Any]], output_dir: str = "results") -> Dict[str, str]:
        """
        ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            results: í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ë”•ì…”ë„ˆë¦¬
        """
        current_file = Path(__file__).resolve()
        rag_dir = current_file.parents[1]
        
        if Path(output_dir).is_absolute():
            output_path = Path(output_dir)
        else:
            if output_dir == "results":
                output_path = rag_dir / "results"
            else:
                output_path = rag_dir / output_dir
        
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ìƒì„¸ ê²°ê³¼ ì €ì¥ (JSON)
        detailed_file = output_path / f"langgraph_evaluation_{timestamp}.json"
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
        summary_file = output_path / f"langgraph_summary_{timestamp}.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"LangGraph Pipeline Evaluation Summary\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Pipeline: {self.pipeline_name}\n")
            f.write(f"Model: {self.model}\n")
            f.write(f"Total Samples: {len(results)}\n\n")
            
            # í†µê³„ ì •ë³´
            successful = [r for r in results if "error" not in r and "ragas_scores" in r]
            valid_pipelines = [r for r in successful if r.get('pipeline_validation', {}).get('is_valid', False)]
            
            f.write(f"=== Summary Statistics ===\n")
            f.write(f"Successful: {len(successful)}/{len(results)}\n")
            f.write(f"Valid Pipelines: {len(valid_pipelines)}/{len(successful)}\n")
            f.write(f"Grounded Answers: {sum(1 for r in successful if r.get('grounded'))}\n")
            f.write(f"Avg Retrieved: {sum(r.get('retrieved_count', 0) for r in successful) / len(successful) if successful else 0:.2f}\n")
            f.write(f"Avg Citations: {sum(r.get('citations_count', 0) for r in successful) / len(successful) if successful else 0:.2f}\n")
            f.write(f"Avg Execution Time: {sum(r.get('execution_time_ms', 0) for r in successful) / len(successful) if successful else 0:.2f}ms\n")
            if successful:
                f.write(f"\n=== RAGAs Metrics ===\n")
                ragas_scores = [r.get('ragas_scores', {}) for r in successful if r.get('ragas_scores')]
                if ragas_scores:
                    avg_faithfulness = sum(s.get('faithfulness', 0) or 0 for s in ragas_scores) / len(ragas_scores)
                    avg_relevancy = sum(s.get('answer_relevancy', 0) or 0 for s in ragas_scores) / len(ragas_scores)
                    avg_precision = sum(s.get('context_precision', 0) or 0 for s in ragas_scores) / len(ragas_scores)
                    
                    # Context Recall (ground_truthê°€ ìˆì„ ë•Œë§Œ)
                    recall_scores = [s.get('context_recall') for s in ragas_scores if s.get('context_recall') is not None]
                    avg_recall = sum(recall_scores) / len(recall_scores) if recall_scores else None
                    
                    avg_score = sum(s.get('average_score', 0) or 0 for s in ragas_scores) / len(ragas_scores)
                    
                    f.write(f"Avg Faithfulness: {avg_faithfulness:.3f}\n")
                    f.write(f"Avg Answer Relevancy: {avg_relevancy:.3f}\n")
                    f.write(f"Avg Context Precision: {avg_precision:.3f}\n")
                    if avg_recall is not None:
                        f.write(f"Avg Context Recall: {avg_recall:.3f}\n")
                    f.write(f"Avg RAGAs Score: {avg_score:.3f}\n")
            f.write("\n")
            
            # ê°œë³„ ê²°ê³¼
            f.write("=== Individual Results ===\n")
            for result in results:
                f.write(f"\n{result['query_id']}: {result['query'][:60]}...\n")
                if "error" in result:
                    f.write(f"  Error: {result['error']}\n")
                else:
                    f.write(f"  Retrieved: {result.get('retrieved_count', 0)}\n")
                    f.write(f"  Reranked: {result.get('reranked_count', 0)}\n")
                    f.write(f"  Citations: {result.get('citations_count', 0)}\n")
                    f.write(f"  Grounded: {result.get('grounded', False)}\n")
                    
                    # Pipeline validation
                    validation = result.get('pipeline_validation', {})
                    f.write(f"  Pipeline Valid: {validation.get('is_valid', False)}\n")
                    if validation.get('issues'):
                        f.write(f"    Issues: {', '.join(validation['issues'])}\n")
                    if validation.get('warnings'):
                        f.write(f"    Warnings: {', '.join(validation['warnings'])}\n")
                    
                    # RAGAs scores
                    if 'ragas_scores' in result and result['ragas_scores']:
                        f.write(f"  RAGAs Score: {result['ragas_scores'].get('average_score', 0):.3f}\n")
                        f.write(f"    - Faithfulness: {result['ragas_scores'].get('faithfulness', 0):.3f}\n")
                        f.write(f"    - Answer Relevancy: {result['ragas_scores'].get('answer_relevancy', 0):.3f}\n")
                        f.write(f"    - Context Precision: {result['ragas_scores'].get('context_precision', 0):.3f}\n")
                        recall = result['ragas_scores'].get('context_recall')
                        if recall is not None:
                            f.write(f"    - Context Recall: {recall:.3f}\n")
                    f.write(f"  Time: {result.get('execution_time_ms', 0):.0f}ms\n")
        
        logger.info(f"ğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {detailed_file}")
        logger.info(f"ğŸ“„ ìš”ì•½ ë³´ê³ ì„œ ì €ì¥: {summary_file}")
        
        return {
            "detailed_results": str(detailed_file),
            "summary_report": str(summary_file)
        }


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="LangGraph íŒŒì´í”„ë¼ì¸ í‰ê°€ ë„êµ¬ - ì „ì²´ íŒŒì´í”„ë¼ì¸ í‰ê°€",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê¸°ë³¸ í‰ê°€ (intermediate level)
  python langgraph_evaluator.py
  
  # Beginner level í‰ê°€
  python langgraph_evaluator.py --level beginner
  
  # Advanced level í‰ê°€
  python langgraph_evaluator.py --level advanced
  
  # ë©”íƒ€ë°ì´í„° ê²€ì¦ ì—†ì´ ì‹¤í–‰
  python langgraph_evaluator.py --skip-verify
        """
    )
    
    parser.add_argument("--level", type=str, default="intermediate", 
                       choices=["beginner", "intermediate", "advanced"],
                       help="ì‚¬ìš©ì ìˆ˜ì¤€ (ê¸°ë³¸ê°’: intermediate)")
    parser.add_argument("--skip-verify", action="store_true",
                       help="ë©”íƒ€ë°ì´í„° ê²€ì¦ ê±´ë„ˆë›°ê¸°")
    
    args = parser.parse_args()
    
    # í‰ê°€ ë„êµ¬ ì´ˆê¸°í™”
    evaluator = LangGraphEvaluator(model="gemma", verify_metadata=not args.skip_verify)
    
    # í‰ê°€ ì‹¤í–‰
    results = evaluator.run_evaluation(user_level=args.level)
    
    # ê²°ê³¼ ì €ì¥
    saved_files = evaluator.save_results(results, "results")
    
    # ê²°ê³¼ ì¶œë ¥
    successful = [r for r in results if "error" not in r]
    print(f"\nğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½:")
    print(f"   - ì´ ì¿¼ë¦¬ ìˆ˜: {len(results)}")
    print(f"   - ì„±ê³µ: {len(successful)}")
    print(f"   - ì‹¤íŒ¨: {len(results) - len(successful)}")
    print(f"   - Grounded: {sum(1 for r in successful if r.get('grounded'))}")
    
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜:")
    print(f"   - ìƒì„¸ ê²°ê³¼: {saved_files['detailed_results']}")
    print(f"   - ìš”ì•½ ë³´ê³ ì„œ: {saved_files['summary_report']}")


if __name__ == "__main__":
    main()

