#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ì—¬ëŸ¬ LLM ëª¨ë¸ë¡œ ê°™ì€ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ë¹„êµ
"""

import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(project_root))

from ..rag_system import RAGSystem
from ..generation.generator import OllamaGenerator, GenerationConfig
from ..models.config import EmbeddingModelType

import logging
import json
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def get_db_config() -> dict:
    """ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •"""
    return {
        'host': os.getenv('PG_HOST', 'localhost'),
        'port': os.getenv('PG_PORT', '5432'),
        'database': os.getenv('PG_DB', 'rey'),
        'user': os.getenv('PG_USER', 'postgres'),
        'password': os.getenv('PG_PASSWORD', 'post1234')
    }


def compare_models(query: str, models: list, embedding_model: str = "E5", save_results: bool = True):
    """
    ì—¬ëŸ¬ ëª¨ë¸ë¡œ ê°™ì€ ì§ˆë¬¸ì— ë‹µë³€ ìƒì„±í•˜ê³  ë¹„êµ

    Args:
        query: ì§ˆë¬¸
        models: ë¹„êµí•  LLM ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["gemma3:4b", "llama3.2:latest", "qwen3:4b"])
        embedding_model: ì„ë² ë”© ëª¨ë¸
        save_results: ê²°ê³¼ ì €ì¥ ì—¬ë¶€
    """
    db_config = get_db_config()
    results = {}

    print(f"\n{'='*80}")
    print(f"ì§ˆë¬¸: {query}")
    print(f"ì„ë² ë”© ëª¨ë¸: {embedding_model}")
    print(f"ë¹„êµ ëª¨ë¸: {', '.join(models)}")
    print(f"{'='*80}\n")

    # ê° ëª¨ë¸ë³„ë¡œ ë‹µë³€ ìƒì„±
    for model in models:
        print(f"\nğŸ¤– {model} ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
        print("-" * 80)

        try:
            # Generator ì´ˆê¸°í™”
            generator = OllamaGenerator(
                base_url="http://localhost:11434",
                default_model=model
            )

            # ëª¨ë¸ ìƒíƒœ í™•ì¸
            if not generator.check_health():
                print(f"âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                results[model] = {"error": "Ollama server not available"}
                continue

            available_models = generator.list_models()
            if model not in available_models:
                print(f"âš ï¸  ëª¨ë¸ '{model}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print(f"ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜: ollama pull {model}")
                results[model] = {"error": "Model not found"}
                continue

            # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            rag_system = RAGSystem(
                model_type=EmbeddingModelType.MULTILINGUAL_E5_SMALL,
                db_config=db_config,
                llm_generator=generator,
                enable_generation=True
            )

            # ë‹µë³€ ìƒì„±
            response = rag_system.generate_answer(
                query=query,
                top_k=5,
                use_reranker=False
            )

            # ê²°ê³¼ ì €ì¥
            results[model] = {
                "answer": response.generated_answer.answer,
                "generation_time_ms": response.generated_answer.generation_time_ms,
                "tokens_used": response.generated_answer.tokens_used,
                "num_docs": len(response.retrieved_documents),
                "avg_similarity": sum(d.get('similarity', 0) for d in response.retrieved_documents) / len(response.retrieved_documents) if response.retrieved_documents else 0
            }

            # ë‹µë³€ ì¶œë ¥
            print(f"\në‹µë³€:")
            print(response.generated_answer.answer)
            print(f"\nâ±ï¸  ìƒì„± ì‹œê°„: {response.generated_answer.generation_time_ms:.2f}ms")
            print(f"ğŸ“Š í† í° ìˆ˜: {response.generated_answer.tokens_used}")
            print(f"ğŸ“š ì°¸ê³  ë¬¸ì„œ: {len(response.retrieved_documents)}ê°œ")

        except Exception as e:
            logger.error(f"ëª¨ë¸ {model} í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
            results[model] = {"error": str(e)}

    # ë¹„êµ ê²°ê³¼ ì¶œë ¥
    print(f"\n\n{'='*80}")
    print("ğŸ“Š ëª¨ë¸ ë¹„êµ ê²°ê³¼")
    print(f"{'='*80}\n")

    print(f"{'ëª¨ë¸':<20} {'ìƒì„±ì‹œê°„(ms)':>15} {'í† í°ìˆ˜':>10} {'ë‹µë³€ê¸¸ì´':>10}")
    print("-" * 80)

    for model, result in results.items():
        if "error" not in result:
            gen_time = result['generation_time_ms']
            tokens = result['tokens_used']
            answer_len = len(result['answer'])
            print(f"{model:<20} {gen_time:>15.2f} {tokens:>10} {answer_len:>10}")
        else:
            print(f"{model:<20} {'ERROR':>15} {'-':>10} {'-':>10}")

    # ê²°ê³¼ ì €ì¥
    if save_results:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_query = "".join(c for c in query if c.isalnum() or c in (' ', '-', '_')).rstrip()
        safe_query = safe_query.replace(' ', '_')[:50]
        filename = f"model_comparison_{safe_query}_{timestamp}.json"

        os.makedirs("results", exist_ok=True)
        filepath = os.path.join("results", filename)

        comparison_data = {
            "query": query,
            "embedding_model": embedding_model,
            "timestamp": timestamp,
            "models": results
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(comparison_data, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ ë¹„êµ ê²°ê³¼ ì €ì¥: {filepath}")

    return results


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="ì—¬ëŸ¬ LLM ëª¨ë¸ ë‹µë³€ ë¹„êµ")
    parser.add_argument("query", type=str, help="ì§ˆë¬¸")
    parser.add_argument(
        "--models",
        type=str,
        nargs="+",
        default=["gemma3:4b", "llama3.2:latest", "qwen3:4b"],
        help="ë¹„êµí•  ëª¨ë¸ë“¤"
    )
    parser.add_argument("--embedding", type=str, default="E5", help="ì„ë² ë”© ëª¨ë¸")
    parser.add_argument("--no-save", action="store_true", help="ê²°ê³¼ ì €ì¥ ì•ˆ í•¨")

    args = parser.parse_args()

    compare_models(
        query=args.query,
        models=args.models,
        embedding_model=args.embedding,
        save_results=not args.no_save
    )
