#!/usr/bin/env python3
"""
RAG JSONL CLI - JSONL ê¸°ë°˜ RAG ì‹œìŠ¤í…œ ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤

JSONL íŒŒì¼ì„ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” RAG ì‹œìŠ¤í…œì˜ CLI ë„êµ¬ì…ë‹ˆë‹¤.
Parquet ë³€í™˜ ì—†ì´ JSONL íŒŒì¼ì—ì„œ ì§ì ‘ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ê²€ìƒ‰í•©ë‹ˆë‹¤.
"""

import argparse
import sys
import logging
import json
import os
from pathlib import Path
from typing import List, Dict, Any, Optional
import psycopg2
from psycopg2.extras import RealDictCursor
import numpy as np
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class RAGJSONLSystem:
    """JSONL ê¸°ë°˜ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(
        self,
        db_config: Dict[str, str],
        embedding_model: str = "intfloat/multilingual-e5-small"
    ):
        """
        Args:
            db_config: PostgreSQL ì—°ê²° ì„¤ì •
            embedding_model: ì„ë² ë”© ëª¨ë¸ëª…
        """
        self.db_config = db_config
        self.embedding_model = embedding_model
        self.encoder = None
        self.conn = None
        
    def _connect_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        try:
            self.conn = psycopg2.connect(**self.db_config)
            self.conn.autocommit = True
            logger.info("PostgreSQL ì—°ê²° ì„±ê³µ")
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
    
    def _load_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        try:
            self.encoder = SentenceTransformer(self.embedding_model)
            logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.embedding_model}")
        except Exception as e:
            logger.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def search(
        self, 
        query: str, 
        top_k: int = 5, 
        min_similarity: float = 0.0,
        corp_filter: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ê²€ìƒ‰"""
        if not self.conn:
            self._connect_db()
        if not self.encoder:
            self._load_embedding_model()
        
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self.encoder.encode([query])[0]
        
        # ëª¨ë¸ëª…ìœ¼ë¡œ í…Œì´ë¸”ëª… ìƒì„± (ì‹¤ì œ í…Œì´ë¸”ëª…ì— ë§ê²Œ)
        if "multilingual-e5-small" in self.embedding_model:
            model_name = "multilingual_e5_small"
        elif "kakaobank" in self.embedding_model:
            model_name = "kakaobank"
        elif "fine5" in self.embedding_model:
            model_name = "fine5"
        else:
            model_name = self.embedding_model.replace("/", "_").replace("-", "_")
        
        # ë²¡í„°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        embedding_str = '[' + ','.join(map(str, query_embedding.tolist())) + ']'
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        base_query = f"""
            SELECT 
                c.chunk_id,
                c.doc_id,
                c.chunk_type,
                c.natural_text,
                c.metadata,
                c.token_count,
                1 - (e.embedding <=> %s::vector) as similarity
            FROM chunks c
            JOIN embeddings_{model_name} e ON c.chunk_id = e.chunk_id
        """
        
        where_conditions = ["1 - (e.embedding <=> %s::vector) >= %s"]
        params = [embedding_str, embedding_str, min_similarity]
        
        if corp_filter:
            where_conditions.append("c.metadata->>'corp_name' = %s")
            params.append(corp_filter)
        
        query_sql = f"""
            {base_query}
            WHERE {' AND '.join(where_conditions)}
            ORDER BY e.embedding <=> %s::vector
            LIMIT %s
        """
        params.extend([embedding_str, top_k])
        
        
        # ê²€ìƒ‰ ì‹¤í–‰
        cursor = self.conn.cursor(cursor_factory=RealDictCursor)
        cursor.execute(query_sql, params)
        results = cursor.fetchall()
        cursor.close()
        
        # ê²°ê³¼ ë³€í™˜
        search_results = []
        for row in results:
            search_results.append({
                'chunk_id': row['chunk_id'],
                'doc_id': row['doc_id'],
                'chunk_type': row['chunk_type'],
                'natural_text': row['natural_text'],
                'metadata': row['metadata'],
                'token_count': row['token_count'],
                'similarity': float(row['similarity'])
            })
        
        return search_results
    
    def get_stats(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ í†µê³„ ì¡°íšŒ"""
        if not self.conn:
            self._connect_db()
        
        cursor = self.conn.cursor()
        
        # ì²­í¬ ìˆ˜ ì¡°íšŒ
        cursor.execute("SELECT COUNT(*) FROM chunks;")
        chunk_count = cursor.fetchone()[0]
        
        # ëª¨ë¸ë³„ ì„ë² ë”© ìˆ˜ ì¡°íšŒ (ì‹¤ì œ í…Œì´ë¸”ëª…ì— ë§ê²Œ)
        if "multilingual-e5-small" in self.embedding_model:
            model_name = "multilingual_e5_small"
        elif "kakaobank" in self.embedding_model:
            model_name = "kakaobank"
        elif "fine5" in self.embedding_model:
            model_name = "fine5"
        else:
            model_name = self.embedding_model.replace("/", "_").replace("-", "_")
        cursor.execute(f"SELECT COUNT(*) FROM embeddings_{model_name};")
        embedding_count = cursor.fetchone()[0]
        
        # ê¸°ì—…ë³„ í†µê³„
        cursor.execute("""
            SELECT metadata->>'corp_name' as corp_name, COUNT(*) as count
            FROM chunks 
            WHERE metadata->>'corp_name' IS NOT NULL
            GROUP BY metadata->>'corp_name'
            ORDER BY count DESC
            LIMIT 10;
        """)
        corp_stats = cursor.fetchall()
        
        cursor.close()
        
        return {
            'total_chunks': chunk_count,
            'total_embeddings': embedding_count,
            'model_name': self.embedding_model,
            'top_corporations': corp_stats
        }


def get_db_config() -> Dict[str, str]:
    """ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
    return {
        'host': os.getenv('POSTGRES_HOST', 'localhost'),
        'port': os.getenv('POSTGRES_PORT', '5432'),
        'database': os.getenv('POSTGRES_DB', 'skn_project'),
        'user': os.getenv('POSTGRES_USER', 'postgres'),
        'password': os.getenv('POSTGRES_PASSWORD', 'post1234')
    }


def test_connection() -> bool:
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        db_config = get_db_config()
        conn = psycopg2.connect(**db_config)
        conn.close()
        return True
    except Exception as e:
        logger.error(f"DB ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


def run_search_command(args, db_config: Dict[str, str]):
    """ê²€ìƒ‰ ëª…ë ¹ì–´ ì‹¤í–‰"""
    logger.info(f"ê²€ìƒ‰ ì‹œì‘: '{args.query}'")
    
    rag_system = RAGJSONLSystem(
        db_config=db_config,
        embedding_model=args.model
    )
    
    try:
        results = rag_system.search(
            query=args.query,
            top_k=args.top_k,
            min_similarity=args.min_similarity,
            corp_filter=args.corp_filter
        )
        
        print(f"\nğŸ” ê²€ìƒ‰ ê²°ê³¼: '{args.query}'")
        print(f"ğŸ“Š ì´ {len(results)}ê°œ ê²°ê³¼")
        print("=" * 80)
        
        for i, result in enumerate(results, 1):
            print(f"\n{i}. ì²­í¬ ID: {result['chunk_id']}")
            print(f"   ê¸°ì—…: {result['metadata'].get('corp_name', 'N/A')}")
            print(f"   ìœ ì‚¬ë„: {result['similarity']:.4f}")
            print(f"   í…ìŠ¤íŠ¸: {result['natural_text'][:200]}...")
            print(f"   í† í° ìˆ˜: {result['token_count']}")
        
        # ê²°ê³¼ ì €ì¥
        if args.save_results:
            output_file = f"search_results_{args.query.replace(' ', '_')}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            logger.info(f"ê²€ìƒ‰ ê²°ê³¼ ì €ì¥: {output_file}")
        
        return True
        
    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return False


def run_stats_command(args, db_config: Dict[str, str]):
    """í†µê³„ ëª…ë ¹ì–´ ì‹¤í–‰"""
    logger.info("ì‹œìŠ¤í…œ í†µê³„ ì¡°íšŒ")
    
    rag_system = RAGJSONLSystem(
        db_config=db_config,
        embedding_model=args.model
    )
    
    try:
        stats = rag_system.get_stats()
        
        print("\nğŸ“Š RAG ì‹œìŠ¤í…œ í†µê³„")
        print("=" * 50)
        print(f"ì´ ì²­í¬ ìˆ˜: {stats['total_chunks']:,}")
        print(f"ì´ ì„ë² ë”© ìˆ˜: {stats['total_embeddings']:,}")
        print(f"ì‚¬ìš© ëª¨ë¸: {stats['model_name']}")
        print(f"\nìƒìœ„ ê¸°ì—…ë“¤:")
        for corp_name, count in stats['top_corporations']:
            print(f"  - {corp_name}: {count:,}ê°œ")
        
        return True
        
    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="JSONL ê¸°ë°˜ RAG ì‹œìŠ¤í…œ CLI",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ê²€ìƒ‰
  python rag_cli.py search --query "ì‚¼ì„±ì „ì ë§¤ì¶œ" --top-k 5
  
  # ê¸°ì—… í•„í„°ë§ ê²€ìƒ‰
  python rag_cli.py search --query "ë§¤ì¶œ ì¦ê°€" --corp-filter "ì‚¼ì„±ì „ì"
  
  # í†µê³„ ì¡°íšŒ
  python rag_cli.py stats
  
  # ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©
  python rag_cli.py search --query "AI ê¸°ìˆ " --model sentence-transformers/all-MiniLM-L6-v2
        """
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='ìƒì„¸ ë¡œê·¸ ì¶œë ¥'
    )
    
    subparsers = parser.add_subparsers(dest='command', help='ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´')
    
    # ê²€ìƒ‰ ëª…ë ¹ì–´
    search_parser = subparsers.add_parser('search', help='ë¬¸ì„œ ê²€ìƒ‰')
    search_parser.add_argument('--query', required=True, help='ê²€ìƒ‰ ì¿¼ë¦¬')
    search_parser.add_argument('--top-k', type=int, default=5, help='ë°˜í™˜í•  ê²°ê³¼ ìˆ˜')
    search_parser.add_argument('--min-similarity', type=float, default=0.0, help='ìµœì†Œ ìœ ì‚¬ë„')
    search_parser.add_argument('--corp-filter', help='ê¸°ì—…ëª… í•„í„°')
    search_parser.add_argument('--model', default="intfloat/multilingual-e5-small", help='ì„ë² ë”© ëª¨ë¸')
    search_parser.add_argument('--save-results', action='store_true', help='ê²€ìƒ‰ ê²°ê³¼ ì €ì¥')
    
    # í†µê³„ ëª…ë ¹ì–´
    stats_parser = subparsers.add_parser('stats', help='ì‹œìŠ¤í…œ í†µê³„ ì¡°íšŒ')
    stats_parser.add_argument('--model', default="intfloat/multilingual-e5-small", help='ì„ë² ë”© ëª¨ë¸')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        sys.exit(1)
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # DB ì—°ê²° í…ŒìŠ¤íŠ¸
    logger.info("DB ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
    if not test_connection():
        logger.error("DB ì—°ê²° ì‹¤íŒ¨")
        sys.exit(1)
    logger.info("DB ì—°ê²° ì„±ê³µ")
    
    db_config = get_db_config()
    success = False
    
    # ëª…ë ¹ì–´ ì‹¤í–‰
    if args.command == 'search':
        success = run_search_command(args, db_config)
    elif args.command == 'stats':
        success = run_stats_command(args, db_config)
    
    if success:
        logger.info("ëª…ë ¹ì–´ ì‹¤í–‰ ì™„ë£Œ")
        sys.exit(0)
    else:
        logger.error("ëª…ë ¹ì–´ ì‹¤í–‰ ì‹¤íŒ¨")
        sys.exit(1)


if __name__ == "__main__":
    main()
