#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì„ë² ë”© ëª¨ë¸ë³„ RAG ë¹„êµ ë¶„ì„
4ê°€ì§€ ì„ë² ë”© ëª¨ë¸ë¡œ ê²€ìƒ‰, ì¦ê°•, ìƒì„± ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
"""

import sys
import os
from pathlib import Path
from datetime import datetime

sys.path.insert(0, str(Path(__file__).resolve().parents[4]))

from ..rag_system import RAGSystem
from ..models.config import EmbeddingModelType
from ..generation.generator import OllamaGenerator, GenerationConfig
from ..augmentation.formatters import EnhancedPromptFormatter, PolicyFormatter
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def get_db_config() -> dict:
    """ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •"""
    return {
        'host': os.getenv('PG_HOST', 'localhost'),
        'port': os.getenv('PG_PORT', '5432'),
        'database': os.getenv('PG_DB', 'rey'),
        'user': os.getenv('PG_USER', 'postgres'),
        'password': os.getenv('PG_PASSWORD', 'post1234')
    }


def compare_all_models(
    query: str,
    llm_model: str = "gemma3:4b",
    formatter_name: str = "enhanced",
    output_dir: str = "results"
):
    """
    4ê°€ì§€ ì„ë² ë”© ëª¨ë¸ë¡œ ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ë¹„êµ

    Args:
        query: í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
        llm_model: ì‚¬ìš©í•  LLM ëª¨ë¸
        formatter_name: ì‚¬ìš©í•  í¬ë§·í„° (enhanced ë˜ëŠ” policy)
        output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    """
    db_config = get_db_config()

    # LLM Generator ì´ˆê¸°í™”
    llm_generator = OllamaGenerator(
        base_url="http://localhost:11434",
        default_model=llm_model
    )

    # Ollama ì„œë²„ í™•ì¸
    if not llm_generator.check_health():
        logger.error("Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤.")
        return None

    # í¬ë§·í„° ì„ íƒ
    if formatter_name == "policy":
        formatter = PolicyFormatter()
    else:
        formatter = EnhancedPromptFormatter()

    # ì„ë² ë”© ëª¨ë¸ ëª©ë¡
    models = [
        ("E5", EmbeddingModelType.MULTILINGUAL_E5_SMALL),
        ("KAKAO", EmbeddingModelType.KAKAOBANK_DEBERTA),
        ("QWEN", EmbeddingModelType.QWEN_EMBEDDING),
        ("GEMMA", EmbeddingModelType.EMBEDDING_GEMMA)
    ]

    results = {}

    print(f"\n{'='*80}")
    print(f"ì„ë² ë”© ëª¨ë¸ë³„ RAG ë¹„êµ ë¶„ì„")
    print(f"{'='*80}")
    print(f"ì§ˆë¬¸: {query}")
    print(f"LLM ëª¨ë¸: {llm_model}")
    print(f"í¬ë§·í„°: {formatter_name}")
    print(f"{'='*80}\n")

    # ê° ì„ë² ë”© ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
    for name, model_type in models:
        print(f"\n{'='*80}")
        print(f"ğŸ” {name} ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
        print(f"{'='*80}\n")

        try:
            # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            rag_system = RAGSystem(
                model_type=model_type,
                db_config=db_config,
                formatter=formatter,
                llm_generator=llm_generator,
                enable_generation=True
            )

            # 1. ê²€ìƒ‰ë§Œ ìˆ˜í–‰
            print(f"1ï¸âƒ£ ê²€ìƒ‰ ì¤‘...")
            search_results = rag_system.search_only(query=query, top_k=5)
            print(f"   âœ… {len(search_results)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
            avg_similarity = sum(d.get('similarity', 0) for d in search_results) / len(search_results) if search_results else 0
            print(f"   í‰ê·  ìœ ì‚¬ë„: {avg_similarity:.3f}\n")

            # 2. ê²€ìƒ‰ + ì¦ê°•
            print(f"2ï¸âƒ£ ì¦ê°• ì¤‘...")
            augment_response = rag_system.retrieve_and_augment(query=query, top_k=5)
            augmented_context = augment_response.augmented_context.context_text
            print(f"   âœ… ì¦ê°• ì™„ë£Œ")
            print(f"   ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(augmented_context)} ê¸€ì\n")

            # 3. ì „ì²´ íŒŒì´í”„ë¼ì¸ (ê²€ìƒ‰ + ì¦ê°• + ìƒì„±)
            print(f"3ï¸âƒ£ LLM ë‹µë³€ ìƒì„± ì¤‘...")
            full_response = rag_system.generate_answer(
                query=query,
                top_k=5,
                generation_config=GenerationConfig(
                    model=llm_model,
                    temperature=0.7,
                    max_tokens=1500
                )
            )

            generated_answer = full_response.generated_answer.answer
            generation_time = full_response.generated_answer.generation_time_ms
            tokens_used = full_response.generated_answer.tokens_used

            print(f"   âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
            print(f"   ìƒì„± ì‹œê°„: {generation_time:.2f}ms")
            print(f"   í† í° ìˆ˜: {tokens_used}\n")

            # ê²°ê³¼ ì €ì¥
            results[name] = {
                "model_type": model_type.name,
                "search_results": search_results,
                "avg_similarity": avg_similarity,
                "augmented_context": augmented_context,
                "context_length": len(augmented_context),
                "generated_answer": generated_answer,
                "generation_time_ms": generation_time,
                "tokens_used": tokens_used
            }

        except Exception as e:
            logger.error(f"{name} ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
            results[name] = {
                "error": str(e)
            }

    # ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
    report_path = generate_markdown_report(
        query=query,
        llm_model=llm_model,
        formatter_name=formatter_name,
        results=results,
        output_dir=output_dir
    )

    print(f"\n{'='*80}")
    print(f"âœ… ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“„ ë³´ê³ ì„œ ì €ì¥: {report_path}")
    print(f"{'='*80}\n")

    return report_path


def generate_markdown_report(
    query: str,
    llm_model: str,
    formatter_name: str,
    results: dict,
    output_dir: str
) -> str:
    """ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±"""

    os.makedirs(output_dir, exist_ok=True)

    # íŒŒì¼ëª… ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_query = "".join(c for c in query if c.isalnum() or c in (' ', '-', '_')).rstrip()
    safe_query = safe_query.replace(' ', '_')[:50]
    filename = f"embedding_model_comparison_{safe_query}_{timestamp}.md"
    filepath = os.path.join(output_dir, filename)

    # ë§ˆí¬ë‹¤ìš´ ìƒì„±
    md_content = f"""# ì„ë² ë”© ëª¨ë¸ë³„ RAG ì„±ëŠ¥ ë¹„êµ ë³´ê³ ì„œ

## ì‹¤í—˜ ì •ë³´

- **ì§ˆë¬¸**: {query}
- **LLM ëª¨ë¸**: {llm_model}
- **í¬ë§·í„°**: {formatter_name}
- **ìƒì„± ì‹œê°„**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- **ë¹„êµ ëª¨ë¸ ìˆ˜**: {len(results)}ê°œ

---

## ğŸ“Š ì¢…í•© ë¹„êµí‘œ

| ì„ë² ë”© ëª¨ë¸ | í‰ê·  ìœ ì‚¬ë„ | ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ | ìƒì„± ì‹œê°„(ms) | í† í° ìˆ˜ | ë‹µë³€ ê¸¸ì´ |
|------------|-------------|---------------|---------------|---------|-----------|
"""

    # ë¹„êµí‘œ ì‘ì„±
    for model_name, result in results.items():
        if "error" in result:
            md_content += f"| {model_name} | ERROR | - | - | - | - |\n"
        else:
            avg_sim = result['avg_similarity']
            ctx_len = result['context_length']
            gen_time = result['generation_time_ms']
            tokens = result['tokens_used']
            ans_len = len(result['generated_answer'])
            md_content += f"| {model_name} | {avg_sim:.3f} | {ctx_len} | {gen_time:.2f} | {tokens} | {ans_len} |\n"

    md_content += "\n---\n\n"

    # ê° ëª¨ë¸ë³„ ìƒì„¸ ê²°ê³¼
    for model_name, result in results.items():
        if "error" in result:
            md_content += f"## âŒ {model_name} ëª¨ï¿½ï¿½ - ì˜¤ë¥˜ ë°œìƒ\n\n"
            md_content += f"```\n{result['error']}\n```\n\n---\n\n"
            continue

        md_content += f"## ğŸ” {model_name} ëª¨ë¸\n\n"
        md_content += f"**ëª¨ë¸ íƒ€ì…**: `{result['model_type']}`\n\n"

        # 1. ê²€ìƒ‰ ê²°ê³¼
        md_content += f"### 1ï¸âƒ£ ê²€ìƒ‰ ê²°ê³¼\n\n"
        md_content += f"- **ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜**: {len(result['search_results'])}ê°œ\n"
        md_content += f"- **í‰ê·  ìœ ì‚¬ë„**: {result['avg_similarity']:.3f}\n\n"

        md_content += "#### ê²€ìƒ‰ëœ ë¬¸ì„œ ëª©ë¡\n\n"
        for i, doc in enumerate(result['search_results'], 1):
            similarity = doc.get('similarity', 0)
            source = doc.get('metadata', {}).get('source', 'N/A')
            content = doc.get('content', '')[:200].replace('\n', ' ')

            md_content += f"{i}. **[ìœ ì‚¬ë„: {similarity:.3f}]** {source}\n"
            md_content += f"   > {content}...\n\n"

        # 2. ì¦ê°• ê²°ê³¼
        md_content += f"### 2ï¸âƒ£ ì¦ê°• ê²°ê³¼\n\n"
        md_content += f"- **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´**: {result['context_length']} ê¸€ì\n"
        md_content += f"- **í¬ë§·í„°**: {formatter_name}\n\n"

        md_content += "#### ì¦ê°•ëœ ì»¨í…ìŠ¤íŠ¸\n\n"
        md_content += "```\n"
        # ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ
        context = result['augmented_context']
        if len(context) > 2000:
            md_content += context[:2000] + "\n... (ì´í•˜ ìƒëµ)\n"
        else:
            md_content += context + "\n"
        md_content += "```\n\n"

        # 3. LLM ìƒì„± ê²°ê³¼
        md_content += f"### 3ï¸âƒ£ LLM ìƒì„± ê²°ê³¼\n\n"
        md_content += f"- **ìƒì„± ì‹œê°„**: {result['generation_time_ms']:.2f}ms\n"
        md_content += f"- **ì‚¬ìš© í† í°**: {result['tokens_used']}ê°œ\n"
        md_content += f"- **ë‹µë³€ ê¸¸ì´**: {len(result['generated_answer'])} ê¸€ì\n\n"

        md_content += "#### ìƒì„±ëœ ë‹µë³€\n\n"
        md_content += f"{result['generated_answer']}\n\n"

        md_content += "---\n\n"

    # ë¶„ì„ ë° ê²°ë¡ 
    md_content += "## ğŸ“ˆ ë¶„ì„ ë° ê²°ë¡ \n\n"

    # ìœ ì‚¬ë„ ë¹„êµ
    valid_results = {k: v for k, v in results.items() if "error" not in v}
    if valid_results:
        best_similarity = max(valid_results.items(), key=lambda x: x[1]['avg_similarity'])
        worst_similarity = min(valid_results.items(), key=lambda x: x[1]['avg_similarity'])

        md_content += "### ê²€ìƒ‰ í’ˆì§ˆ (í‰ê·  ìœ ì‚¬ë„)\n\n"
        md_content += f"- **ìµœê³ **: {best_similarity[0]} ({best_similarity[1]['avg_similarity']:.3f})\n"
        md_content += f"- **ìµœì €**: {worst_similarity[0]} ({worst_similarity[1]['avg_similarity']:.3f})\n\n"

        # ìƒì„± ì†ë„ ë¹„êµ
        fastest = min(valid_results.items(), key=lambda x: x[1]['generation_time_ms'])
        slowest = max(valid_results.items(), key=lambda x: x[1]['generation_time_ms'])

        md_content += "### ìƒì„± ì†ë„\n\n"
        md_content += f"- **ê°€ì¥ ë¹ ë¦„**: {fastest[0]} ({fastest[1]['generation_time_ms']:.2f}ms)\n"
        md_content += f"- **ê°€ì¥ ëŠë¦¼**: {slowest[0]} ({slowest[1]['generation_time_ms']:.2f}ms)\n\n"

        # ë‹µë³€ ê¸¸ì´ ë¹„êµ
        longest = max(valid_results.items(), key=lambda x: len(x[1]['generated_answer']))
        shortest = min(valid_results.items(), key=lambda x: len(x[1]['generated_answer']))

        md_content += "### ë‹µë³€ ê¸¸ì´\n\n"
        md_content += f"- **ê°€ì¥ ê¸´ ë‹µë³€**: {longest[0]} ({len(longest[1]['generated_answer'])} ê¸€ì)\n"
        md_content += f"- **ê°€ì¥ ì§§ì€ ë‹µë³€**: {shortest[0]} ({len(shortest[1]['generated_answer'])} ê¸€ì)\n\n"

    md_content += "### ê¶Œì¥ ì‚¬í•­\n\n"
    md_content += "- **ê²€ìƒ‰ í’ˆì§ˆ ìš°ì„ **: í‰ê·  ìœ ì‚¬ë„ê°€ ë†’ì€ ëª¨ë¸ ì„ íƒ\n"
    md_content += "- **ì†ë„ ìš°ì„ **: ìƒì„± ì‹œê°„ì´ ì§§ì€ ëª¨ë¸ ì„ íƒ\n"
    md_content += "- **ê· í˜•**: ê²€ìƒ‰ í’ˆì§ˆê³¼ ì†ë„ë¥¼ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ ì„ íƒ\n\n"

    md_content += "---\n\n"
    md_content += f"*ë³´ê³ ì„œ ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"

    # íŒŒì¼ ì €ì¥
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(md_content)

    return filepath


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="ì„ë² ë”© ëª¨ë¸ë³„ RAG ì„±ëŠ¥ ë¹„êµ")
    parser.add_argument(
        "query",
        type=str,
        nargs="?",
        default="ì²­ë…„ ì „ì„¸ëŒ€ì¶œ ì¡°ê±´ê³¼ ê¸ˆë¦¬",
        help="í…ŒìŠ¤íŠ¸ ì§ˆë¬¸"
    )
    parser.add_argument(
        "--llm-model",
        type=str,
        default="gemma3:4b",
        help="ì‚¬ìš©í•  LLM ëª¨ë¸ (ê¸°ë³¸: gemma3:4b)"
    )
    parser.add_argument(
        "--formatter",
        type=str,
        default="enhanced",
        choices=["enhanced", "policy"],
        help="ì‚¬ìš©í•  í¬ë§·í„° (ê¸°ë³¸: enhanced)"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="results",
        help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: results)"
    )

    args = parser.parse_args()

    compare_all_models(
        query=args.query,
        llm_model=args.llm_model,
        formatter_name=args.formatter,
        output_dir=args.output_dir
    )
