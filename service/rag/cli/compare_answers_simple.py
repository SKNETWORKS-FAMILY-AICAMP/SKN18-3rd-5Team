#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì„ë² ë”© ëª¨ë¸ë³„ ë‹µë³€ ë¹„êµ (ê°„ë‹¨ ë²„ì „)
ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ í•œ ë²ˆì— 1ê°œ ëª¨ë¸ì”© ì‹¤í–‰í•˜ì—¬ ë‹µë³€ë§Œ ë¹„êµ
"""

import sys
import os
from pathlib import Path
from datetime import datetime
import gc

sys.path.insert(0, str(Path(__file__).resolve().parents[4]))

from ..rag_system import RAGSystem
from ..models.config import EmbeddingModelType
from ..generation.generator import OllamaGenerator, GenerationConfig
from ..augmentation.formatters import EnhancedPromptFormatter
import logging

logging.basicConfig(level=logging.WARNING)  # ë¡œê·¸ ìµœì†Œí™”
logger = logging.getLogger(__name__)


def get_db_config() -> dict:
    """ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •"""
    return {
        'host': os.getenv('PG_HOST', 'localhost'),
        'port': os.getenv('PG_PORT', '5432'),
        'database': os.getenv('PG_DB', 'rey'),
        'user': os.getenv('PG_USER', 'postgres'),
        'password': os.getenv('PG_PASSWORD', 'post1234')
    }


def test_single_model(
    model_name: str,
    model_type: EmbeddingModelType,
    query: str,
    llm_model: str,
    db_config: dict
) -> dict:
    """ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""

    print(f"\n{'='*60}")
    print(f"ğŸ” {model_name} ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
    print(f"{'='*60}")

    try:
        # LLM Generator ì´ˆê¸°í™”
        llm_generator = OllamaGenerator(
            base_url="http://localhost:11434",
            default_model=llm_model
        )

        # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        rag_system = RAGSystem(
            model_type=model_type,
            db_config=db_config,
            formatter=EnhancedPromptFormatter(),
            llm_generator=llm_generator,
            enable_generation=True
        )

        # ê²€ìƒ‰
        print(f"1ï¸âƒ£ ê²€ìƒ‰ ì¤‘...")
        search_results = rag_system.search_only(query=query, top_k=3)
        avg_similarity = sum(d.get('similarity', 0) for d in search_results) / len(search_results) if search_results else 0
        print(f"   âœ… {len(search_results)}ê°œ ë¬¸ì„œ (í‰ê·  ìœ ì‚¬ë„: {avg_similarity:.3f})")

        # ë‹µë³€ ìƒì„±
        print(f"2ï¸âƒ£ ë‹µë³€ ìƒì„± ì¤‘...")
        full_response = rag_system.generate_answer(
            query=query,
            top_k=3,  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ 3ê°œë§Œ
            generation_config=GenerationConfig(
                model=llm_model,
                temperature=0.7,
                max_tokens=1000
            )
        )

        answer = full_response.generated_answer.answer
        gen_time = full_response.generated_answer.generation_time_ms

        print(f"   âœ… ì™„ë£Œ ({gen_time:.0f}ms)")

        # ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´
        doc_sources = [
            doc.get('metadata', {}).get('source', 'N/A')
            for doc in search_results
        ]

        result = {
            "model_name": model_name,
            "answer": answer,
            "generation_time_ms": gen_time,
            "avg_similarity": avg_similarity,
            "doc_sources": doc_sources,
            "success": True
        }

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del rag_system
        del llm_generator
        gc.collect()

        return result

    except Exception as e:
        print(f"   âŒ ì˜¤ë¥˜: {e}")
        return {
            "model_name": model_name,
            "error": str(e),
            "success": False
        }


def generate_comparison_report(
    query: str,
    llm_model: str,
    results: list,
    output_dir: str
) -> str:
    """ìì—°ì–´ ë‹µë³€ ì¤‘ì‹¬ ë¹„êµ ë³´ê³ ì„œ ìƒì„±"""

    os.makedirs(output_dir, exist_ok=True)

    # íŒŒì¼ëª…
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"answer_comparison_{timestamp}.md"
    filepath = os.path.join(output_dir, filename)

    # ë§ˆí¬ë‹¤ìš´ ìƒì„±
    md = f"""# ì„ë² ë”© ëª¨ë¸ë³„ ë‹µë³€ ë¹„êµ

## ğŸ“‹ ì‹¤í—˜ ì •ë³´

- **ì§ˆë¬¸**: {query}
- **LLM ëª¨ë¸**: {llm_model}
- **ë¹„êµ ëª¨ë¸**: {len([r for r in results if r['success']])}ê°œ
- **ìƒì„± ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ğŸ“Š ê°„ë‹¨ ìš”ì•½

| ëª¨ë¸ | ìƒì„± ì‹œê°„ | ìœ ì‚¬ë„ | ìƒíƒœ |
|------|-----------|--------|------|
"""

    for result in results:
        if result['success']:
            md += f"| {result['model_name']} | {result['generation_time_ms']:.0f}ms | {result['avg_similarity']:.3f} | âœ… |\n"
        else:
            md += f"| {result['model_name']} | - | - | âŒ |\n"

    md += "\n---\n\n"

    # ê° ëª¨ë¸ì˜ ë‹µë³€ ë¹„êµ
    md += "## ğŸ’¬ ë‹µë³€ ë‚´ìš© ë¹„êµ\n\n"

    for i, result in enumerate(results, 1):
        if not result['success']:
            md += f"### {i}. âŒ {result['model_name']} ëª¨ë¸ - ì˜¤ë¥˜\n\n"
            md += f"```\n{result.get('error', 'Unknown error')}\n```\n\n"
            md += "---\n\n"
            continue

        md += f"### {i}. {result['model_name']} ëª¨ë¸\n\n"

        # ê²€ìƒ‰ ì •ë³´
        md += f"**ê²€ìƒ‰ ì •ë³´**:\n"
        md += f"- í‰ê·  ìœ ì‚¬ë„: {result['avg_similarity']:.3f}\n"
        md += f"- ì°¸ê³  ë¬¸ì„œ: {len(result['doc_sources'])}ê°œ\n"
        for j, source in enumerate(result['doc_sources'], 1):
            md += f"  {j}. {source}\n"
        md += f"\n**ìƒì„± ì‹œê°„**: {result['generation_time_ms']:.0f}ms\n\n"

        # ë‹µë³€
        md += f"**ë‹µë³€**:\n\n{result['answer']}\n\n"
        md += "---\n\n"

    # ë¶„ì„
    successful = [r for r in results if r['success']]
    if len(successful) > 1:
        md += "## ğŸ” ë‹µë³€ ë¶„ì„\n\n"

        # ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸
        fastest = min(successful, key=lambda x: x['generation_time_ms'])
        md += f"### âš¡ ê°€ì¥ ë¹ ë¥¸ ë‹µë³€\n**{fastest['model_name']}** ({fastest['generation_time_ms']:.0f}ms)\n\n"

        # ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„
        best_sim = max(successful, key=lambda x: x['avg_similarity'])
        md += f"### ğŸ¯ ê°€ì¥ ë†’ì€ ê²€ìƒ‰ í’ˆì§ˆ\n**{best_sim['model_name']}** (ìœ ì‚¬ë„: {best_sim['avg_similarity']:.3f})\n\n"

        # ê°€ì¥ ê¸´ ë‹µë³€
        longest = max(successful, key=lambda x: len(x['answer']))
        md += f"### ğŸ“ ê°€ì¥ ìƒì„¸í•œ ë‹µë³€\n**{longest['model_name']}** ({len(longest['answer'])} ê¸€ì)\n\n"

        md += "### ğŸ’¡ ê¶Œì¥ ì‚¬í•­\n\n"
        md += f"- **ê²€ìƒ‰ í’ˆì§ˆ ìš°ì„ **: {best_sim['model_name']}\n"
        md += f"- **ì†ë„ ìš°ì„ **: {fastest['model_name']}\n"
        md += f"- **ìƒì„¸í•¨ ìš°ì„ **: {longest['model_name']}\n\n"

    md += "---\n\n"
    md += f"*ë³´ê³ ì„œ ìƒì„±: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"

    # ì €ì¥
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(md)

    return filepath


def main():
    import argparse

    parser = argparse.ArgumentParser(description="ì„ë² ë”© ëª¨ë¸ë³„ ë‹µë³€ ë¹„êµ (ê°„ë‹¨ ë²„ì „)")
    parser.add_argument(
        "query",
        type=str,
        nargs="?",
        default="ì²­ë…„ ì „ì„¸ëŒ€ì¶œ ì¡°ê±´ê³¼ ê¸ˆë¦¬",
        help="í…ŒìŠ¤íŠ¸ ì§ˆë¬¸"
    )
    parser.add_argument(
        "--llm-model",
        type=str,
        default="gemma3:4b",
        help="LLM ëª¨ë¸ (ê¸°ë³¸: gemma3:4b)"
    )
    parser.add_argument(
        "--models",
        type=str,
        nargs="+",
        default=["E5", "KAKAO", "QWEN", "GEMMA"],
        help="ë¹„êµí•  ì„ë² ë”© ëª¨ë¸ë“¤"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="results",
        help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬"
    )

    args = parser.parse_args()

    # ëª¨ë¸ ë§¤í•‘
    model_mapping = {
        "E5": EmbeddingModelType.MULTILINGUAL_E5_SMALL,
        "KAKAO": EmbeddingModelType.KAKAOBANK_DEBERTA,
        "QWEN": EmbeddingModelType.QWEN_EMBEDDING,
        "GEMMA": EmbeddingModelType.EMBEDDING_GEMMA
    }

    db_config = get_db_config()

    print(f"\n{'='*60}")
    print(f"ì„ë² ë”© ëª¨ë¸ë³„ ë‹µë³€ ë¹„êµ")
    print(f"{'='*60}")
    print(f"ì§ˆë¬¸: {args.query}")
    print(f"LLM: {args.llm_model}")
    print(f"ëª¨ë¸: {', '.join(args.models)}")
    print(f"{'='*60}\n")

    # ê° ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ìˆœì°¨ ì‹¤í–‰)
    results = []
    for model_name in args.models:
        if model_name not in model_mapping:
            print(f"âš ï¸  '{model_name}' ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœ€.")
            continue

        result = test_single_model(
            model_name=model_name,
            model_type=model_mapping[model_name],
            query=args.query,
            llm_model=args.llm_model,
            db_config=db_config
        )
        results.append(result)

    # ë³´ê³ ì„œ ìƒì„±
    print(f"\n{'='*60}")
    print(f"ğŸ“ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
    report_path = generate_comparison_report(
        query=args.query,
        llm_model=args.llm_model,
        results=results,
        output_dir=args.output_dir
    )

    print(f"\n{'='*60}")
    print(f"âœ… ì™„ë£Œ!")
    print(f"ğŸ“„ ë³´ê³ ì„œ: {report_path}")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    main()
