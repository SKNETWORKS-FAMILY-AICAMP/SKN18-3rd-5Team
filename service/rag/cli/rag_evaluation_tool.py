#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í†µí•© RAG í‰ê°€ ë„êµ¬
- RAG ê²€ìƒ‰ ê²°ê³¼ ìƒì„± (rag_success í˜•ì‹)
- ë‹¨ì¼ ëª¨ë“œ, ë‹¨ì¼ ê²°ê³¼ë¬¼
"""

import sys
import json
import argparse
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€ (ì§ì ‘ ì‹¤í–‰ ì‹œ ê²½ë¡œ ë¬¸ì œ í•´ê²°)
current_file = Path(__file__).resolve()
project_root = current_file.parents[3]  # service/rag/cli/ -> project_root (SKN18-3rd-5Team)
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from service.rag.rag_system import RAGSystem
from service.rag.models.config import EmbeddingModelType
from service.rag.evaluation.evaluator import RAGEvaluator, EvaluationConfig
from service.rag.evaluation.metrics import MetricsCalculator
from service.rag.cli.rag_cli import RAGJSONLSystem, get_db_config
from config.vector_database import get_vector_db_config
import logging
import json

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RAGEvaluationTool:
    """í†µí•© RAG í‰ê°€ ë„êµ¬"""
    
    def __init__(self, embedding_model: str = "intfloat/multilingual-e5-small"):
        """ì´ˆê¸°í™”"""
        self.embedding_model = embedding_model
        self.rag_system = RAGJSONLSystem(
            db_config=get_db_config(),
            embedding_model="intfloat/multilingual-e5-small"
        )
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°ê¸° ì´ˆê¸°í™”
        self.metrics_calculator = MetricsCalculator()
        
        logger.info(f"RAG Evaluation Tool ì´ˆê¸°í™” ì™„ë£Œ: {embedding_model}")
    
    def _load_evaluation_queries(self) -> List[Dict[str, Any]]:
        """í‰ê°€ ì¿¼ë¦¬ ë¡œë“œ"""
        queries_path = project_root / "service" / "rag" / "cli" / "evaluation_queries.json"
        try:
            with open(queries_path, 'r', encoding='utf-8') as f:
                queries = json.load(f)
            logger.info(f"í‰ê°€ ì¿¼ë¦¬ ë¡œë“œ ì™„ë£Œ: {len(queries)}ê°œ")
            return queries
        except Exception as e:
            logger.error(f"í‰ê°€ ì¿¼ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def run_evaluation(self, 
                      queries: List[Dict[str, Any]] = None,
                      top_k: int = 5,
                      min_similarity: float = 0.0,
                      corp_filter: str = None) -> List[Dict[str, Any]]:
        """
        RAG í‰ê°€ ì‹¤í–‰ (í†µí•©ëœ ë‹¨ì¼ ë©”ì„œë“œ)
        
        Args:
            queries: ê²€ìƒ‰í•  ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: evaluation_queries.json)
            top_k: ìƒìœ„ Kê°œ ê²°ê³¼ ë°˜í™˜
            min_similarity: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
            corp_filter: ê¸°ì—… í•„í„° (ì˜ˆ: "ì‚¼ì„±ì „ì")
        
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if queries is None:
            queries = self._load_evaluation_queries()
        
        logger.info("ğŸš€ RAG í‰ê°€ ì‹œì‘")
        logger.info(f"ğŸ“Š ëª¨ë¸: {self.embedding_model}")
        logger.info(f"ğŸ” Top-K: {top_k}")
        if corp_filter:
            logger.info(f"ğŸ¢ ê¸°ì—… í•„í„°: {corp_filter}")
        logger.info("=" * 80)
        
        results = []
        
        for i, query_data in enumerate(queries, 1):
            query_id = query_data.get("query_id", f"Q{i:03d}")
            query_text = query_data.get("query", "")
            
            logger.info(f"[{i}/{len(queries)}] {query_id}: {query_text}")
            
            try:
                # ê²€ìƒ‰ ì‹¤í–‰
                start_time = time.time()
                search_results = self.rag_system.search(
                    query=query_text,
                    top_k=top_k,
                    min_similarity=min_similarity,
                    corp_filter=corp_filter
                )
                search_time = (time.time() - start_time) * 1000  # ms
                
                # ê²°ê³¼ í¬ë§·íŒ…
                formatted_results = []
                for result in search_results:
                    formatted_result = {
                        "chunk_id": result["chunk_id"],
                        "content": result["natural_text"],  # natural_textë¥¼ contentë¡œ ë§¤í•‘
                        "similarity": result["similarity"],
                        "search_time_ms": search_time,
                        "metadata": result["metadata"]
                    }
                    formatted_results.append(formatted_result)
                
                # ë©”íŠ¸ë¦­ ê³„ì‚° (complete_evaluation.pyì˜ ê¸°ëŠ¥)
                expected_keywords = query_data.get("expected_keywords", [])
                expected_docs = query_data.get("expected_docs", [])
                
                retrieval_metrics = self.metrics_calculator.calculate_retrieval_metrics(
                    query=query_text,
                    retrieved_docs=formatted_results,
                    expected_keywords=expected_keywords,
                    expected_docs=expected_docs,
                    k=top_k
                )
                
                # ì¿¼ë¦¬ ê²°ê³¼ ì €ì¥ (ë©”íŠ¸ë¦­ í¬í•¨)
                query_result = {
                    "query_id": query_id,
                    "query": query_text,
                    "search_results": formatted_results,
                    "retrieval_metrics": {
                        "recall_at_k": retrieval_metrics.recall_at_k,
                        "precision_at_k": retrieval_metrics.precision_at_k,
                        "mrr": retrieval_metrics.mrr,
                        "ndcg_at_k": retrieval_metrics.ndcg_at_k,
                        "avg_similarity": retrieval_metrics.avg_similarity,
                        "keyword_coverage": retrieval_metrics.keyword_coverage
                    }
                }
                
                results.append(query_result)
                
                if formatted_results:
                    logger.info(f"âœ… {len(formatted_results)}ê°œ ê²°ê³¼ ë°˜í™˜ (ìœ ì‚¬ë„: {formatted_results[0]['similarity']:.4f} ~ {formatted_results[-1]['similarity']:.4f})")
                else:
                    logger.info(f"âš ï¸  ê²°ê³¼ ì—†ìŒ (ì„ë² ë”© ë°ì´í„° ë¶€ì¡± ë˜ëŠ” í•„í„° ì¡°ê±´ ë¶ˆì¼ì¹˜)")
                
            except Exception as e:
                logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰
                error_result = {
                    "query_id": query_id,
                    "query": query_text,
                    "search_results": [],
                    "error": str(e)
                }
                results.append(error_result)
        
        logger.info(f"ğŸ‰ RAG í‰ê°€ ì™„ë£Œ!")
        logger.info(f"ğŸ“ˆ ì´ {len(results)}ê°œ ì¿¼ë¦¬ ì²˜ë¦¬ ì™„ë£Œ")
        
        return results
    
    def save_results(self, results: List[Dict[str, Any]]) -> str:
        """
        ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (JSON + í…ìŠ¤íŠ¸ íŒŒì¼)
        
        Args:
            results: ì €ì¥í•  ê²°ê³¼
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        timestamp_int = int(time.time())
        
        # JSON íŒŒì¼ ì €ì¥
        json_filename = f"rag_evaluation_{timestamp}.json"
        json_path = project_root / "service" / "rag" / "results" / json_filename
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        json_path.parent.mkdir(parents=True, exist_ok=True)
        
        # JSON ê²°ê³¼ ì €ì¥
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ğŸ’¾ JSON ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {json_path}")
        
        # í…ìŠ¤íŠ¸ íŒŒì¼ë“¤ ì €ì¥ (complete_evaluation.pyì™€ ë™ì¼í•œ í˜•ì‹)
        self._save_detailed_results(results, timestamp_int)
        self._save_summary_report(results, timestamp_int)
        
        return str(json_path)
    
    def _save_detailed_results(self, results: List[Dict[str, Any]], timestamp: int):
        """ìƒì„¸ ê²°ê³¼ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥"""
        detailed_filename = f"detailed_results_{timestamp}.json"
        detailed_path = project_root / "service" / "rag" / "results" / detailed_filename
        
        # ìƒì„¸ ê²°ê³¼ í¬ë§·íŒ… (complete_evaluation.pyì™€ ë™ì¼í•œ í˜•ì‹)
        detailed_results = []
        for result in results:
            detailed_result = {
                "query_id": result.get("query_id", "unknown"),
                "query": result.get("query", "unknown"),
                "query_type": "factual_numerical",  # ê¸°ë³¸ê°’
                "difficulty": "easy",  # ê¸°ë³¸ê°’
                "retrieval": result.get("retrieval_metrics", {}),
                "generation": None,
                "overall_score": 0.0,
                "response_time_ms": 0.0,
                "retrieved_docs_count": len(result.get("search_results", [])),
                "error_message": None
            }
            
            # ì „ì²´ ì ìˆ˜ ê³„ì‚° (retrieval ë©”íŠ¸ë¦­ ê¸°ë°˜)
            if detailed_result["retrieval"]:
                recall = detailed_result["retrieval"].get("recall_at_k", 0.0)
                precision = detailed_result["retrieval"].get("precision_at_k", 0.0)
                mrr = detailed_result["retrieval"].get("mrr", 0.0)
                detailed_result["overall_score"] = (recall + precision + mrr) / 3 * 100
            
            # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
            if result.get("search_results"):
                detailed_result["response_time_ms"] = result["search_results"][0].get("search_time_ms", 0.0)
            
            detailed_results.append(detailed_result)
        
        # ìƒì„¸ ê²°ê³¼ ì €ì¥
        with open(detailed_path, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ğŸ“„ ìƒì„¸ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {detailed_path}")
    
    def _save_summary_report(self, results: List[Dict[str, Any]], timestamp: int):
        """ìš”ì•½ ë¦¬í¬íŠ¸ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥"""
        summary_filename = f"summary_report_{timestamp}.txt"
        summary_path = project_root / "service" / "rag" / "results" / summary_filename
        
        # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("RAG í‰ê°€ ìš”ì•½ ë¦¬í¬íŠ¸\n")
            f.write("=" * 80 + "\n\n")
            
            f.write(f"í‰ê°€ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ëª¨ë¸: {self.embedding_model}\n")
            f.write(f"ì´ ì¿¼ë¦¬ ìˆ˜: {len(results)}\n\n")
            
            # ì„±ê³µí•œ ì¿¼ë¦¬ ìˆ˜
            successful_queries = [r for r in results if r.get("search_results")]
            f.write(f"ì„±ê³µí•œ ì¿¼ë¦¬: {len(successful_queries)}/{len(results)}\n\n")
            
            # í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚°
            valid_results = [r for r in results if "retrieval_metrics" in r and r["retrieval_metrics"]]
            if valid_results:
                avg_recall = sum(r["retrieval_metrics"]["recall_at_k"] for r in valid_results) / len(valid_results)
                avg_precision = sum(r["retrieval_metrics"]["precision_at_k"] for r in valid_results) / len(valid_results)
                avg_mrr = sum(r["retrieval_metrics"]["mrr"] for r in valid_results) / len(valid_results)
                avg_ndcg = sum(r["retrieval_metrics"]["ndcg_at_k"] for r in valid_results) / len(valid_results)
                avg_keyword_coverage = sum(r["retrieval_metrics"]["keyword_coverage"] for r in valid_results) / len(valid_results)
                
                f.write("í‰ê·  ì„±ëŠ¥ ì§€í‘œ:\n")
                f.write(f"  - Recall@K: {avg_recall:.4f}\n")
                f.write(f"  - Precision@K: {avg_precision:.4f}\n")
                f.write(f"  - MRR: {avg_mrr:.4f}\n")
                f.write(f"  - NDCG@K: {avg_ndcg:.4f}\n")
                f.write(f"  - Keyword Coverage: {avg_keyword_coverage:.4f}\n\n")
            
            # í‰ê·  ìœ ì‚¬ë„
            if successful_queries:
                all_similarities = []
                for result in successful_queries:
                    for search_result in result["search_results"]:
                        all_similarities.append(search_result.get("similarity", 0.0))
                
                if all_similarities:
                    avg_similarity = sum(all_similarities) / len(all_similarities)
                    f.write(f"í‰ê·  ìœ ì‚¬ë„: {avg_similarity:.4f}\n\n")
            
            # ê°œë³„ ì¿¼ë¦¬ ê²°ê³¼
            f.write("ê°œë³„ ì¿¼ë¦¬ ê²°ê³¼:\n")
            f.write("-" * 80 + "\n")
            
            for i, result in enumerate(results, 1):
                query_id = result.get("query_id", f"Q{i:03d}")
                query = result.get("query", "unknown")
                search_count = len(result.get("search_results", []))
                
                f.write(f"{i}. {query_id}: {query}\n")
                f.write(f"   ê²€ìƒ‰ ê²°ê³¼: {search_count}ê°œ\n")
                
                if "retrieval_metrics" in result and result["retrieval_metrics"]:
                    metrics = result["retrieval_metrics"]
                    f.write(f"   Recall@K: {metrics.get('recall_at_k', 0.0):.4f}\n")
                    f.write(f"   Precision@K: {metrics.get('precision_at_k', 0.0):.4f}\n")
                    f.write(f"   MRR: {metrics.get('mrr', 0.0):.4f}\n")
                    f.write(f"   Keyword Coverage: {metrics.get('keyword_coverage', 0.0):.4f}\n")
                
                f.write("\n")
        
        logger.info(f"ğŸ“‹ ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {summary_path}")
    
    def print_summary(self, results: List[Dict[str, Any]]):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\nğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½:")
        print(f"   - ì´ ì¿¼ë¦¬ ìˆ˜: {len(results)}")
        print(f"   - ëª¨ë¸: {self.embedding_model}")
        
        # ì„±ê³µí•œ ì¿¼ë¦¬ ìˆ˜ ê³„ì‚°
        successful_queries = [r for r in results if r.get("search_results")]
        print(f"   - ì„±ê³µí•œ ì¿¼ë¦¬: {len(successful_queries)}/{len(results)}")
        
        # ë©”íŠ¸ë¦­ì´ ìˆëŠ” ê²°ê³¼ë§Œ í•„í„°ë§
        valid_results = [r for r in results if "retrieval_metrics" in r and r["retrieval_metrics"]]
        
        if valid_results:
            # í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚° (complete_evaluation.pyì™€ ë™ì¼)
            avg_recall_at_k = sum(r["retrieval_metrics"]["recall_at_k"] for r in valid_results) / len(valid_results)
            avg_precision_at_k = sum(r["retrieval_metrics"]["precision_at_k"] for r in valid_results) / len(valid_results)
            avg_mrr = sum(r["retrieval_metrics"]["mrr"] for r in valid_results) / len(valid_results)
            avg_ndcg_at_k = sum(r["retrieval_metrics"]["ndcg_at_k"] for r in valid_results) / len(valid_results)
            avg_keyword_coverage = sum(r["retrieval_metrics"]["keyword_coverage"] for r in valid_results) / len(valid_results)
            
            print(f"\nğŸ“ˆ í‰ê·  ì„±ëŠ¥ ì§€í‘œ:")
            print(f"   - Recall@K: {avg_recall_at_k:.4f}")
            print(f"   - Precision@K: {avg_precision_at_k:.4f}")
            print(f"   - MRR: {avg_mrr:.4f}")
            print(f"   - NDCG@K: {avg_ndcg_at_k:.4f}")
            print(f"   - Keyword Coverage: {avg_keyword_coverage:.4f}")
        
        if successful_queries:
            # í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
            all_similarities = []
            for result in successful_queries:
                for search_result in result["search_results"]:
                    all_similarities.append(search_result.get("similarity", 0.0))
            
            if all_similarities:
                avg_similarity = sum(all_similarities) / len(all_similarities)
                print(f"   - í‰ê·  ìœ ì‚¬ë„: {avg_similarity:.4f}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="í†µí•© RAG í‰ê°€ ë„êµ¬")
    parser.add_argument("--top-k", type=int, default=5, help="ê²€ìƒ‰í•  ìƒìœ„ Kê°œ ë¬¸ì„œ")
    parser.add_argument("--min-similarity", type=float, default=0.0, help="ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’")
    parser.add_argument("--corp-filter", type=str, help="ê¸°ì—… í•„í„° (ì˜ˆ: ì‚¼ì„±ì „ì)")
    parser.add_argument("--model", choices=["multilingual-e5-small", "kakaobank", "fine5"], 
                       default="multilingual-e5-small", help="ì„ë² ë”© ëª¨ë¸")
    
    args = parser.parse_args()
    
    try:
        # ë„êµ¬ ì´ˆê¸°í™”
        tool = RAGEvaluationTool(embedding_model=args.model)
        
        # í‰ê°€ ì‹¤í–‰
        results = tool.run_evaluation(
            top_k=args.top_k,
            min_similarity=args.min_similarity,
            corp_filter=args.corp_filter
        )
        
        # ê²°ê³¼ ì €ì¥ ë° ìš”ì•½ ì¶œë ¥
        output_path = tool.save_results(results)
        tool.print_summary(results)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_path}")
        
    except Exception as e:
        logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()