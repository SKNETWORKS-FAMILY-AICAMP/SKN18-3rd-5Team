#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ë°ì´í„° ìˆ˜ì§‘ ë° ì„ë² ë”© ìƒì„± ìŠ¤í¬ë¦½íŠ¸
JSON ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ë²¡í„° ì„ë² ë”©ì„ ìƒì„±í•˜ì—¬ pgvectorì— ì €ì¥
"""

import sys
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from .config import EmbeddingModelType
from .encoder import EmbeddingEncoder
from ..vectorstore.pgvector_store import PgVectorStore

logger = logging.getLogger(__name__)


class DataIngestionPipeline:
    """ë°ì´í„° ìˆ˜ì§‘ ë° ì„ë² ë”© íŒŒì´í”„ë¼ì¸"""

    def __init__(
        self,
        db_config: Optional[Dict[str, str]] = None,
        device: Optional[str] = None
    ):
        """
        Args:
            db_config: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
            device: ë””ë°”ì´ìŠ¤ ('cuda', 'cpu', None)
        """
        self.db_config = db_config
        self.device = device
        self.vector_store = PgVectorStore(db_config)

    def load_json_documents(self, json_path: str) -> List[Dict[str, Any]]:
        """JSON ë¬¸ì„œ ë¡œë“œ"""
        logger.info(f"Loading documents from: {json_path}")
        
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                documents = json.load(f)
            
            logger.info(f"Loaded {len(documents)} documents")
            return documents
            
        except Exception as e:
            logger.error(f"Failed to load documents: {e}")
            raise

    def process_documents(
        self,
        documents: List[Dict[str, Any]],
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        skip_chunking: bool = False
    ) -> List[Dict[str, Any]]:
        """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ê³  ì „ì²˜ë¦¬"""
        logger.info(f"Processing {len(documents)} documents into chunks")
        
        processed_chunks = []
        
        for doc_idx, doc in enumerate(documents):
            content = doc.get('content', '')
            if not content.strip():
                continue
            
            # ì²­í‚¹ ë¹„í™œì„±í™” ì˜µì…˜
            if skip_chunking:
                chunks = [content]  # ì›ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            else:
                # ê°„ë‹¨í•œ ì²­í‚¹ (ë¬¸ì¥ ë‹¨ìœ„)
                chunks = self._split_into_chunks(content, chunk_size, chunk_overlap)
            
            for chunk_idx, chunk_text in enumerate(chunks):
                if not chunk_text.strip():
                    continue
                
                chunk_data = {
                    'id': f"{doc.get('id', f'doc_{doc_idx}')}_{chunk_idx}",
                    'source': doc.get('source', 'unknown'),
                    'content': chunk_text.strip(),
                    'chunk_index': chunk_idx,
                    'metadata': {
                        'chunk_index': chunk_idx,
                        'total_chunks': len(chunks),
                        'source_doc_id': doc.get('id', f'doc_{doc_idx}'),
                        'source_doc_title': doc.get('source', 'unknown'),
                        'chunk_length': len(chunk_text.strip())
                    }
                }
                
                processed_chunks.append(chunk_data)
        
        logger.info(f"Created {len(processed_chunks)} chunks from {len(documents)} documents")
        return processed_chunks

    def _split_into_chunks(
        self,
        text: str,
        chunk_size: int,
        chunk_overlap: int
    ) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            if end >= len(text):
                chunks.append(text[start:])
                break
            
            # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„ (í•œêµ­ì–´ + ì˜ì–´)
            last_period = text.rfind('.', start, end)
            last_korean_period = text.rfind('ã€‚', start, end)
            last_question = text.rfind('?', start, end)
            last_exclamation = text.rfind('!', start, end)
            
            # ê°€ì¥ ê°€ê¹Œìš´ ë¬¸ì¥ ë ì°¾ê¸°
            sentence_end = max(last_period, last_korean_period, last_question, last_exclamation)
            
            if sentence_end > start + chunk_size // 2:  # ë„ˆë¬´ ì•ì—ì„œ ìë¥´ì§€ ì•Šë„ë¡
                end = sentence_end + 1
            
            chunks.append(text[start:end])
            start = end - chunk_overlap
        
        return chunks

    def create_embeddings(
        self,
        chunks: List[Dict[str, Any]],
        model_type: EmbeddingModelType,
        batch_size: int = 32,
        show_progress: bool = True
    ) -> List[Dict[str, Any]]:
        """ì²­í¬ë“¤ì— ëŒ€í•œ ì„ë² ë”© ìƒì„±"""
        logger.info(f"Creating embeddings with {model_type.value}")
        
        encoder = EmbeddingEncoder(model_type, self.device)
        
        # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        texts = [chunk['content'] for chunk in chunks]
        
        # ë°°ì¹˜ë³„ë¡œ ì„ë² ë”© ìƒì„±
        embeddings = encoder.encode_documents(
            texts=texts,
            batch_size=batch_size,
            show_progress=show_progress
        )
        
        # ì²­í¬ì— ì„ë² ë”© ì¶”ê°€
        for chunk, embedding in zip(chunks, embeddings):
            chunk['embedding'] = embedding
        
        logger.info(f"Created {len(embeddings)} embeddings")
        return chunks

    def store_embeddings(
        self,
        chunks_with_embeddings: List[Dict[str, Any]],
        model_type: EmbeddingModelType,
        source_type: str = "finance_support"
    ) -> int:
        """ì„ë² ë”©ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        logger.info(f"Storing embeddings in database")
        
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
            self.vector_store.connect()
            
            # ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” (í•„ìš”ì‹œ)
            self._initialize_schema()
            
            # ë¬¸ì„œ ì €ì¥
            inserted_count = self.vector_store.insert_documents_with_embeddings(
                documents=chunks_with_embeddings,
                model_type=model_type,
                source_type=source_type,
                batch_size=100
            )
            
            # ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
            logger.info("Creating vector index...")
            self.vector_store.create_vector_index(model_type, lists=100)
            
            logger.info(f"Successfully stored {inserted_count} documents with embeddings")
            return inserted_count
            
        except Exception as e:
            logger.error(f"Failed to store embeddings: {e}")
            raise
        finally:
            self.vector_store.disconnect()

    def _initialize_schema(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™”"""
        try:
            # ìŠ¤í‚¤ë§ˆê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            self.vector_store.cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables
                    WHERE table_schema = 'vector_db'
                    AND table_name = 'embedding_models'
                )
            """)
            schema_exists = self.vector_store.cursor.fetchone()[0]

            if schema_exists:
                logger.info("Database schema already exists, skipping initialization")
                return

            # ìŠ¤í‚¤ë§ˆ íŒŒì¼ ì½ê¸°
            schema_path = Path(__file__).parent.parent / "vectorstore" / "schema.sql"
            with open(schema_path, 'r', encoding='utf-8') as f:
                schema_sql = f.read()

            # ìŠ¤í‚¤ë§ˆ ì‹¤í–‰ (ë³„ë„ ì»¤ë°‹)
            self.vector_store.cursor.execute(schema_sql)
            self.vector_store.conn.commit()
            logger.info("Database schema initialized successfully")

        except Exception as e:
            # ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ë¡¤ë°±í•˜ê³  ê²½ê³ ë§Œ ì¶œë ¥
            self.vector_store.conn.rollback()
            logger.warning(f"Schema initialization failed (may already exist): {e}")
            # ìŠ¤í‚¤ë§ˆê°€ ì´ë¯¸ ì¡´ì¬í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚¤ì§€ ì•ŠìŒ

    def run_full_pipeline(
        self,
        json_path: str,
        model_type: EmbeddingModelType,
        source_type: str = "finance_support",
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        batch_size: int = 32,
        skip_chunking: bool = False
    ) -> Dict[str, Any]:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("Starting full ingestion pipeline")
        
        pipeline_stats = {
            'input_file': json_path,
            'model_type': model_type.value,
            'source_type': source_type,
            'chunk_size': chunk_size,
            'chunk_overlap': chunk_overlap,
            'batch_size': batch_size,
            'steps': {}
        }
        
        try:
            # 1. ë¬¸ì„œ ë¡œë“œ
            logger.info("Step 1: Loading documents")
            documents = self.load_json_documents(json_path)
            pipeline_stats['steps']['documents_loaded'] = len(documents)
            
            # 2. ë¬¸ì„œ ì²˜ë¦¬ (ì²­í‚¹)
            logger.info("Step 2: Processing documents into chunks")
            chunks = self.process_documents(documents, chunk_size, chunk_overlap, skip_chunking)
            pipeline_stats['steps']['chunks_created'] = len(chunks)
            
            # 3. ì„ë² ë”© ìƒì„±
            logger.info("Step 3: Creating embeddings")
            chunks_with_embeddings = self.create_embeddings(
                chunks, model_type, batch_size, show_progress=True
            )
            pipeline_stats['steps']['embeddings_created'] = len(chunks_with_embeddings)
            
            # 4. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            logger.info("Step 4: Storing in database")
            inserted_count = self.store_embeddings(
                chunks_with_embeddings, model_type, source_type
            )
            pipeline_stats['steps']['documents_stored'] = inserted_count
            
            pipeline_stats['status'] = 'success'
            logger.info("Pipeline completed successfully")
            
        except Exception as e:
            pipeline_stats['status'] = 'failed'
            pipeline_stats['error'] = str(e)
            logger.error(f"Pipeline failed: {e}")
            raise
        
        return pipeline_stats


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # ì„¤ì •
    json_path = "backend/data/vector_db/structured/ì„œìš¸ì‹œ_ì£¼ê±°ë³µì§€ì‚¬ì—…_pgvector_ready_clecd ..aned.json"
    model_type = EmbeddingModelType.MULTILINGUAL_E5_SMALL  # ì‹œì‘ì€ E5-Smallë¡œ
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
    db_config = {
        'host': 'localhost',
        'port': '5432',
        'database': 'rey',
        'user': 'postgres',
        'password': 'post1234'
    }
    
    print("ğŸš€ ë°ì´í„° ìˆ˜ì§‘ ë° ì„ë² ë”© ìƒì„± ì‹œì‘")
    print(f"ì…ë ¥ íŒŒì¼: {json_path}")
    print(f"ëª¨ë¸: {model_type.value}")
    print()
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = DataIngestionPipeline(db_config=db_config)
    
    try:
        stats = pipeline.run_full_pipeline(
            json_path=json_path,
            model_type=model_type,
            source_type="finance_support",
            chunk_size=500,
            chunk_overlap=50,
            batch_size=32
        )
        
        print("\n" + "="*60)
        print("íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼")
        print("="*60)
        print(f"ìƒíƒœ: {stats['status']}")
        print(f"ë¡œë“œëœ ë¬¸ì„œ: {stats['steps']['documents_loaded']}")
        print(f"ìƒì„±ëœ ì²­í¬: {stats['steps']['chunks_created']}")
        print(f"ìƒì„±ëœ ì„ë² ë”©: {stats['steps']['embeddings_created']}")
        print(f"ì €ì¥ëœ ë¬¸ì„œ: {stats['steps']['documents_stored']}")
        
        if stats['status'] == 'success':
            print("\nâœ… ë°ì´í„° ìˆ˜ì§‘ ë° ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
            print("ì´ì œ performance_test.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            print(f"\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {stats.get('error', 'Unknown error')}")
    
    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.exception("Pipeline execution failed")


if __name__ == "__main__":
    main()
