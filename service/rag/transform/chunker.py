#!/usr/bin/env python3
"""
====================================================================================
Transform Pipeline - Step 3: ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ë° ë©”íƒ€ë°ì´í„° ê°•í™”
====================================================================================

[íŒŒì´í”„ë¼ì¸ ìˆœì„œ]
1. structured.py      â†’ ë§ˆí¬ë‹¤ìš´ì„ êµ¬ì¡°í™”ëœ ì²­í¬ë¡œ ë³€í™˜
2. data_normalizer.py â†’ ë°ì´í„° ì •ê·œí™” ë° ìì—°ì–´ í’ˆì§ˆ ê°œì„ 
3. chunker.py         â†’ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ë° ë©”íƒ€ë°ì´í„° ê°•í™” (í˜„ì¬ íŒŒì¼)

[ì´ íŒŒì¼ì˜ ì—­í• ]
- ì‘ì€ ì²­í¬ë“¤ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë³‘í•©
- í† í° ìˆ˜ ì œí•œ ë‚´ì—ì„œ ìµœì  í¬ê¸° ì¡°ì •
- ì•ë’¤ ë¬¸ë§¥ ìœˆë„ìš° ì¶”ê°€
- ë©”íƒ€ë°ì´í„° ê°•í™” (doc_type, data_category, keywords ë“±)
- ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ í† í° ìˆ˜ ê³„ì‚°

[ì…ë ¥]
- data/transform/normalized/*_chunks.jsonl (normalizer.py ì¶œë ¥)

[ì¶œë ¥]
- data/transform/final/*_chunks.jsonl (ìµœì¢… ì²­í¬, ë²¡í„° DB ì €ì¥ ì¤€ë¹„ ì™„ë£Œ)

[ë‹¤ìŒ ë‹¨ê³„]
- ë²¡í„° DBì— ì„ë² ë”© ë° ì €ì¥
====================================================================================
"""

import re
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
import tiktoken


@dataclass
class ChunkConfig:
    """ì²­í‚¹ ì„¤ì •"""
    # ìµœëŒ€ í† í° ìˆ˜ (OpenAI embedding ê¸°ì¤€)
    max_tokens: int = 8000  # ì—¬ìœ ìˆê²Œ ì„¤ì •
    
    # ì²­í¬ ì˜¤ë²„ë© (ë¬¸ë§¥ ë³´ì¡´)
    overlap_tokens: int = 200
    
    # ìµœì†Œ ì²­í¬ í¬ê¸°
    min_tokens: int = 50
    
    # ì„ë² ë”© ëª¨ë¸
    embedding_model: str = "text-embedding-3-small"


class SmartChunker:
    """ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì²˜ë¦¬"""
    
    def __init__(self, config: ChunkConfig = None):
        self.config = config or ChunkConfig()
        
        # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        try:
            self.tokenizer = tiktoken.encoding_for_model(self.config.embedding_model)
        except:
            # fallback
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
    
    def should_merge_chunks(self, chunks: List[Dict]) -> List[Dict]:
        """
        ì‘ì€ ì²­í¬ë“¤ì„ ë³‘í•©
        - í…Œì´ë¸” í–‰ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ê´€ë ¨ í–‰ë“¤ê³¼ ë³‘í•©
        - í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ì•ë’¤ì™€ ë³‘í•©
        """
        
        merged = []
        buffer = []
        buffer_tokens = 0
        
        for chunk in chunks:
            chunk_tokens = self._count_tokens(chunk['natural_text'])
            
            # ë²„í¼ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì¶”ê°€
            if not buffer:
                buffer.append(chunk)
                buffer_tokens = chunk_tokens
                continue
            
            # ê°™ì€ ì„¹ì…˜ì´ê³  í† í° í•©ì´ max ì´í•˜ë©´ ë³‘í•©
            if (self._same_section(buffer[-1], chunk) and 
                buffer_tokens + chunk_tokens <= self.config.max_tokens):
                buffer.append(chunk)
                buffer_tokens += chunk_tokens
            else:
                # ë²„í¼ë¥¼ ë³‘í•©í•´ì„œ ì €ì¥
                merged.append(self._merge_buffer(buffer))
                buffer = [chunk]
                buffer_tokens = chunk_tokens
        
        # ë‚¨ì€ ë²„í¼ ì²˜ë¦¬
        if buffer:
            merged.append(self._merge_buffer(buffer))
        
        return merged
    
    def _same_section(self, chunk1: Dict, chunk2: Dict) -> bool:
        """ê°™ì€ ì„¹ì…˜ì¸ì§€ í™•ì¸"""
        return (chunk1['section_path'] == chunk2['section_path'] and
                chunk1['chunk_type'] == chunk2['chunk_type'])
    
    def _merge_buffer(self, buffer: List[Dict]) -> Dict:
        """ë²„í¼ì˜ ì²­í¬ë“¤ì„ í•˜ë‚˜ë¡œ ë³‘í•©"""
        
        if len(buffer) == 1:
            return buffer[0]
        
        # ìì—°ì–´ í…ìŠ¤íŠ¸ ë³‘í•©
        natural_texts = [c['natural_text'] for c in buffer]
        merged_text = ' '.join(natural_texts)
        
        # êµ¬ì¡°í™” ë°ì´í„° ë³‘í•© (í…Œì´ë¸” í–‰ì¸ ê²½ìš°)
        merged_structured = {}
        if buffer[0]['chunk_type'] == 'table_row':
            for chunk in buffer:
                merged_structured.update(chunk.get('structured_data', {}))
        
        # ì²« ë²ˆì§¸ ì²­í¬ì˜ ë©”íƒ€ë°ì´í„° ì‚¬ìš© (chunk_idëŠ” ìƒˆë¡œ ìƒì„±)
        merged_metadata = buffer[0]['metadata'].copy()
        merged_metadata['merged_count'] = len(buffer)
        
        return {
            'chunk_id': f"{buffer[0]['chunk_id']}_merged",
            'doc_id': buffer[0]['doc_id'],
            'chunk_type': buffer[0]['chunk_type'],
            'section_path': buffer[0]['section_path'],
            'structured_data': merged_structured,
            'natural_text': merged_text,
            'metadata': merged_metadata
        }
    
    def add_context_window(self, chunks: List[Dict]) -> List[Dict]:
        """
        ê° ì²­í¬ì— ì•ë’¤ ë¬¸ë§¥ ì¶”ê°€
        - ì´ì „/ë‹¤ìŒ ì²­í¬ì˜ ì¼ë¶€ë¥¼ ë©”íƒ€ë°ì´í„°ì— í¬í•¨
        """
        
        for i, chunk in enumerate(chunks):
            # ì´ì „ ì²­í¬ ë¬¸ë§¥
            if i > 0:
                prev_text = chunks[i-1]['natural_text']
                chunk['metadata']['prev_context'] = self._truncate_text(
                    prev_text, 
                    max_tokens=100
                )
            
            # ë‹¤ìŒ ì²­í¬ ë¬¸ë§¥
            if i < len(chunks) - 1:
                next_text = chunks[i+1]['natural_text']
                chunk['metadata']['next_context'] = self._truncate_text(
                    next_text,
                    max_tokens=100
                )
        
        return chunks
    
    def enhance_metadata(self, chunk: Dict) -> Dict:
        """ë©”íƒ€ë°ì´í„° ê°•í™”"""
        
        metadata = chunk['metadata']
        
        # 1. ë¬¸ì„œ íƒ€ì… ì¶”ë¡ 
        metadata['doc_type'] = self._infer_doc_type(metadata.get('document_name', ''))
        
        # 2. ë°ì´í„° íƒ€ì… ë¶„ë¥˜
        metadata['data_category'] = self._classify_data_category(
            chunk['natural_text'],
            chunk.get('section_path', '')
        )
        
        # 3. íšŒê³„ ì—°ë„ ì¶”ì¶œ
        metadata['fiscal_year'] = self._extract_fiscal_year(
            chunk['natural_text'],
            chunk.get('structured_data', {})
        )
        
        # 4. í‚¤ì›Œë“œ ì¶”ì¶œ (ê²€ìƒ‰ ê°•í™”)
        metadata['keywords'] = self._extract_keywords(
            chunk['natural_text'],
            chunk.get('structured_data', {})
        )
        
        # 5. í† í° ìˆ˜ ê³„ì‚°
        metadata['token_count'] = self._count_tokens(chunk['natural_text'])
        
        return chunk
    
    def _infer_doc_type(self, doc_name: str) -> str:
        """ë¬¸ì„œ íƒ€ì… ì¶”ë¡ """
        if 'ê°ì‚¬ë³´ê³ ì„œ' in doc_name:
            return 'audit_report'
        elif 'ì‚¬ì—…ë³´ê³ ì„œ' in doc_name:
            return 'business_report'
        elif 'ë¶„ê¸°ë³´ê³ ì„œ' in doc_name:
            return 'quarterly_report'
        else:
            return 'other'
    
    def _classify_data_category(self, text: str, section: str) -> str:
        """ë°ì´í„° ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        
        # ì¬ë¬´ì œí‘œ
        if any(keyword in section for keyword in ['ì¬ë¬´ìƒíƒœí‘œ', 'ì†ìµê³„ì‚°ì„œ', 'í˜„ê¸ˆíë¦„']):
            return 'financial_statement'
        
        # ì£¼ì‹ ì •ë³´
        elif 'ì£¼ì‹' in section:
            return 'stock_info'
        
        # ê°ì‚¬ ì˜ê²¬
        elif 'ê°ì‚¬' in section and 'ì˜ê²¬' in text:
            return 'audit_opinion'
        
        # ì£¼ì„
        elif 'ì£¼ì„' in section:
            return 'footnote'
        
        # ê¸°íƒ€
        else:
            return 'general'
    
    def _extract_fiscal_year(self, text: str, structured_data: Dict) -> Optional[int]:
        """íšŒê³„ ì—°ë„ ì¶”ì¶œ"""
        
        # êµ¬ì¡°í™” ë°ì´í„°ì—ì„œ ì¶”ì¶œ
        for key, value in structured_data.items():
            if 'ê¸°' in key or 'ë…„ë„' in key:
                match = re.search(r'20\d{2}', str(value))
                if match:
                    return int(match.group())
        
        # í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
        years = re.findall(r'20\d{2}', text)
        if years:
            return int(years[0])
        
        return None
    
    def _extract_keywords(self, text: str, structured_data: Dict) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)"""
        
        keywords = set()
        
        # êµ¬ì¡°í™” ë°ì´í„°ì—ì„œ í‚¤ ì¶”ì¶œ
        keywords.update(structured_data.keys())
        
        # í…ìŠ¤íŠ¸ì—ì„œ ì¬ë¬´ ìš©ì–´ ì¶”ì¶œ
        financial_terms = [
            'ìì‚°', 'ë¶€ì±„', 'ìë³¸', 'ë§¤ì¶œ', 'ì˜ì—…ì´ìµ', 'ë‹¹ê¸°ìˆœì´ìµ',
            'í˜„ê¸ˆ', 'ìœ ë™', 'ë¹„ìœ ë™', 'ê°ê°€ìƒê°', 'ì´ì', 'ë°°ë‹¹',
            'ì£¼ì‹', 'ë³´í†µì£¼', 'ìš°ì„ ì£¼'
        ]
        
        for term in financial_terms:
            if term in text:
                keywords.add(term)
        
        return list(keywords)[:10]  # ìµœëŒ€ 10ê°œ
    
    def _count_tokens(self, text: str) -> int:
        """í† í° ìˆ˜ ê³„ì‚°"""
        try:
            return len(self.tokenizer.encode(text))
        except:
            # fallback: ëŒ€ëµ 4ìë‹¹ 1í† í°
            return len(text) // 4
    
    def _truncate_text(self, text: str, max_tokens: int) -> str:
        """í…ìŠ¤íŠ¸ë¥¼ ìµœëŒ€ í† í° ìˆ˜ë¡œ ìë¥´ê¸°"""
        tokens = self.tokenizer.encode(text)
        if len(tokens) <= max_tokens:
            return text
        
        truncated_tokens = tokens[:max_tokens]
        return self.tokenizer.decode(truncated_tokens) + "..."


def process_chunks_with_enhancement(chunks: List[Dict]) -> List[Dict]:
    """ì²­í¬ ê°•í™” ì²˜ë¦¬"""
    
    chunker = SmartChunker()
    
    # 1. ì‘ì€ ì²­í¬ ë³‘í•©
    merged = chunker.should_merge_chunks(chunks)
    print(f"âœ… ì²­í¬ ë³‘í•©: {len(chunks)} â†’ {len(merged)}")
    
    # 2. ë¬¸ë§¥ ìœˆë„ìš° ì¶”ê°€
    with_context = chunker.add_context_window(merged)
    
    # 3. ë©”íƒ€ë°ì´í„° ê°•í™”
    enhanced = [chunker.enhance_metadata(chunk) for chunk in with_context]
    
    return enhanced


def process_jsonl_file(input_file: str, output_file: str):
    """
    Step 3: JSONL íŒŒì¼ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹
    
    ì…ë ¥: step2_normalizedì˜ JSONL íŒŒì¼
    ì¶œë ¥: finalì˜ JSONL íŒŒì¼
    """
    import json
    
    # ëª¨ë“  ì²­í¬ ì½ê¸°
    chunks = []
    with open(input_file, 'r', encoding='utf-8') as f:
        for line in f:
            chunks.append(json.loads(line))
    
    # ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì ìš©
    enhanced_chunks = process_chunks_with_enhancement(chunks)
    
    # ì €ì¥
    with open(output_file, 'w', encoding='utf-8') as f:
        for chunk in enhanced_chunks:
            f.write(json.dumps(chunk, ensure_ascii=False) + '\n')
    
    print(f"âœ… {len(enhanced_chunks)}ê°œ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ")
    
    # í†µê³„ ì¶œë ¥
    token_counts = [c['metadata']['token_count'] for c in enhanced_chunks]
    print(f"  ğŸ“Š í‰ê·  í† í° ìˆ˜: {sum(token_counts) / len(token_counts):.0f}")
    print(f"  ğŸ“Š ìµœì†Œ í† í° ìˆ˜: {min(token_counts)}")
    print(f"  ğŸ“Š ìµœëŒ€ í† í° ìˆ˜: {max(token_counts)}")


def process_directory(input_dir: str, output_dir: str):
    """
    Step 3: ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  JSONL íŒŒì¼ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹
    
    ì…ë ¥: data/transform/normalized/
    ì¶œë ¥: data/transform/final/
    """
    from pathlib import Path
    
    input_path = Path(input_dir)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    jsonl_files = list(input_path.glob("*_chunks.jsonl"))
    
    if not jsonl_files:
        print("âŒ JSONL íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("=" * 80)
    print("Transform Pipeline - Step 3: ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ë° ë©”íƒ€ë°ì´í„° ê°•í™”")
    print("=" * 80)
    print(f"ğŸ“ ì…ë ¥: {input_path}")
    print(f"ğŸ“ ì¶œë ¥: {output_path}")
    print(f"ğŸ“„ ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {len(jsonl_files)}ê°œ")
    print(f"\në‹¤ìŒ ë‹¨ê³„: ë²¡í„° DBì— ì„ë² ë”© ë° ì €ì¥")
    print("=" * 80)
    print()
    
    total_input = 0
    total_output = 0
    
    for i, input_file in enumerate(jsonl_files, 1):
        print(f"[{i}/{len(jsonl_files)}] ì²˜ë¦¬ ì¤‘: {input_file.name}")
        
        # ì…ë ¥ ì²­í¬ ìˆ˜ ì¹´ìš´íŠ¸
        with open(input_file, 'r', encoding='utf-8') as f:
            input_count = sum(1 for _ in f)
        total_input += input_count
        
        output_file = output_path / input_file.name
        process_jsonl_file(str(input_file), str(output_file))
        
        # ì¶œë ¥ ì²­í¬ ìˆ˜ ì¹´ìš´íŠ¸
        with open(output_file, 'r', encoding='utf-8') as f:
            output_count = sum(1 for _ in f)
        total_output += output_count
        
        print(f"  ğŸ’¾ ì €ì¥: {output_file.name}")
        print()
    
    print("=" * 80)
    print("Step 3 ì™„ë£Œ!")
    print("=" * 80)
    print(f"ì´ ì²˜ë¦¬: {total_input}ê°œ â†’ {total_output}ê°œ ì²­í¬")
    if total_input > total_output:
        reduction = (1 - total_output/total_input) * 100
        print(f"ë³‘í•© íš¨ê³¼: {reduction:.1f}% ê°ì†Œ")


if __name__ == "__main__":
    import sys
    from pathlib import Path
    
    # ë””ë ‰í† ë¦¬ ëª¨ë“œ (ê¶Œì¥)
    if len(sys.argv) == 1:
        # ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
        script_dir = Path(__file__).parent
        data_dir = script_dir.parent.parent.parent / "data"
        input_dir = data_dir / "transform" / "normalized"
        output_dir = data_dir / "transform" / "final"
        
        process_directory(str(input_dir), str(output_dir))
    
    # ë‹¨ì¼ íŒŒì¼ ëª¨ë“œ
    elif len(sys.argv) == 3:
        input_file = sys.argv[1]
        output_file = sys.argv[2]
        process_jsonl_file(input_file, output_file)
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    elif len(sys.argv) == 2 and sys.argv[1] == "--test":
        print("=" * 80)
        print("í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
        print("=" * 80)
        
        sample_chunks = [
            {
                'chunk_id': 'test_001',
                'doc_id': 'doc_001',
                'chunk_type': 'table_row',
                'section_path': 'ì¬ë¬´ì œí‘œ > ì¬ë¬´ìƒíƒœí‘œ',
                'natural_text': 'ìœ ë™ìì‚°ì€ 14,220ì–µì›',
                'structured_data': {'ê³¼ëª©': 'ìœ ë™ìì‚°', 'ê¸ˆì•¡': '1422091558149'},
                'metadata': {
                    'document_name': 'ê°ì‚¬ë³´ê³ ì„œ'
                }
            }
        ]
        
        enhanced = process_chunks_with_enhancement(sample_chunks)
        
        import json
        print(json.dumps(enhanced[0], ensure_ascii=False, indent=2))
    
    else:
        print("ì‚¬ìš©ë²•:")
        print("  1. ë””ë ‰í† ë¦¬ ëª¨ë“œ (ê¶Œì¥): python chunker.py")
        print("  2. ë‹¨ì¼ íŒŒì¼ ëª¨ë“œ:      python chunker.py <input.jsonl> <output.jsonl>")
        print("  3. í…ŒìŠ¤íŠ¸ ëª¨ë“œ:         python chunker.py --test")
        sys.exit(1)