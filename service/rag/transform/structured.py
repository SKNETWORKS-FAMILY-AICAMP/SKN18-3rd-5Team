#!/usr/bin/env python3
"""
====================================================================================
Transform Pipeline - Step 1: êµ¬ì¡°í™” ë° 1ì°¨ ì²­í‚¹
====================================================================================

[íŒŒì´í”„ë¼ì¸ ìˆœì„œ]
1. structured.py      â†’ ë§ˆí¬ë‹¤ìš´ì„ êµ¬ì¡°í™”ëœ ì²­í¬ë¡œ ë³€í™˜
2. normalizer.py â†’ ë°ì´í„° ì •ê·œí™” ë° ìì—°ì–´ í’ˆì§ˆ ê°œì„ 
3. chunker.py         â†’ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ë° ë©”íƒ€ë°ì´í„° ê°•í™”

[ì´ íŒŒì¼ì˜ ì—­í• ]
- ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ í…Œì´ë¸”/í…ìŠ¤íŠ¸ ë‹¨ìœ„ë¡œ íŒŒì‹±
- í…Œì´ë¸” í–‰ì„ ê¸°ë³¸ ìì—°ì–´ë¡œ ë³€í™˜ (ìµœì†Œí•œì˜ ì²˜ë¦¬ë§Œ)
- ì„¹ì…˜ ê²½ë¡œ ë° ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
- ì›ë³¸ ë°ì´í„° ë³´ì¡´ (structured_data)

[ì…ë ¥]
- data/markdown/*.md (XMLì—ì„œ ë³€í™˜ëœ ë§ˆí¬ë‹¤ìš´)

[ì¶œë ¥]
- data/transform/structured/*_chunks.jsonl (1ì°¨ ì²­í¬)
====================================================================================
"""

import re
import json
from pathlib import Path
from typing import Dict, Any, List
from dataclasses import dataclass, asdict, field
import hashlib
import sys


# ==========================================
# Table -> Natural Language (Convert)
# ==========================================
@dataclass
class Chunk:
    """ì²­í¬ ë°ì´í„° êµ¬ì¡°"""
    chunk_id: str
    doc_id: str
    chunk_type: str  # 'text', 'table_row', 'list_item'
    section_path: str
    
    # êµ¬ì¡°í™”ëœ ë°ì´í„°
    structured_data: Dict[str, Any] = field(default_factory=dict)
    
    # ìì—°ì–´ ë³€í™˜ (ê²€ìƒ‰ìš©)
    natural_text: str = ""
    
    # ë©”íƒ€ë°ì´í„° (ë¶€ê°€ ì •ë³´ë§Œ)
    metadata: Dict[str, Any] = field(default_factory=dict)


class TableRowConverter:
    """í…Œì´ë¸” í–‰ì„ ìì—°ì–´ë¡œ ë³€í™˜"""
    
    @staticmethod
    def convert(headers: List[str], row: List[str], section_path: str) -> str:
        """
        í…Œì´ë¸” í–‰ì„ ìì—°ì–´ë¡œ ë³€í™˜
        ì„¹ì…˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•œ ë³€í™˜
        """
        
        # ë¹ˆ ë°ì´í„° ì²´í¬
        non_empty_values = [v for v in row if v and v.strip() and v != '-']
        if not non_empty_values:
            return ""
        
        # ë³´ê³ ì„œ(ì‚¬ì—…ë³´ê³ ì„œ, ë¶„ê¸°ë³´ê³ ì„œ, ì£¼ìš”ì‚¬í•­ë³´ê³ ì„œ(+ ìê¸°ì·¨ë“ì£¼ì‹ê²°ì •ë³´ê³ ì„œ)) -> í…Œì´ë¸” ìœ í˜• íŒŒì•… í•„ìš”.

        # ì„¹ì…˜ë³„ íŠ¹í™” ë³€í™˜
        if "ì£¼ì‹ì˜ ì´ìˆ˜" in section_path:
            return TableRowConverter._convert_stock_table(headers, row) # ì£¼ì‹ í…Œì´ë¸”
        elif "ì¬ë¬´" in section_path or "ì†ìµ" in section_path:
            return TableRowConverter._convert_financial_table(headers, row) # ì¬ë¬´ í…Œì´ë¸”
        else:
            return TableRowConverter._convert_generic_table(headers, row) # ì¼ë°˜ í…Œì´ë¸”
    
    # ==========================================
    # 1. ì£¼ì‹ ì—´ í…Œì´ë¸”
    # ==========================================
    @staticmethod
    def _convert_stock_table(headers: List[str], row: List[str]) -> str:
        """ì£¼ì‹ í…Œì´ë¸” ì „ìš© ë³€í™˜ (ê¸°ë³¸ í˜•íƒœë§Œ)"""
        
        # êµ¬ ë¶„ ì—´ ì°¾ê¸°
        gubun_indices = [i for i, h in enumerate(headers) if 'êµ¬ ë¶„' in h]
        
        # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
        categories = []
        for idx in gubun_indices:
            if idx < len(row) and row[idx] and row[idx] != '-':
                categories.append(row[idx])
        
        # ë°ì´í„° ì¶”ì¶œ
        data_map = {}
        for i, header in enumerate(headers):
            if 'êµ¬ ë¶„' not in header and i < len(row) and row[i] and row[i] != '-':
                value = row[i].strip()
                header_clean = header.strip()
                
                if value:
                    data_map[header_clean] = value
        
        # ìì—°ì–´ ìƒì„± (ê°„ê²°í•˜ê²Œ, ì¡°ì‚¬ ì—†ì´)
        if categories:
            category_text = " - ".join(categories)
            
            if data_map:
                # "í‚¤: ê°’" í˜•íƒœë¡œ ê°„ê²°í•˜ê²Œ (ì¡°ì‚¬ ì œê±°)
                data_items = [f"{k} {v}" for k, v in data_map.items()]
                return f"{category_text}: {', '.join(data_items)}"
            else:
                return category_text
        
        return ""
    

    # ==========================================
    # 2. ì¬ë¬´ ì—´ í…Œì´ë¸”
    # ==========================================
    @staticmethod
    def _convert_financial_table(headers: List[str], row: List[str]) -> str:
        """ì¬ë¬´ í…Œì´ë¸” ì „ìš© ë³€í™˜ (ê¸°ë³¸ í˜•íƒœë§Œ, ë‹¨ìœ„ ë³€í™˜ì€ normalizerì—ì„œ)"""
        
        # ì²« ë²ˆì§¸ ì—´ì€ ë³´í†µ í•­ëª©ëª…
        if not row or not row[0] or row[0] == '-':
            return ""
        
        item_name = row[0].strip()
        
        # ë‚˜ë¨¸ì§€ ì—´ì€ ê°’ (ê°„ê²°í•˜ê²Œ, ì¡°ì‚¬ ì—†ì´)
        values = []
        for i in range(1, min(len(headers), len(row))):
            if row[i] and row[i] != '-':
                header = headers[i].strip()
                value = row[i].strip()
                # "í•­ëª©: ê°’" í˜•íƒœë¡œ ê°„ê²°í•˜ê²Œ
                values.append(f"{header} {value}")
        
        if values:
            # "ê³¼ëª©ëª… - í•­ëª©1 ê°’1, í•­ëª©2 ê°’2" í˜•íƒœ
            return f"{item_name} - {', '.join(values)}"
        else:
            return item_name
    
    # ==========================================
    # 3. ì¼ë°˜ í…Œì´ë¸”
    # ==========================================
    @staticmethod
    def _convert_generic_table(headers: List[str], row: List[str]) -> str:
        """ì¼ë°˜ í…Œì´ë¸” ë³€í™˜ (ê°„ê²°í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ)"""
        
        items = []
        for i in range(min(len(headers), len(row))):
            if row[i] and row[i] != '-' and row[i].strip():
                header = headers[i].strip()
                value = row[i].strip()
                
                if header and value:
                    # "í‚¤: ê°’" í˜•íƒœë¡œ ê°„ê²°í•˜ê²Œ
                    items.append(f"{header}: {value}")
        
        if items:
            return ", ".join(items)
        else:
            return ""

# ==========================================
# Markdown Chunking
# ==========================================
class MarkdownChunker:
    """ë§ˆí¬ë‹¤ìš´ì„ ì²­í¬ë¡œ ë¶„í• """
    
    def __init__(self, markdown_content: str, doc_metadata: Dict[str, Any]):
        self.content = markdown_content
        self.doc_metadata = doc_metadata
        self.chunks = []
        self.current_section_path = []
    
    def process(self) -> List[Chunk]:
        """ë§ˆí¬ë‹¤ìš´ ì²˜ë¦¬"""
        
        lines = self.content.split('\n')
        
        i = 0
        while i < len(lines):
            line = lines[i]
            
            # YAML í—¤ë” ê±´ë„ˆë›°ê¸°
            if i == 0 and line.startswith('---'):
                while i < len(lines) and (i == 0 or not lines[i].startswith('---')):
                    i += 1
                i += 1
                continue
            
            # ì„¹ì…˜ í—¤ë”
            if line.startswith('#'):
                self._update_section(line)
                i += 1
                continue
            
            # í…Œì´ë¸”
            if line.startswith('|'):
                i = self._process_table(lines, i)
                continue
            
            # í…ìŠ¤íŠ¸
            if line.strip() and not line.startswith('#'):
                i = self._process_text(lines, i)
                continue
            
            i += 1
        
        return self.chunks
    
    def _update_section(self, header_line: str):
        """ì„¹ì…˜ ì—…ë°ì´íŠ¸"""
        level = header_line.count('#')
        title = header_line.lstrip('#').strip()
        
        # ë ˆë²¨ì— ë§ê²Œ ì¡°ì •
        while len(self.current_section_path) >= level:
            self.current_section_path.pop()
        
        self.current_section_path.append(title)
    
    def _process_table(self, lines: List[str], start_idx: int) -> int:
        """í…Œì´ë¸” ì²˜ë¦¬"""
        
        # í…Œì´ë¸” ì¶”ì¶œ
        table_lines = []
        i = start_idx
        while i < len(lines) and (lines[i].startswith('|') or lines[i].strip() == ''):
            if lines[i].startswith('|'):
                table_lines.append(lines[i])
            i += 1
        
        if len(table_lines) < 3:
            return i
        
        # í—¤ë” íŒŒì‹± ([0,0]ì€ 'êµ¬ë¶„'ìœ¼ë¡œ ì²˜ë¦¬)
        headers = [h.strip() for h in table_lines[0].split('|')[1:-1]]
        
        # ë°ì´í„° í–‰ ì²˜ë¦¬
        for row_idx in range(2, len(table_lines)):
            row = [c.strip() for c in table_lines[row_idx].split('|')[1:-1]]
            
            if len(row) != len(headers):
                continue
            
            # êµ¬ì¡°í™”
            structured_data = {}
            for j, header in enumerate(headers):
                if j < len(row):
                    structured_data[header] = row[j]
            
            # ìì—°ì–´ ë³€í™˜
            section_path = ' > '.join(self.current_section_path)
            natural_text = TableRowConverter.convert(headers, row, section_path)
            
            if not natural_text:
                continue
            
            # ì²­í¬ ìƒì„±
            chunk = Chunk(
                chunk_id=self._generate_id(natural_text, 'table_row', section_path),
                doc_id=f"{self.doc_metadata.get('rcept_dt', '')}_{self.doc_metadata.get('corp_code', '')}",
                chunk_type='table_row',
                section_path=section_path,
                structured_data=structured_data,
                natural_text=natural_text,
                metadata={
                    'corp_name': self.doc_metadata.get('corp_name', ''),
                    'document_name': self.doc_metadata.get('document_name', ''),
                    'rcept_dt': self.doc_metadata.get('rcept_dt', ''),
                }
            )
            
            self.chunks.append(chunk)
        
        return i
    
    def _process_text(self, lines: List[str], start_idx: int) -> int:
        """í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
        
        text_lines = []
        i = start_idx
        
        while i < len(lines):
            line = lines[i]
            
            if line.startswith('|') or line.startswith('#'):
                break
            
            if not line.strip():
                if i + 1 < len(lines) and not lines[i + 1].strip():
                    break
            
            text_lines.append(line)
            i += 1
        
        text = '\n'.join(text_lines).strip()
        
        if len(text) > 10:
            section_path = ' > '.join(self.current_section_path)
            chunk = Chunk(
                chunk_id=self._generate_id(text, 'text', section_path),
                doc_id=f"{self.doc_metadata.get('rcept_dt', '')}_{self.doc_metadata.get('corp_code', '')}",
                chunk_type='text',
                section_path=section_path,
                structured_data={},
                natural_text=text,
                metadata={
                    'corp_name': self.doc_metadata.get('corp_name', ''),
                    'document_name': self.doc_metadata.get('document_name', ''),
                    'rcept_dt': self.doc_metadata.get('rcept_dt', ''),
                }
            )
            self.chunks.append(chunk)
        
        return i
    
    def _generate_id(self, content: str, chunk_type: str, section_path: str = "") -> str:
        """ID ìƒì„± - ì˜ë¯¸ìˆëŠ” IDë¡œ ê°œì„ """
        # ë¬¸ì„œ ID ê¸°ë°˜ + íƒ€ì… + ìˆœë²ˆìœ¼ë¡œ ìƒì„±
        doc_id = f"{self.doc_metadata.get('rcept_dt', '')}_{self.doc_metadata.get('corp_code', '')}"
        
        # ì„¹ì…˜ ê²½ë¡œì—ì„œ ë§ˆì§€ë§‰ ë¶€ë¶„ ì¶”ì¶œ
        section_name = section_path.split(' > ')[-1] if section_path else "unknown"
        section_clean = re.sub(r'[^\wê°€-í£]', '', section_name)[:10]  # íŠ¹ìˆ˜ë¬¸ì ì œê±°, 10ì ì œí•œ
        
        # íƒ€ì…ë³„ ìˆœë²ˆ ê³„ì‚°
        type_count = sum(1 for chunk in self.chunks if chunk.chunk_type == chunk_type)
        
        # ì˜ë¯¸ìˆëŠ” ID ìƒì„±: doc_id + type + section + ìˆœë²ˆ
        return f"{doc_id}_{chunk_type}_{section_clean}_{type_count:03d}"


def main():
    """
    Step 1: ë§ˆí¬ë‹¤ìš´ â†’ 1ì°¨ ì²­í¬ ë³€í™˜
    
    ì…ë ¥: data/markdown/*.md
    ì¶œë ¥: data/transform/structured/*_chunks.jsonl
    """
    
    # ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬
    if len(sys.argv) > 1 and sys.argv[1] == "--all":
        max_files = None  # ì „ì²´ ì²˜ë¦¬
        print("ğŸ”§ ì „ì²´ íŒŒì¼ ì²˜ë¦¬ ëª¨ë“œ")
    else:
        max_files = 10  # í…ŒìŠ¤íŠ¸ìš© 10ê°œë§Œ
        print("ğŸ”§ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (10ê°œ íŒŒì¼ë§Œ ì²˜ë¦¬)")

    # ê²½ë¡œ ì„¤ì • (transform í´ë” ê¸°ì¤€)
    script_dir = Path(__file__).parent  # service/rag/transform
    data_dir = script_dir.parent.parent.parent / "data"  # í”„ë¡œì íŠ¸ ë£¨íŠ¸/data
    markdown_dir = data_dir / "markdown"
    output_dir = data_dir / "transform" / "structured"

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir.mkdir(parents=True, exist_ok=True)

    # ëª¨ë“  ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì°¾ê¸°
    all_markdown_files = list(markdown_dir.glob("*.md"))
    
    if not all_markdown_files:
        print("âŒ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # íŒŒì¼ ìˆ˜ ì œí•œ ì ìš©
    if max_files:
        markdown_files = all_markdown_files[:max_files]
        print(f"ğŸ“„ ì „ì²´ {len(all_markdown_files)}ê°œ ì¤‘ {len(markdown_files)}ê°œ íŒŒì¼ ì²˜ë¦¬")
    else:
        markdown_files = all_markdown_files
        print(f"ğŸ“„ ì „ì²´ {len(markdown_files)}ê°œ íŒŒì¼ ì²˜ë¦¬")

    print("=" * 80)
    print("Transform Pipeline - Step 1: êµ¬ì¡°í™” ë° 1ì°¨ ì²­í‚¹")
    print("=" * 80)
    print(f"ğŸ“ ì…ë ¥: {markdown_dir}")
    print(f"ğŸ“ ì¶œë ¥: {output_dir}")
    print(f"ğŸ“„ ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {len(markdown_files)}ê°œ")
    if max_files:
        print(f"ğŸ’¡ ì „ì²´ ì²˜ë¦¬í•˜ë ¤ë©´: python {Path(__file__).name} --all")
    print(f"\në‹¤ìŒ ë‹¨ê³„: normalizer.pyë¡œ ì •ê·œí™” ìˆ˜í–‰")
    print("=" * 80)
    print()

    # ì „ì²´ í†µê³„
    total_chunks = 0
    processed_files = 0
    failed_files = 0

    # ê° ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì²˜ë¦¬
    for i, md_file in enumerate(markdown_files, 1):
        print(f"[{i}/{len(markdown_files)}] ì²˜ë¦¬ ì¤‘: {md_file.name}")
        
        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                markdown_content = f.read()

            # YAML í—¤ë”ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            doc_metadata = {}
            if markdown_content.startswith('---'):
                yaml_end = markdown_content.find('---', 3)
                if yaml_end != -1:
                    yaml_header = markdown_content[3:yaml_end]
                    for line in yaml_header.split('\n'):
                        if ':' in line:
                            key, value = line.split(':', 1)
                            doc_metadata[key.strip()] = value.strip()

            # ê¸°ë³¸ê°’ ì„¤ì •
            if not doc_metadata.get('corp_code'):
                doc_metadata = {
                    'corp_name': 'Unknown',
                    'document_name': 'Unknown'
                }

            print(f"  ğŸ“Š ë¬¸ì„œ ì •ë³´: {doc_metadata.get('corp_name')} ({doc_metadata.get('stock_code')})")

            # ì²­í¬ ìƒì„±
            chunker = MarkdownChunker(markdown_content, doc_metadata)
            chunks = chunker.process()

            print(f"  âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

            # JSONL ì €ì¥
            output_file = output_dir / f"{md_file.stem}_chunks.jsonl"

            with open(output_file, 'w', encoding='utf-8') as f:
                for chunk in chunks:
                    f.write(json.dumps(asdict(chunk), ensure_ascii=False) + '\n')

            print(f"  ğŸ’¾ ì €ì¥: {output_file.name}")

            # í†µê³„ ì—…ë°ì´íŠ¸
            total_chunks += len(chunks)
            processed_files += 1

            # ì²­í¬ íƒ€ì…ë³„ í†µê³„
            chunk_types = {}
            for chunk in chunks:
                chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1

            print(f"  ğŸ“ˆ ì²­í¬ íƒ€ì…: {', '.join([f'{k}({v})' for k, v in chunk_types.items()])}")
            print()

        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜: {e}")
            failed_files += 1
            print()

    # ì „ì²´ ê²°ê³¼ ì¶œë ¥
    print("=" * 60)
    print("     ì²˜ë¦¬ ì™„ë£Œ")
    print("=" * 60)

    # í…Œì´ë¸” ì²­í¬ ìƒ˜í”Œ
    table_chunks = [c for c in chunks if c.chunk_type == 'table_row'][:3]

    for i, chunk in enumerate(table_chunks):
        print(f"\n[{i+1}] {chunk.section_path}")
        print(f"íƒ€ì…: {chunk.chunk_type}")
        print(f"ìì—°ì–´: {chunk.natural_text}")
        print(f"êµ¬ì¡°í™”: {json.dumps(chunk.structured_data, ensure_ascii=False, indent=2)}")

    # í…ìŠ¤íŠ¸ ì²­í¬ ìƒ˜í”Œ
    text_chunks = [c for c in chunks if c.chunk_type == 'text'][:2]

    if text_chunks:
        print("\n" + "=" * 60)
        print("í…ìŠ¤íŠ¸ ì²­í¬ ìƒ˜í”Œ:")
        print("=" * 60)

        for i, chunk in enumerate(text_chunks):
            print(f"\n[{i+1}] {chunk.section_path}")
            print(f"ë‚´ìš©: {chunk.natural_text[:200]}...")

    # í†µê³„
    print("\n" + "=" * 60)
    print("ì²­í¬ í†µê³„:")
    print("=" * 60)

    chunk_types = {}
    for chunk in chunks:
        chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1

    for chunk_type, count in chunk_types.items():
        print(f"  {chunk_type}: {count}ê°œ")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] in ["--help", "-h"]:
        print("ì‚¬ìš©ë²•:")
        print(f"  python {Path(__file__).name}        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ (10ê°œ íŒŒì¼ë§Œ)")
        print(f"  python {Path(__file__).name} --all  # ì „ì²´ íŒŒì¼ ì²˜ë¦¬")
        print(f"  python {Path(__file__).name} --help # ë„ì›€ë§")
        sys.exit(0)
    
    main()