############################
# ì½”ë“œ ì‹¤í–‰í•œ ìœ ì˜ì‚¬í•­!!
'''
requirements_crawling.txtì— ìˆëŠ” íŒ¨í‚¤ì§€ ì„¤ì¹˜
playwright install -> ë¸Œë¼ìš°ì € ì—”ì§„ ì„¤ì¹˜

ìœ„ ë‘ ë‹¨ê³„ë¥¼ ë¨¼ì € í•´ì£¼ì…”ì•¼í•©ë‹ˆë‹¤!
'''
############################
import asyncio
from playwright.async_api import async_playwright
import pandas as pd
from datetime import datetime

async def crawl_shinhan_reports():
    base_url = "https://m.shinhansec.com/mweb/invt/shrh/ishrh1001?tabIdx=1"
    results = []
    filter_date = datetime(2025, 1, 1)
    MAX_CARDS = 1000  # ğŸš§ í…ŒìŠ¤íŠ¸ ì‹œ 10ê°œë§Œ (ì™„ë£Œë˜ë©´ 1000ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥)

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context(
            viewport={"width": 430, "height": 932},
            user_agent=(
                "Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) "
                "AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148"
            ),
        )

        page = await context.new_page()
        await page.goto(base_url, timeout=60000)
        await page.wait_for_selector("li.list__card-items")
        print(f"ğŸ“œ ë¬´í•œ ìŠ¤í¬ë¡¤ ì‹œì‘... (ìµœëŒ€ {MAX_CARDS}ê°œê¹Œì§€)")

        # ğŸ”½ ë¬´í•œ ìŠ¤í¬ë¡¤
        prev_count, same_count = 0, 0
        while True:
            await page.evaluate("""
                const el = document.querySelector('.content');
                if (el) el.scrollTo(0, el.scrollHeight);
                else window.scrollBy(0, document.body.scrollHeight);
            """)
            await page.wait_for_timeout(1500)

            cards = await page.query_selector_all("li.list__card-items")
            new_count = len(cards)
            print(f"ğŸ“ˆ ê°ì§€ëœ ì¹´ë“œ ìˆ˜: {new_count}")

            if new_count >= MAX_CARDS:
                print(f"âœ… {MAX_CARDS}ê°œ ë„ë‹¬ â€” ìŠ¤í¬ë¡¤ ì¢…ë£Œ")
                break

            if new_count == prev_count:
                same_count += 1
            else:
                same_count = 0
                prev_count = new_count

            if same_count >= 3:
                print("âœ… ë” ì´ìƒ ìƒˆë¡œìš´ ì¹´ë“œ ì—†ìŒ â€” ìŠ¤í¬ë¡¤ ì¢…ë£Œ")
                break

        cards = await page.query_selector_all("li.list__card-items")
        print(f"ì´ {len(cards)}ê°œ ì¹´ë“œ ê°ì§€ ì™„ë£Œ!\n")

        # ğŸ§© ê° ì¹´ë“œ ë³¸ë¬¸ í¬ë¡¤ë§
        for i, card in enumerate(cards[:MAX_CARDS]):
            try:
                data = await card.query_selector("div.list_data_area")

                title = await data.get_attribute("data-title")
                subtitle = await data.get_attribute("data-subtitle")
                date_str = await data.get_attribute("data-date")
                author = await data.get_attribute("data-nickname")
                category = await data.get_attribute("data-gubun")
                link = await data.get_attribute("data-message_url")

                if not link or not date_str:
                    continue

                date_obj = datetime.strptime(date_str, "%Y.%m.%d")
                if date_obj < filter_date:
                    continue

                # ìƒˆ íƒ­ ì—´ê¸°
                detail_page = await context.new_page()
                await detail_page.goto(link, timeout=90000)
                await detail_page.wait_for_load_state("domcontentloaded")
                await detail_page.wait_for_timeout(2000)

                content = ""

                try:
                    # âœ… iframe ë‚´ë¶€ ì ‘ê·¼
                    frames = detail_page.frames
                    target_frame = None
                    for f in frames:
                        if "bbs2.shinhaninvest.com" in (f.url or ""):
                            target_frame = f
                            break

                    if target_frame:
                        await target_frame.wait_for_selector("#contents", timeout=20000)

                        # âœ… ìš”ì•½(span) ì œê±° (ë³¸ë¬¸ ì²« span ì „ì²´ ì‚­ì œ)
                        await target_frame.evaluate("""
                            const container = document.querySelector('#contents');
                            if (container) {
                                const firstSpan = container.querySelector('span');
                                if (firstSpan) firstSpan.remove();
                            }
                        """)

                        # âœ… ë³¸ë¬¸ ì¶”ì¶œ
                        content_el = await target_frame.query_selector("#contents")
                        if content_el:
                            content = await content_el.inner_text()
                        else:
                            print(f"âš ï¸ ë³¸ë¬¸ ìš”ì†Œ ì—†ìŒ: {title}")
                    else:
                        print(f"âš ï¸ iframeì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {title}")

                except Exception as e:
                    print(f"âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")

                await detail_page.close()

                results.append({
                    "ì œëª©": title.strip() if title else "",
                    "ë¶€ì œ": subtitle.strip() if subtitle else "",
                    "ë‚ ì§œ": date_str,
                    "ì‘ì„±ì": author.strip() if author else "",
                    "ì¹´í…Œê³ ë¦¬": category.strip() if category else "",
                    "ë§í¬": link,
                    "ë³¸ë¬¸": content.strip() if content else "",
                })

                print(f"âœ… {i+1}ë²ˆì§¸ ì™„ë£Œ: {title} ({date_str})")

            except Exception as e:
                print(f"âš ï¸ {i+1}ë²ˆì§¸ ì¹´ë“œ ì—ëŸ¬: {e}")
                continue

        await browser.close()

    # ğŸ’¾ CSV ì €ì¥
    df = pd.DataFrame(results)
    df.to_csv("./data/shinhan_research_2025_playwright.csv", index=False, encoding="utf-8-sig")
    print(f"\nâœ… ì´ {len(df)}ê°œ ë°ì´í„° ì €ì¥ ì™„ë£Œ! â†’ shinhan_research_2025_playwright.csv")


if __name__ == "__main__":
    asyncio.run(crawl_shinhan_reports())
