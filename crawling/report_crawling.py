from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import time
import pandas as pd
from datetime import datetime
from webdriver_manager.chrome import ChromeDriverManager

def crawl_shinhan_research_after_2025():
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument(
        "user-agent=Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148"
    )

    driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)
    driver.get("https://m.shinhansec.com/mweb/invt/shrh/ishrh1001?tabIdx=1")
    time.sleep(2)

    # ğŸŒ€ ë¬´í•œ ìŠ¤í¬ë¡¤ (ë°ì´í„° ë‹¤ ë¶ˆëŸ¬ì˜¤ê¸°)
    last_height = driver.execute_script("return document.body.scrollHeight")
    while True:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height

    # ğŸ§© ì¹´ë“œ ì„ íƒì (í•„ìš”ì‹œ ìˆ˜ì •)
    cards = driver.find_elements(By.CSS_SELECTOR, "div.card")  
    print(f"ì´ {len(cards)}ê°œ ì¹´ë“œ ê°ì§€ë¨")

    results = []
    filter_date = datetime(2025, 1, 1)  # âœ… ê¸°ì¤€ ë‚ ì§œ (2025ë…„ 1ì›” 1ì¼)

    for i, card in enumerate(cards):
        try:
            cards = driver.find_elements(By.CSS_SELECTOR, "div.card")
            card = cards[i]
            driver.execute_script("arguments[0].scrollIntoView(true);", card)
            time.sleep(1)
            card.click()
            time.sleep(2)

            html = driver.page_source
            soup = BeautifulSoup(html, "html.parser")

            title = soup.select_one(".title").get_text(strip=True)
            subtitle = soup.select_one(".sub-title").get_text(strip=True)
            date_str = soup.select_one(".date").get_text(strip=True)
            author = soup.select_one(".author").get_text(strip=True)
            content = "\n".join([p.get_text(strip=True) for p in soup.select(".content p")])

            # âœ… ë‚ ì§œ íŒŒì‹± (ì˜ˆ: 2025.10.20)
            date_obj = datetime.strptime(date_str, "%Y.%m.%d")

            # âœ… ë‚ ì§œ í•„í„° ì ìš©
            if date_obj >= filter_date:
                results.append({
                    "ì œëª©": title,
                    "ë¶€ì œ": subtitle,
                    "ë‚ ì§œ": date_str,
                    "ì‘ì„±ì": author,
                    "ë³¸ë¬¸": content,
                })
                print(f"âœ… {i+1}ë²ˆì§¸ ({date_str}) ì¹´ë“œ ìˆ˜ì§‘ë¨: {title}")
            else:
                print(f"â© {i+1}ë²ˆì§¸ ({date_str})ëŠ” 2025ë…„ 1ì›” 1ì¼ ì´ì „ â€” ê±´ë„ˆëœ€")

            driver.back()
            time.sleep(2)

        except Exception as e:
            print(f"âš ï¸ {i+1}ë²ˆì§¸ ì¹´ë“œ ì—ëŸ¬: {e}")
            driver.back()
            time.sleep(2)
            continue

    driver.quit()

    df = pd.DataFrame(results)
    df.to_csv("shinhan_research_2025.csv", index=False, encoding="utf-8-sig")
    print(f"âœ… ì´ {len(df)}ê°œ ë°ì´í„° ì €ì¥ ì™„ë£Œ!")

    return df

# ì‹¤í–‰
df = crawl_shinhan_research_after_2025()
